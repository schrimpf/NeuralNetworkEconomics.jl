{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution-ShareAlike\n",
    "4.0 International\n",
    "License](http://creativecommons.org/licenses/by-sa/4.0/)\n",
    "\n",
    "\n",
    "### About this document\n",
    "\n",
    "This document was created using Weave.jl. The code is available in\n",
    "[on github](https://github.com/schrimpf/NeuralNetworkEconomics.jl). The same\n",
    "document generates both static webpages and associated [jupyter\n",
    "notebook](transformers.ipynb).\n",
    "\n",
    "$$\n",
    "\\def\\indep{\\perp\\!\\!\\!\\perp}\n",
    "\\def\\Er{\\mathrm{E}}\n",
    "\\def\\R{\\mathbb{R}}\n",
    "\\def\\En{{\\mathbb{E}_n}}\n",
    "\\def\\Pr{\\mathrm{P}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n",
    "\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:23:26.121000Z",
     "iopub.status.busy": "2022-10-26T20:23:25.212000Z",
     "iopub.status.idle": "2022-10-26T20:23:39.435000Z",
     "shell.execute_reply": "2022-10-26T20:23:39.373000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/.julia/dev/NeuralNetworkEconomics/docs`\n"
     ]
    }
   ],
   "source": [
    "markdown = try\n",
    "  \"md\" in keys(WEAVE_ARGS) && WEAVE_ARGS[\"md\"]\n",
    "catch\n",
    "  false\n",
    "end\n",
    "\n",
    "if !(\"DISPLAY\" ∈ keys(ENV))\n",
    "  # Make gr and pyplot backends for Plots work without a DISPLAY\n",
    "  ENV[\"GKSwstype\"]=\"nul\"\n",
    "  ENV[\"MPLBACKEND\"]=\"Agg\"\n",
    "end\n",
    "# Make gr backend work with λ and other unicode\n",
    "ENV[\"GKS_ENCODING\"] = \"utf-8\"\n",
    "\n",
    "using NeuralNetworkEconomics\n",
    "docdir = joinpath(dirname(Base.pathof(NeuralNetworkEconomics)), \"..\",\"docs\")\n",
    "\n",
    "using Pkg\n",
    "Pkg.activate(docdir)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Transformers have become the leading architecture for language related\n",
    "tasks. Transformers are also being applied to other domains, like\n",
    "images.\n",
    "\n",
    "Transformers were developed to overcome some of the downsides of\n",
    "recurrent networks. We briefly discussed the vanishing and exploding\n",
    "gradients problems. Recurrent networks also have a practical\n",
    "computational downside of being difficult to parallelize. Transformers\n",
    "were designed to be easy to parallelize while retaining some ability\n",
    "to represent short and long run dependencies in sequential data.\n",
    "\n",
    "Transformers encode sequential text data into numeric features in a\n",
    "learned manner. The resulting encoding preserves sequential\n",
    "information and can be readily parallelized.\n",
    "\n",
    "The @vaswani2017 paper that popularized transformers was\n",
    "about a translation task, and many introductory references  about\n",
    "transformers focus on this setting (such as\n",
    "[the illustrated transfomer](https://jalammar.github.io/illustrated-transformer/)).\n",
    "\n",
    "Translation tackles the following\n",
    "setting: given a whole text (usually sentence) in one language,\n",
    "$z_0, ..., z_T$, and a partial translation in another language,\n",
    "$x_0, ..., x_t$,\n",
    "the goal is to predict the next word, $x_{t+1}$. Transformers\n",
    "are also used for generative tasks---given $x_0, ..., x_t$, predict\n",
    "$x_{t+1}$. We will focus on a generative transformer since it is simpler and seems\n",
    "more relevant to economics.\n",
    "\n",
    "# Transformer\n",
    "\n",
    "Transformers create a mapping from\n",
    "$$\n",
    "(x_0, ..., x_t) \\to \\tilde{x}_t\n",
    "$$\n",
    "where\n",
    "$\\tilde{x}_t$\n",
    "is meant to contain all information relevant for\n",
    "predicting\n",
    "$x_{t+1}$\n",
    ".\n",
    "Moreover, the same mapping can be applied to all\n",
    "$t$ in parallel. This mapping consists of the following layers.\n",
    "\n",
    "## Embedding\n",
    "\n",
    "Each $x_t \\in X \\subseteq \\R^K$ is often contained in a high\n",
    "dimensional space. In text, $x_t$ in a vector of indicator variables\n",
    "representing which token is the $t$th token in the sequence. These\n",
    "tokens could be characters, or more commonly now, words. In either\n",
    "case, the dimension of $x_t$ is in the hundreds or thousands. Anyway,\n",
    "$x_t$ is often embedded into a lower dimensional space by\n",
    "$$\n",
    " x_t^e = W_e x_t\n",
    "$$\n",
    "where $W_e: \\R^k \\to \\R^d$ is linear.\n",
    "\n",
    "## Positional Encoding\n",
    "\n",
    "With the exception of this layer, the entire transformer is a\n",
    "symmetric function $(x_0, ..., x_t)$ --- it ignores order. Positional\n",
    "encoding adds some position information to $x_t^e$. This could be done\n",
    "by simply adding a coordinate containining e.g. $t/T$, but is most\n",
    "often done (following @vaswani2017) by\n",
    "$$\n",
    "x_t^{pe} = x_t^e + p(t;d)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "p(t;d) = \\left( \\sin(t/10000^{2/d}) , \\cos(t/10000^{2/d})\n",
    "    \\sin(t/10000^{4/d}) , \\cos(t/10000^{4/d}), ...\n",
    "    \\sin(t/10000^{d/d}) , \\cos(t/10000^{d/d}) \\right).\n",
    "$$\n",
    "The motivation was that this positional encoding betters represents\n",
    "intervals between words and offsets.\n",
    "\n",
    "## Encoder\n",
    "\n",
    "The $x_t^{pe}$ are now further transformed to incorporate information\n",
    "from other $x_s^{pe}$. This is done through multiple attention\n",
    "layers. To describe attention layers, let $x_t^{A,0} = x_t^{pe}$. An\n",
    "attention layer consists of:\n",
    "\n",
    "### (Masked) Self-Attention\n",
    "\n",
    "$$\n",
    "z_{0,t}^{A,\\ell} = \\sum_{j=0}^t \\frac{e^{ {x_t^{A,\\ell-1}}' Q_\\ell' K_\\ell\n",
    "x_j^{A,\\ell-1}}} { \\sum_{i=0}^t e^{{x_t^{A,\\ell-1}}' Q_\\ell' K_\\ell\n",
    "x_i^{A,\\ell-1}}} V_\\ell x_{j}^{A,\\ell-1}\n",
    "$$\n",
    "where $Q_{ell}$, $K_{ell},$ and $V_{\\ell}$ are all $m \\times d$\n",
    "matrices. These are often referred to as query, key, and value\n",
    "transformations respectively. The idea is that the query and key\n",
    "matrices determine how relevant $x_j$ is for $x_t$, and the value\n",
    "gives an altered representation of $x_j$.\n",
    "\n",
    "This is \"masked\" because $z_{0,t}^{A,\\ell}$ looks at the data from $0$\n",
    "to $t$ instead of the whole sequence from $0$ to $T$.\n",
    "\n",
    "If $d \\neq m$, then $d$ must be a multiple of $m$. If $d < m$, then\n",
    "there must be $d/m$ such $Q$, $K$, and $V$ matrices, and their outputs\n",
    "are concatenated together to ensure that $z_t^{A,\\ell}$ has\n",
    "the same dimension as $x_t^{A,\\ell-1}$\n",
    "\n",
    "### Residual Connection\n",
    "\n",
    "The output of the attention layer is then added to the input,\n",
    "$z_{1,t}^{A,\\ell} = x_t^{A,\\ell-1} + z_{0,t}^{A,\\ell}$\n",
    "This sort of residual connection is often used in deep\n",
    "learning. (E.g. Resnet is a well known convolutional network with residual\n",
    "connections that did very on image classification). It\n",
    "helps to ensure that gradients even deep in many layers are not zero.\n",
    "See @jastrzebski2017 for some theoretical justification for residual\n",
    "connections.\n",
    "\n",
    "### Layer Norm\n",
    "\n",
    "A layer normalization is then applied as in @ba2016. That is, we\n",
    "transform\n",
    "$$\n",
    "z_{2,t}^{A,\\ell} = \\frac{g^\\ell_t}{\\sigma_\\ell} (z_{1,t}^{A,\\ell} -\n",
    "\\mu_\\ell) + b_t^\\ell\n",
    "$$\n",
    "where $\\mu_\\ell$ and $\\sigma_\\ell$ are the mean and standard deviation\n",
    "of $z_{1,t}^{A,\\ell}$ across $t$.\n",
    "\n",
    "### Feed-Forward Layer\n",
    "\n",
    "A single layer feed forward network is then applied to each\n",
    "$z_{2,t}^{A,\\ell}$. That is, we take\n",
    "\n",
    "$$\n",
    "z_{3,t}^{A,\\ell} = f_\\ell(z_{2,t}^{A,\\ell})\n",
    "$$\n",
    "\n",
    "where $f_\\ell$ is a single layer feed forward network.\n",
    "\n",
    "### Residual Connection & Layer Norm Again\n",
    "\n",
    "Finally there is another residual connection and layer norm applied.\n",
    "\n",
    "$$ z_{4,t}^{A,\\ell} = z_{3,t}^{A,\\ell} + z_{2,t}^{A,\\ell} $$\n",
    "\n",
    "$$\n",
    "x_{t}^{A,\\ell} = \\frac{g^{\\ell 2}_t}{\\sigma_\\ell} (z_{4,t}^{A,\\ell} -\n",
    "\\mu_\\ell) + b_t^{\\ell 2}\n",
    "$$\n",
    "\n",
    "\n",
    "### Repeat\n",
    "\n",
    "## Prediction Layer\n",
    "\n",
    "Finally, the output of the encoder, $x_t^{A_L}$, is used to predict\n",
    "$x_{t+1}$. When $x_{t+1}$ is discrete, this is done with a linear and\n",
    "then softmax layet. When $x_{t+1}$ is continuous, it can be done with\n",
    "just a linear layer.\n",
    "\n",
    "## Why?\n",
    "\n",
    "The architecture of transformers developed step-by-step, combining\n",
    "ideas that seemed to work. The idea of an encoder grew out of\n",
    "embeddings and was originally combined with recurrent\n",
    "networks. Positional embedding and move away from recurrence was\n",
    "motivated by the difficulty with parallelizing recurrent\n",
    "models. Residual connections and layer norms help with gradient\n",
    "descent and vanishing gradient problems. Theoretical understanding of\n",
    "transformers has lagged behind their practical application, but theory\n",
    "is advancing rapidly. E.g. @bhattamishra2020 , etc\n",
    "\n",
    "# Example Code\n",
    "\n",
    "## Data\n",
    "\n",
    "For comparison, we will start by using the same Dylan example as in\n",
    "the recurrent neural network notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:23:39.440000Z",
     "iopub.status.busy": "2022-10-26T20:23:39.440000Z",
     "iopub.status.idle": "2022-10-26T20:23:59.489000Z",
     "shell.execute_reply": "2022-10-26T20:23:59.489000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: method definition for CuArray at /home/paul/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable A but does not use it.\n",
      "WARNING: method definition for CuArray at /home/paul/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable N+1 but does not use it.\n",
      "WARNING: method definition for CuArray at /home/paul/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable K but does not use it.\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching TextEncodeBase.Vocab(::Vector{Char}, ::Char)\n\u001b[0mClosest candidates are:\n\u001b[0m  TextEncodeBase.Vocab(::A, ::T, \u001b[91m::Int64\u001b[39m) where {T, A<:AbstractVector{T}} at ~/.julia/packages/TextEncodeBase/XKtFj/src/vocab.jl:8\n\u001b[0m  TextEncodeBase.Vocab(::AbstractVector) at ~/.julia/packages/TextEncodeBase/XKtFj/src/vocab.jl:21\n\u001b[0m  TextEncodeBase.Vocab(::AbstractVector, \u001b[91m::AbstractString\u001b[39m) at ~/.julia/packages/TextEncodeBase/XKtFj/src/vocab.jl:21",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching TextEncodeBase.Vocab(::Vector{Char}, ::Char)\n\u001b[0mClosest candidates are:\n\u001b[0m  TextEncodeBase.Vocab(::A, ::T, \u001b[91m::Int64\u001b[39m) where {T, A<:AbstractVector{T}} at ~/.julia/packages/TextEncodeBase/XKtFj/src/vocab.jl:8\n\u001b[0m  TextEncodeBase.Vocab(::AbstractVector) at ~/.julia/packages/TextEncodeBase/XKtFj/src/vocab.jl:21\n\u001b[0m  TextEncodeBase.Vocab(::AbstractVector, \u001b[91m::AbstractString\u001b[39m) at ~/.julia/packages/TextEncodeBase/XKtFj/src/vocab.jl:21",
      "",
      "Stacktrace:",
      " [1] Transformers.Basic.Vocabulary(list::Vector{Char}, unk::Char)",
      "   @ Transformers.Basic ~/.julia/packages/Transformers/bXd7C/src/basic/embeds/vocab.jl:18",
      " [2] top-level scope",
      "   @ In[2]:14",
      " [3] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "using ProgressMeter, JLD2\n",
    "import HTTP, Gumbo, Cascadia\n",
    "using StatsBase: wsample\n",
    "using Base.Iterators: partition\n",
    "using Transformers, Flux, CUDA\n",
    "\n",
    "text = collect(String(read(joinpath(docdir,\"jmd\",\"dylanchords.txt\"))))\n",
    "#startchar = 'α'\n",
    "#endchar = 'Ω' # any character not in original text\n",
    "unkchar = 'Ξ'\n",
    "#alphabet = [startchar, unique(text)..., endchar]\n",
    "alphabet = unique(text)\n",
    "N = length(alphabet)\n",
    "vocab = Transformers.Vocabulary(alphabet, unkchar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:23:59.753000Z",
     "iopub.status.busy": "2022-10-26T20:23:59.496000Z",
     "iopub.status.idle": "2022-10-26T20:24:02.850000Z",
     "shell.execute_reply": "2022-10-26T20:24:02.850000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_transformer (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enable_gpu(true)\n",
    "\n",
    "function create_transformer(modeldim, L; heads=1, feedforwardsize=4*modeldim, vocab=vocab)\n",
    "  embed = Transformers.Basic.Embed(modeldim,length(vocab))\n",
    "  pe = Transformers.Basic.PositionEmbedding(modeldim)\n",
    "  topo = @nntopo_str \"x → e → pe:(e,pe) → t → $L:t → logitp\"\n",
    "  m = Stack(topo,\n",
    "            embed,\n",
    "            pe,\n",
    "            (e,pe) -> e .+ pe,\n",
    "            [Transformer(modeldim, heads, feedforwardsize, act=relu, future=false, pdrop=0.1) for l ∈ 1:L]...,\n",
    "            Transformers.Basic.Positionwise(Dense(modeldim,length(vocab))))\n",
    "  return(m)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:24:02.855000Z",
     "iopub.status.busy": "2022-10-26T20:24:02.855000Z",
     "iopub.status.idle": "2022-10-26T20:24:04.656000Z",
     "shell.execute_reply": "2022-10-26T20:24:04.656000Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: vocab not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: vocab not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[4]:65",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "function cbgenerator(N, loss, printiter=Int(round(N/10)))\n",
    "  p = Progress(N, 1, \"Training\", 25)\n",
    "  i=0\n",
    "  function cb()\n",
    "    next!(p)\n",
    "    if (i % printiter==0)\n",
    "      @show loss()\n",
    "    end\n",
    "    i+=1\n",
    "  end\n",
    "  return(cb)\n",
    "end\n",
    "\n",
    "function sample(m, alphabet, len, seqlen)\n",
    "  m = cpu(m)\n",
    "  buf = IOBuffer()\n",
    "  c = 'w' #rand(alphabet)\n",
    "  cseq = vocab(collect(\"so much younger than that no\")) #Vector{Int}(undef,0)\n",
    "  ind2alpha = Dict(vocab(a) => a for a ∈ alphabet)\n",
    "  for i = 1:len\n",
    "    write(buf, c)\n",
    "    if (i < seqlen)\n",
    "      push!(cseq, vocab(c))\n",
    "    else\n",
    "      cseq[1:(end-1)] .= cseq[2:end]\n",
    "      cseq[end] = vocab(c)\n",
    "    end\n",
    "    c = ind2alpha[wsample(1:length(vocab), softmax(m(cseq)[:,end]))]\n",
    "  end\n",
    "  return String(take!(buf))\n",
    "end\n",
    "\n",
    "function createdata(vocab, text, seqlength, seqperbatch)\n",
    "  sequences = [vocab.(x) for x ∈ partition(text, seqlength)]\n",
    "  xy = [(s[1:(end-1)],Flux.onehot(vocab,s[2:end])) for s ∈ sequences]\n",
    "  if length(xy[end][1]) < length(xy[1][1])\n",
    "    pop!(xy)\n",
    "  end\n",
    "  xybatches = [ (hcat([z[1] for z ∈ p]...), cat([z[2] for z ∈ p]..., dims=3)) for p ∈ partition(xy, seqperbatch) ]\n",
    "  return(xybatches)\n",
    "end\n",
    "\n",
    "function train_model(m; data=data,\n",
    "                     modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-t.jld2\"),\n",
    "                     opt=opt, epochs=20 )\n",
    "  loss(xb, yb) = Flux.Losses.logitcrossentropy(m(xb),yb)\n",
    "  cb=cbgenerator(length(data),()->loss(first(data)...))\n",
    "\n",
    "  if isfile(modelfile)\n",
    "    @load modelfile cpum\n",
    "    #m = gpu(cpum)\n",
    "    m = cpum\n",
    "  else\n",
    "    @time Flux.train!(loss, Flux.params(m), data, opt, cb = cb)\n",
    "    println(\"Sampling after 1 epoch:\")\n",
    "    sample(m, alphabet, 1000, size(first(data)[1],1)) |> println\n",
    "\n",
    "    Flux.@epochs epochs Flux.train!(loss, Flux.params(m), data, opt, cb = cb)\n",
    "    cpum = cpu(m)\n",
    "    @save modelfile cpum\n",
    "  end\n",
    "  return(m)\n",
    "end\n",
    "\n",
    "m = create_transformer(64,4,heads=1, vocab=vocab) |> gpu\n",
    "data = createdata(vocab, text, 500, 50) |> gpu\n",
    "opt = RMSProp(0.001)\n",
    "m = train_model(m, data=data, modelfile=\"64d_4level_50e.jld2\", opt=opt, epochs=50)\n",
    "\n",
    "sample(m, alphabet, 1000, size(first(data)[1],1)) |> println"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks okay, but not quite as good as with RNNs. I did some\n",
    "ad-hoc exploration with alternate widths and depths. The one above\n",
    "seemed to work best.\n",
    "\n",
    "Qualitatively, these results are typical. Although transformers\n",
    "outperform RNNs when the underlying tokens are words or\n",
    "word-fragments, RNNs outperform transformers when the tokens are\n",
    "characters. Various modifications of transformers can make them\n",
    "competitive. See e.g. @wu2020 , @al2019 ,\n",
    "\n",
    "\n",
    "# Pre-trained Models\n",
    "\n",
    "Huggingface and Transformers.jl interface to it.\n",
    "\n",
    "## Transfer Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
