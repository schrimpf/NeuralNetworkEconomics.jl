@article{belloni2014,
author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
title = {Inference on Treatment Effects after Selection among High-Dimensional Controls†},
journal = {The Review of Economic Studies},
volume = {81},
number = {2},
pages = {608-650},
year = {2014},
doi = {10.1093/restud/rdt044},
URL = {http://dx.doi.org/10.1093/restud/rdt044},
}


@article{belloni2012,
author = {Belloni, A. and Chen, D. and Chernozhukov, V. and Hansen, C.},
title = {Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain},
journal = {Econometrica},
volume = {80},
number = {6},
pages = {2369-2429},
year = {2012},
url={https://doi.org/10.3982/ECTA9626}
}

@article{athey2017,
Author = {Athey, Susan and Imbens, Guido W.},
Title = {The State of Applied Econometrics: Causality and Policy Evaluation},
Journal = {Journal of Economic Perspectives},
Volume = {31},
Number = {2},
Year = {2017},
Month = {May},
Pages = {3-32},
DOI = {10.1257/jep.31.2.3},
URL = {http://www.aeaweb.org/articles?id=10.1257/jep.31.2.3}}


@article{mullainathan2017,
Author = {Mullainathan, Sendhil and Spiess, Jann},
Title = {Machine Learning: An Applied Econometric Approach},
Journal = {Journal of Economic Perspectives},
Volume = {31},
Number = {2},
Year = {2017},
Month = {May},
Pages = {87-106},
DOI = {10.1257/jep.31.2.87},
URL = {http://www.aeaweb.org/articles?id=10.1257/jep.31.2.87}}

@misc{chernozhukov2016,
Author = {Victor Chernozhukov and Denis Chetverikov and Mert Demirer and Esther Duflo and Christian Hansen and Whitney Newey and James Robins},
Title = {Double/Debiased Machine Learning for Treatment and Causal Parameters},
Year = {2016},
Eprint = {arXiv:1608.00060},
url = {https://arxiv.org/abs/1608.00060}
}


@article{chernozhukov2018,
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
title = {Double/debiased machine learning for treatment and structural parameters},
journal = {The Econometrics Journal},
volume = {21},
number = {1},
year = {2018},
pages = {C1-C68},
doi = {10.1111/ectj.12097},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097},
abstract = {Summary We revisit the classic semi-parametric problem of inference on a low-dimensional parameter θ0 in the presence of high-dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an -neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.}
}



@article{chernozhukov2017,
Author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney},
Title = {Double/Debiased/Neyman Machine Learning of Treatment Effects},
Journal = {American Economic Review},
Volume = {107},
Number = {5},
Year = {2017},
Month = {May},
Pages = {261-65},
DOI = {10.1257/aer.p20171038},
URL = {http://www.aeaweb.org/articles?id=10.1257/aer.p20171038}}


@article{athey2017b,
Author = {Athey, Susan and Imbens, Guido and Pham, Thai and Wager, Stefan},
Title = {Estimating Average Treatment Effects: Supplementary Analyses and Remaining Challenges},
Journal = {American Economic Review},
Volume = {107},
Number = {5},
Year = {2017},
Month = {May},
Pages = {278-81},
DOI = {10.1257/aer.p20171042},
URL = {http://www.aeaweb.org/articles?id=10.1257/aer.p20171042}}



@misc{chernozhukov2016b,
Author = {Victor Chernozhukov and Matt Goldman and Vira Semenova and Matt Taddy},
Title = {Orthogonal Machine Learning for Demand Estimation: High Dimensional Causal Inference in Dynamic Panels},
Year = {2017},
Eprint = {arXiv:1712.09988},
url = {https://arxiv.org/abs/1712.09988v2}
}


@article{hansen2018,
author = {Hansen, Stephen and McMahon, Michael and Prat, Andrea},
title = {Transparency and Deliberation Within the FOMC: A Computational Linguistics Approach*},
journal = {The Quarterly Journal of Economics},
volume = {133},
number = {2},
pages = {801-870},
year = {2018},
doi = {10.1093/qje/qjx045},
URL = {http://dx.doi.org/10.1093/qje/qjx045},
eprint = {/oup/backfile/content_public/journal/qje/133/2/10.1093_qje_qjx045/1/qjx045.pdf}
}







@article{chernozhukov2015,
author = {Chernozhukov, Victor and Hansen, Christian and Spindler, Martin},
title = {Valid Post-Selection and Post-Regularization Inference: An Elementary, General Approach},
journal = {Annual Review of Economics},
volume = {7},
number = {1},
pages = {649-688},
year = {2015},
doi = {10.1146/annurev-economics-012315-015826},

URL = {
        https://doi.org/10.1146/annurev-economics-012315-015826

},
eprint = {
        https://doi.org/10.1146/annurev-economics-012315-015826

}
,
    abstract = { We present an expository, general analysis of valid post-selection or post-regularization inference about a low-dimensional target parameter in the presence of a very high-dimensional nuisance parameter that is estimated using selection or regularization methods. Our analysis provides a set of high-level conditions under which inference for the low-dimensional parameter based on testing or point estimation methods will be regular despite selection or regularization biases occurring in the estimation of the high-dimensional nuisance parameter. A key element is the use of so-called immunized or orthogonal estimating equations that are locally insensitive to small mistakes in the estimation of the high-dimensional nuisance parameter. As an illustration, we analyze affine-quadratic models and specialize these results to a linear instrumental variables model with many regressors and many instruments. We conclude with a review of other developments in post-selection inference and note that many can be viewed as special cases of the general encompassing framework of orthogonal estimating equations provided in this article. }
}


@Inbook{belloni2011,
author="Belloni, Alexandre
and Chernozhukov, Victor",
editor="Alquier, Pierre
and Gautier, Eric
and Stoltz, Gilles",
title="High Dimensional Sparse Econometric Models: An Introduction",
bookTitle="Inverse Problems and High-Dimensional Estimation: Stats in the Ch{\^a}teau Summer School, August 31 - September 4, 2009",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="121--156",
abstract="In this chapter we discuss conceptually high dimensional sparse econometric models as well as estimation of these models using l                           1-penalization and post- l                           1-penalization methods. Focusing on linear and nonparametric regression frameworks, we discuss various econometric examples, present basic theoretical results, and illustrate the concepts and methods with Monte Carlo simulations and an empirical application. In the application, we examine and confirm the empirical validity of the Solow-Swan model for international economic growth.",
isbn="978-3-642-19989-9",
doi="10.1007/978-3-642-19989-9_3",
url="https://doi.org/10.1007/978-3-642-19989-9_3"
}

@book{friedman2008,
  title={The elements of statistical learning},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year={2009},
  publisher={Springer series in statistics},
  url = {https://web.stanford.edu/~hastie/ElemStatLearn/}
}


@article{belloni2017,
author = {Belloni, A. and Chernozhukov, V. and Fernández-Val, I. and Hansen, C.},
title = {Program Evaluation and Causal Inference With High-Dimensional Data},
journal = {Econometrica},
volume = {85},
number = {1},
year = {2017},
pages = {233-298},
keywords = {Machine learning, causality, Neyman orthogonality, heterogenous treatment effects, endogeneity, local average and quantile treatment effects, instruments, local effects of treatment on the treated, propensity score, Lasso, inference after model selection, moment-condition models, moment-condition models with a continuum of target parameters, Lasso and Post-Lasso with functional response data, randomized control trials},
doi = {10.3982/ECTA12723},
url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA12723},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA12723},
abstract = {In this paper, we provide efficient estimators and honest confidence bands for a variety of treatment effects including local average (LATE) and local quantile treatment effects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment effects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces efficient estimators and honest bands for (functional) average treatment effects (ATE) and quantile treatment effects (QTE). To make informative inference possible, we assume that key reduced-form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced-form functional parameters. We illustrate the use of the proposed methods with an application to estimating the effect of 401(k) eligibility and participation on accumulated assets. The results on program evaluation are obtained as a consequence of more general results on honest inference in a general moment-condition framework, which arises from structural equation models in econometrics. Here, too, the crucial ingredient is the use of orthogonal moment conditions, which can be constructed from the initial moment conditions. We provide results on honest inference for (function-valued) parameters within this general framework where any high-quality, machine learning methods (e.g., boosted trees, deep neural networks, random forest, and their aggregated and hybrid versions) can be used to learn the nonparametric/high-dimensional components of the model. These include a number of supporting auxiliary results that are of major independent interest: namely, we (1) prove uniform validity of a multiplier bootstrap, (2) offer a uniformly valid functional delta method, and (3) provide results for sparsity-based estimation of regression functions for function-valued outcomes.}
}


@Manual{grf,
    title = {grf: Generalized Random Forests (Beta)},
    author = {Julie Tibshirani and Susan Athey and Stefan Wager and Rina Friedberg and Luke Miner and Marvin Wright},
    year = {2018},
    note = {R package version 0.10.0},
    url = {https://CRAN.R-project.org/package=grf},
  }

  @Article{hdm,
    title = {{hdm}: High-Dimensional Metrics},
    author = {Victor Chernozhukov and Chris Hansen and Martin Spindler},
    journal = {R Journal},
    year = {2016},
    volume = {8},
    number = {2},
    pages = {185-199},
    url = {https://journal.r-project.org/archive/2016/RJ-2016-040/index.html},
  }

@misc{athey2016,
Author = {Susan Athey and Julie Tibshirani and Stefan Wager},
Title = {Generalized Random Forests},
Year = {2016},
Eprint = {arXiv:1610.01271},
url = {https://arxiv.org/abs/1610.01271}
}

@article{wager2018,
author = {Stefan Wager and Susan Athey},
title = {Estimation and Inference of Heterogeneous Treatment Effects using Random Forests},
journal = {Journal of the American Statistical Association},
volume = {0},
number = {0},
pages = {1-15},
year  = {2018},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2017.1319839},

URL = {
        https://doi.org/10.1080/01621459.2017.1319839

},
eprint = {
        https://doi.org/10.1080/01621459.2017.1319839

}

}

@article{belloni2014jep,
Author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
Title = {High-Dimensional Methods and Inference on Structural and Treatment Effects},
Journal = {Journal of Economic Perspectives},
Volume = {28},
Number = {2},
Year = {2014},
Month = {May},
Pages = {29-50},
DOI = {10.1257/jep.28.2.29},
URL = {http://www.aeaweb.org/articles?id=10.1257/jep.28.2.29}}

@misc{athey2015,
  title={Lectures on Machine Learning},
  author={Athey, Susan and Imbens, Guido},
  year={2015},
  publisher={NBER Summer Institute},
  url = {http://www.nber.org/econometrics_minicourse_2015/}
}

@misc{athey2018,
  title={Machine learning and econometrics},
  author={Athey, Susan and Imbens, Guido},
  year={2018},
  publisher={AEA Continuing Education },
  url = {https://www.aeaweb.org/conference/cont-ed/2018-webcasts}
}



@article {athey2016b,
	author = {Athey, Susan and Imbens, Guido},
	title = {Recursive partitioning for heterogeneous causal effects},
	volume = {113},
	number = {27},
	pages = {7353--7360},
	year = {2016},
	doi = {10.1073/pnas.1510489113},
	publisher = {National Academy of Sciences},
	abstract = {In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without {\textquotedblleft}sparsity{\textquotedblright} assumptions. We propose an {\textquotedblleft}honest{\textquotedblright} approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the {\textquotedblleft}ground truth{\textquotedblright} for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90\% confidence intervals, whereas coverage ranges between 74\% and 84\% for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7{\textendash}22\%.},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/113/27/7353},
	eprint = {http://www.pnas.org/content/113/27/7353.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{breiman2001,
  title={Statistical modeling: The two cultures (with comments and a rejoinder by the author)},
  author={Breiman, Leo and others},
  journal={Statistical science},
  volume={16},
  number={3},
  pages={199--231},
  year={2001},
  publisher={Institute of Mathematical Statistics},
  url={https://projecteuclid.org/euclid.ss/1009213726}
}

@book{ap2009,
  title={Mostly harmless econometrics: An empiricist's companion},
  author={Angrist, J.D. and Pischke, J.S.},
  year={2009},
  publisher={Princeton University Press}
}







@article{imbens2004,
author = {Imbens, Guido W.},
title = {Nonparametric Estimation of Average Treatment Effects Under Exogeneity: A Review},
journal = {The Review of Economics and Statistics},
volume = {86},
number = {1},
pages = {4-29},
year = {2004},
doi = {10.1162/003465304323023651},

URL = {
        https://doi.org/10.1162/003465304323023651

},
eprint = {
        https://doi.org/10.1162/003465304323023651

}
,
    abstract = { Recently there has been a surge in econometric work focusing on estimating average treatment effects under various sets of assumptions. One strand of this literature has developed methods for estimating average treatment effects for a binary treatment under assumptions variously described as exogeneity, unconfoundedness, or selection on observables. The implication of these assumptions is that systematic (for example, average or distributional) differences in outcomes between treated and control units with the same values for the covariates are attributable to the treatment. Recent analysis has considered estimation and inference for average treatment effects under weaker assumptions than typical of the earlier literature by avoiding distributional and functional-form assumptions. Various methods of semiparametric estimation have been proposed, including estimating the unknown regression functions, matching, methods using the propensity score such as weighting and blocking, and combinations of these approaches. In this paper I review the state of this literature and discuss some of its unanswered questions, focusing in particular on the practical implementation of these methods, the plausibility of this exogeneity assumption in economic applications, the relative performance of the various semiparametric estimators when the key assumptions (unconfoundedness and overlap) are satisfied, alternative estimands such as quantile treatment effects, and alternate methods such as Bayesian inference. }
}


@article{imbens2015,
author = {Imbens, Guido W.},
title = {Matching Methods in Practice: Three Examples},
volume = {50},
number = {2},
pages = {373-419},
year = {2015},
doi = {10.3368/jhr.50.2.373},
abstract ={There is a large theoretical literature on methods for estimating causal effects under unconfoundedness, exogeneity, or selection-on-observables type assumptions using matching or propensity score methods. Much of this literature is highly technical and has not made inroads into empirical practice where many researchers continue to use simple methods such as ordinary least squares regression even in settings where those methods do not have attractive properties. In this paper, I discuss some of the lessons for practice from the theoretical literature and provide detailed recommendations on what to do. I illustrate the recommendations with three detailed applications.},
URL = {https://muse.jhu.edu/article/581179/pdf},
eprint = {http://jhr.uwpress.org/content/50/2/373.full.pdf+html},
journal = {Journal of Human Resources}
}

@book{james2013,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume={112},
  year={2013},
  publisher={Springer},
  url={http://www-bcf.usc.edu/%7Egareth/ISL/}
}

@article{donohue2001,
author = {Donohue, III ,John J. and Levitt, Steven D.},
title = {The Impact of Legalized Abortion on Crime*},
journal = {The Quarterly Journal of Economics},
volume = {116},
number = {2},
pages = {379-420},
year = {2001},
doi = {10.1162/00335530151144050},
URL = {http://dx.doi.org/10.1162/00335530151144050},
eprint = {/oup/backfile/content_public/journal/qje/116/2/10.1162/00335530151144050/2/116-2-379.pdf}
}

@article{connors1996,
author = {Alfred F. Connors and Theodore Speroff and  Neal V. Dawson and  Charles Thomas and  Frank E. Harrell Jr and  Douglas Wagner and  Norman Desbiens and  Lee Goldman, MPH and  Albert W. Wu and  Robert M. Califf and  William J. Fulkerson Jr and  Humberto Vidaillet and  Steven Broste and  Paul Bellamy and  Joanne Lynn and  William A. Knaus},
title = {The effectiveness of right heart catheterization in the initial care of critically ill patients},
journal = {JAMA},
volume = {276},
number = {11},
pages = {889-897},
year = {1996},
doi = {10.1001/jama.1996.03540110043030},
URL = {http://dx.doi.org/10.1001/jama.1996.03540110043030},
eprint = {/data/journals/jama/9799/jama_276_11_030.pdf}
}

@article{rosenbaum1983,
author = {Rosenbaum, Paul R. and Rubin, Donald B.},
title = {The central role of the propensity score in observational studies for causal effects},
journal = {Biometrika},
volume = {70},
number = {1},
pages = {41-55},
year = {1983},
doi = {10.1093/biomet/70.1.41},
URL = {http://dx.doi.org/10.1093/biomet/70.1.41},
eprint = {/oup/backfile/content_public/journal/biomet/70/1/10.1093/biomet/70.1.41/2/70-1-41.pdf}
}

@misc{athey2018b,
author={Susan Athey},
title={The Impact of Machine Learning on Economics},
year={2018},
url={http://www.nber.org/chapters/c14009.pdf}
}

@article{lee2016,
author = "Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor, Jonathan E.",
doi = "10.1214/15-AOS1371",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "06",
number = "3",
pages = "907--927",
publisher = "The Institute of Mathematical Statistics",
title = "Exact post-selection inference, with application to the lasso",
url = "https://doi.org/10.1214/15-AOS1371",
volume = "44",
year = "2016"
}

@article{taylor2017,
author = {Taylor, Jonathan and Tibshirani, Robert},
title = {Post-selection inference for -penalized likelihood models},
journal = {Canadian Journal of Statistics},
volume = {46},
number = {1},
year = {2017},
pages = {41-61},
keywords = {Logistic regression, Cox model, P-values, MSC2010: Primary 97K70, Secondary 97K80},
doi = {10.1002/cjs.11313},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cjs.11313},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cjs.11313},
abstract = {Abstract We present a new method for post-selection inference for (lasso)'penalized likelihood models, including generalized regression models. Our approach generalizes the post-selection framework presented in Lee et al. (2013). The method provides P-values and confidence intervals that are asymptotically valid, conditional on the inherent selection done by the lasso. We present applications of this work to (regularized) logistic regression, Cox's proportional hazards model, and the graphical lasso. We do not provide rigorous proofs here of the claimed results, but rather conceptual and theoretical sketches. The Canadian Journal of Statistics 46: 41–61; 2018 © 2017 Statistical Society of Canada}
}

@article{caner2018,
title = "Asymptotically honest confidence regions for high dimensional parameters by the desparsified conservative Lasso",
 journal = "Journal of Econometrics",
volume = "203",
number = "1",
pages = "143 - 168",
year = "2018",
issn = "0304-4076",
doi =
"https://doi.org/10.1016/j.jeconom.2017.11.005",
url =
"http://www.sciencedirect.com/science/article/pii/S0304407617302282",
author = "Mehmet Caner and Anders Bredahl Kock",
keywords = "Conservative Lasso, Honest inference, High-dimensional data, Uniform inference, Confidence intervals, Tests"
}

    @article{angrist1991,
     jstor_articletype = {research-article},
     title = {Does Compulsory School Attendance Affect Schooling and Earnings?},
     author = {Angrist, Joshua D. and Krueger, Alan B.},
     journal = {The Quarterly Journal of Economics},
     jstor_issuetitle = {},
     volume = {106},
     number = {4},
     jstor_formatteddate = {Nov., 1991},
     pages = {pp. 979-1014},
     url = {http://www.jstor.org/stable/2937954},
     ISSN = {00335533},
     abstract = {We establish that season of birth is related to educational attainment because of school start age policy and compulsory school attendance laws. Individuals born in the beginning of the year start school at an older age, and can therefore drop out after completing less schooling than individuals born near the end of the year. Roughly 25 percent of potential dropouts remain in school because of compulsory schooling laws. We estimate the impact of compulsory schooling on earnings by using quarter of birth as an instrument for education. The instrumental variables estimate of the return to education is close to the ordinary least squares estimate, suggesting that there is little bias in conventional estimates.},
     language = {English},
     year = {1991},
     publisher = {Oxford University Press},
     copyright = {Copyright © 1991 Oxford University Press},
    }


@misc{chetverikov2017,
Author = {Denis Chetverikov and Zhipeng Liao and Victor Chernozhukov},
Title = {On cross-validated Lasso},
Year = {2016},
url = {https://arxiv.org/abs/1605.02214},
Eprint = {arXiv:1605.02214},
}

@article{biau2012,
  title={Analysis of a random forests model},
  author={Biau, Gérard},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Apr},
  pages={1063--1095},
  url={http://www.jmlr.org/papers/v13/biau12a.html},
  year={2012}
}


@misc{wager2015,
Author = {Stefan Wager and Guenther Walther},
Title = {Adaptive Concentration of Regression Trees, with Application to Random Forests},
Year = {2015},
url = {https://arxiv.org/abs/1503.06388},
Eprint = {arXiv:1503.06388},
}

@article{abadie2003,
title = "Semiparametric instrumental variable estimation of treatment response models",
journal = "Journal of Econometrics",
volume = "113",
number = "2",
pages = "231 - 263",
year = "2003",
issn = "0304-4076",
doi = "https://doi.org/10.1016/S0304-4076(02)00201-4",
url = "http://www.sciencedirect.com/science/article/pii/S0304407602002014",
author = "Alberto Abadie",
keywords = "Treatment effects, Semiparametric estimation, Compliers, 401(k)"
}




@InProceedings{hartford2017,
  title = 	 {Deep {IV}: A Flexible Approach for Counterfactual Prediction},
  author = 	 {Jason Hartford and Greg Lewis and Kevin Leyton-Brown and Matt Taddy},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1414--1423},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/hartford17a.html},
  abstract = 	 {Counterfactual prediction requires understanding causal relationships between so-called treatment and outcome variables. This paper provides a recipe for augmenting deep learning methods to accurately characterize such relationships in the presence of instrument variables (IVs) – sources of treatment randomization that are conditionally independent from the outcomes. Our IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework allows us to take advantage of off-the-shelf supervised learning techniques to estimate causal effects by adapting the loss function. Experiments show that it outperforms existing machine learning approaches.}
}

@book{efron2016,
  title={Computer age statistical inference},
  author={Efron, Bradley and Hastie, Trevor},
  volume={5},
  year={2016},
  url={https://web.stanford.edu/~hastie/CASI/},
  publisher={Cambridge University Press}
}

@article{stone1982,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2240707},
 abstract = {Consider a p-times differentiable unknown regression function θ of a d-dimensional measurement variable. Let T(θ) denote a derivative of θ of order m and set r = (p - m)/(2p + d). Let T̂n denote an estimator of T(θ) based on a training sample of size n, and let | T̂n - T(θ)|q be the usual Lq norm of the restriction of T̂n - T(θ) to a fixed compact set. Under appropriate regularity conditions, it is shown that the optimal rate of convergence for | T̂n - T(θ)|q is n-r if $0 < q < \infty$; while (n-1 log n)r is the optimal rate if q = ∞.},
 author = {Charles J. Stone},
 journal = {The Annals of Statistics},
 number = {4},
 pages = {1040--1053},
 publisher = {Institute of Mathematical Statistics},
 title = {Optimal Global Rates of Convergence for Nonparametric Regression},
 volume = {10},
 year = {1982}
}

@article{li1989,
author = "Li, Ker-Chau",
doi = "10.1214/aos/1176347253",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "09",
number = "3",
pages = "1001--1008",
publisher = "The Institute of Mathematical Statistics",
title = "Honest Confidence Regions for Nonparametric Regression",
url = "https://doi.org/10.1214/aos/1176347253",
volume = "17",
year = "1989"
}

@article{nickl2013,
author = "Nickl, Richard and van de Geer, Sara",
doi = "10.1214/13-AOS1170",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "12",
number = "6",
pages = "2852--2876",
publisher = "The Institute of Mathematical Statistics",
title = "Confidence sets in sparse regression",
url = "https://doi.org/10.1214/13-AOS1170",
volume = "41",
year = "2013"
}

@article{qsw2018,
Author = {Qin, Bei and Strömberg, David and Wu, Yanhui},
Title = {Media Bias in China},
Journal = {American Economic Review},
Volume = {108},
Number = {9},
Year = {2018},
Month = {September},
Pages = {2442-76},
DOI = {10.1257/aer.20170947},
URL = {http://www.aeaweb.org/articles?id=10.1257/aer.20170947}}

@article{bchk2016,
author = {Alexandre Belloni and Victor Chernozhukov and Christian Hansen and Damian Kozbur},
title = {Inference in High-Dimensional Panel Models With an Application to Gun Control},
journal = {Journal of Business \& Economic Statistics},
volume = {34},
number = {4},
pages = {590-605},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/07350015.2015.1102733},

URL = {
        https://doi.org/10.1080/07350015.2015.1102733

},
eprint = {
        https://doi.org/10.1080/07350015.2015.1102733

}

}


@article{speckman1985,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2241119},
 abstract = {Linear estimation is considered in nonparametric regression models of the form Yi = f(xi) + εi, xi ∈ (a, b), where the zero mean errors are uncorrelated with common variance σ2 and the response function f is assumed only to have a bounded square integrable qth derivative. The linear estimator which minimizes the maximum mean squared error summed over the observation points is derived, and the exact minimax rate of convergence is obtained. For practical problems where bounds on |f(q)|2 and σ2 may be unknown, generalized cross-validation is shown to give an adaptive estimator which achieves the minimax optimal rate under the additional assumption of normality.},
 author = {Paul Speckman},
 journal = {The Annals of Statistics},
 number = {3},
 pages = {970--983},
 publisher = {Institute of Mathematical Statistics},
 title = {Spline Smoothing and Optimal Rates of Convergence in Nonparametric Regression Models},
 volume = {13},
 year = {1985}
}

@article{donoho1995,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2291512},
 abstract = {We attempt to recover a function of unknown smoothness from noisy sampled data. We introduce a procedure, SureShrink, that suppresses noise by thresholding the empirical wavelet coefficients. The thresholding is adaptive: A threshold level is assigned to each dyadic resolution level by the principle of minimizing the Stein unbiased estimate of risk (Sure) for threshold estimates. The computational effort of the overall procedure is order N · log(N) as a function of the sample size N. SureShrink is smoothness adaptive: If the unknown function contains jumps, then the reconstruction (essentially) does also; if the unknown function has a smooth piece, then the reconstruction is (essentially) as smooth as the mother wavelet will allow. The procedure is in a sense optimally smoothness adaptive: It is near minimax simultaneously over a whole interval of the Besov scale; the size of this interval depends on the choice of mother wavelet. We know from a previous paper by the authors that traditional smoothing methods--kernels, splines, and orthogonal series estimates--even with optimal choices of the smoothing parameter, would be unable to perform in a near-minimax way over many spaces in the Besov scale. Examples of SureShrink are given. The advantages of the method are particularly evident when the underlying function has jump discontinuities on a smooth background.},
 author = {David L. Donoho and Iain M. Johnstone},
 journal = {Journal of the American Statistical Association},
 number = {432},
 pages = {1200--1224},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Adapting to Unknown Smoothness via Wavelet Shrinkage},
 volume = {90},
 year = {1995}
}

@misc{friedberg2018,
Author = {Rina Friedberg and Julie Tibshirani and Susan Athey and Stefan Wager},
Title = {Local Linear Forests},
Year = {2018},
url = {https://arxiv.org/abs/1807.11408},
Eprint = {arXiv:1807.11408},
}

@article{hornik1989,
title = "Multilayer feedforward networks are universal approximators",
journal = "Neural Networks",
volume = "2",
number = "5",
pages = "359 - 366",
year = "1989",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(89)90020-8",
url = "http://www.sciencedirect.com/science/article/pii/0893608089900208",
author = "Kurt Hornik and Maxwell Stinchcombe and Halbert White",
keywords = "Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks"
}

@ARTICLE{chen1999,
author={Xiaohong Chen and H. White},
journal={IEEE Transactions on Information Theory},
title={Improved rates and asymptotic normality for nonparametric neural network estimators},
year={1999},
volume={45},
number={2},
pages={682-691},
keywords={feedforward neural nets;approximation theory;estimation theory;convergence of numerical methods;transfer functions;time series;learning (artificial intelligence);statistical analysis;asymptotic normality;nonparametric neural network estimators;improved approximation rate;Sobolev norm;single hidden layer feedforward ANN;artificial neural networks;hidden units;nonsigmoid activation functions;target function;smoothness conditions;dimension;method of sieves;root-mean-square convergence rates;sample size;training examples;i.i.d. data;uniform mixing;regular stationary time series data;/spl beta/-mixing;plug-in estimates;smooth functionals;nonlinear time series;sieve estimators;multivariate target functions;conditional mean;conditional quantile;joint density;conditional density;semiparametric model coefficient;average derivative estimators;statistical inference;Neural networks;Artificial neural networks;Kernel;Feedforward neural networks;Associate members;Convergence;Finance;Statistics;Gaussian distribution},
doi={10.1109/18.749011},
ISSN={0018-9448},
month={March},}

@ARTICLE{barron1993,
author={A. R. Barron},
journal={IEEE Transactions on Information Theory},
title={Universal approximation bounds for superpositions of a sigmoidal function},
year={1993},
volume={39},
number={3},
pages={930-945},
keywords={approximation theory;error analysis;feedforward neural nets;Fourier transforms;function approximation;information theory;universal approximation bounds;superpositions;sigmoidal function;artificial neural networks;feedforward networks;sigmoidal nonlinearities;integrated squared error;magnitude distribution;Fourier transform;nonlinear parameters;series expansions;linear combination;parsimony;high-dimensional settings;Artificial neural networks;Fourier transforms;Approximation error;Feeds;Linear approximation;Neural networks;Feedforward neural networks;Information theory;Statistics;Statistical distributions},
doi={10.1109/18.256500},
ISSN={0018-9448},
month={May},}


@techreport{cddf2018,
 title = "Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experimentsxo",
 author = "Chernozhukov, Victor and Demirer, Mert and Duflo, Esther and Fernández-Val, Iván",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "24678",
 year = "2018",
 month = "June",
 doi = {10.3386/w24678},
 URL = "http://www.nber.org/papers/w24678",
 abstract = {We propose strategies to estimate and make inference on key features of heterogeneous effects in randomized experiments. These key features include best linear predictors of the effects using machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. The approach is valid in high dimensional settings, where the effects are proxied by machine learning methods. We post-process these proxies into the estimates of the key features. Our approach is generic, it can be used in conjunction with penalized methods, deep and shallow neural networks, canonical and new random forests, boosted trees, and ensemble methods. It does not rely on strong assumptions. In particular, we don’t require conditions for consistency of the machine learning methods. Estimation and inference relies on repeated data splitting to avoid overfitting and achieve validity. For inference, we take medians of p-values and medians of confidence intervals, resulting from many different data splits, and then adjust their nominal level to guarantee uniform validity. This variational inference method is shown to be uniformly valid and quantifies the uncertainty coming from both parameter estimation and data splitting. An empirical application to the impact of micro-credit on economic development illustrates the use of the approach in randomized experiments.},
}

@misc{bcchk2018,
Author = {Alexandre Belloni and Victor Chernozhukov and Denis Chetverikov and Christian Hansen and Kengo Kato},
Title = {High-Dimensional Econometrics and Regularized GMM},
Year = {2018},
Eprint = {arXiv:1806.01888},
url={https://arxiv.org/abs/1806.01888}
}

@article{bccw2018,
author = "Belloni, Alexandre and Chernozhukov, Victor and Chetverikov, Denis and Wei, Ying",
doi = "10.1214/17-AOS1671",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "12",
number = "6B",
pages = "3643--3675",
publisher = "The Institute of Mathematical Statistics",
title = "Uniformly valid post-regularization confidence regions for many functional parameters in z-estimation framework",
url = "https://doi.org/10.1214/17-AOS1671",
volume = "46",
year = "2018"
}

@misc{cnr2018,
Author = {Victor Chernozhukov and Whitney Newey and James Robins and Rahul Singh},
Title = {Double/De-Biased Machine Learning of Global and Local Parameters Using Regularized Riesz Representers},
                  Year = {2018},
                  Eprint = {arXiv:1802.08667},
                  url={https://arxiv.org/abs/1802.08667}
                  }


@incollection{cfr2007,
title = "Chapter 77 Linear Inverse Problems in Structural Econometrics Estimation Based on Spectral Decomposition and Regularization",
editor = "James J. Heckman and Edward E. Leamer",
series = "Handbook of Econometrics",
publisher = "Elsevier",
volume = "6",
pages = "5633 - 5751",
year = "2007",
issn = "1573-4412",
doi = "https://doi.org/10.1016/S1573-4412(07)06077-1",
url = "http://www.sciencedirect.com/science/article/pii/S1573441207060771",
author = "Marine Carrasco and Jean-Pierre Florens and Eric Renault",
keywords = "additive models, generalized method of moments, instrumental variables, integral equation, many regressors, nonparametric estimation, Tikhonov and Landweber–Fridman regularizations"
}

@article{triyana2016,
Author = {Triyana, Margaret},
Title = {Do Health Care Providers Respond to Demand-Side Incentives? Evidence from Indonesia},
Journal = {American Economic Journal: Economic Policy},
Volume = {8},
Number = {4},
Year = {2016},
Month = {November},
Pages = {255-88},
DOI = {10.1257/pol.20140048},
URL = {http://www.aeaweb.org/articles?id=10.1257/pol.20140048}}

@misc{awm1,
Author={Susan Athey and Stefan Wager and Nicolaj Nørgaard Mühlbach},
Title={Exploring Causal Inference in Experimental and Observational
                  Studies - Part 1},
year={2018},
Month={April},
url={https://drive.google.com/drive/folders/1TX3zlqf6JBAFacjFR4af1cNAqW8JQXHE}
}

@misc{awm2,
Author={Susan Athey and Stefan Wager and Nicolaj Nørgaard Mühlbach},
Title={Estimation of Heterogeneous Treatment Effects
and Treatment Policies},
year={2018},
Month={May},
url={https://drive.google.com/drive/folders/1TX3zlqf6JBAFacjFR4af1cNAqW8JQXHE}
}

@techreport{alatas2011,
  title={Program Keluarga Harapan : impact evaluation of Indonesia's Pilot Household Conditional Cash Transfer Program},
  author={Alatas, Vivi and Cahyadi, Nur and Ekasari, Elisabeth and Harmoun, Sarah and Hidayat, Budi and Janz, Edgar and Jellema, Jon and Tuhiman, H and Wai-Poi, M},
  institution={World Bank},
  url={http://documents.worldbank.org/curated/en/589171468266179965/Program-Keluarga-Harapan-impact-evaluation-of-Indonesias-Pilot-Household-Conditional-Cash-Transfer-Program},
  year={2011}
}


@incollection{chen2007,
title = "Chapter 76 Large Sample Sieve Estimation of Semi-Nonparametric Models",
editor = "James J. Heckman and Edward E. Leamer",
series = "Handbook of Econometrics",
publisher = "Elsevier",
volume = "6",
pages = "5549 - 5632",
year = "2007",
issn = "1573-4412",
doi = "https://doi.org/10.1016/S1573-4412(07)06076-X",
url = "http://www.sciencedirect.com/science/article/pii/S157344120706076X",
author = "Xiaohong Chen",
keywords = "sieve extremum estimation, series, sieve minimum distance, semiparametric two-step estimation, endogeneity in semi-nonparametric models"
}

@book{klok2019,
title={Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence},
author={Hayden Klok and Yoni Nazarathy},
year={2019},
url={https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf},
publisher={DRAFT}
}

@book{goodfellow2016,
year={2016},
title={Deep Learning},
author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
publisher={MIT Press},
url={http://www.deeplearningbook.org}}
}

@misc{farrel2018,
Author = {Max H. Farrell and Tengyuan Liang and Sanjog Misra},
Title = {Deep Neural Networks for Estimation and Inference},
Year = {2018},
url = {https://arxiv.org/abs/1809.09953},
Eprint = {arXiv:1809.09953},
}

@article{farrel2021,
author = {Farrell, Max H. and Liang, Tengyuan and Misra, Sanjog},
title = {Deep Neural Networks for Estimation and Inference},
journal = {Econometrica},
volume = {89},
number = {1},
pages = {181-213},
keywords = {Deep learning, neural networks, rectified linear unit, nonasymptotic bounds, convergence rates, semiparametric inference, treatment effects, program evaluation},
doi = {https://doi.org/10.3982/ECTA16901},
url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA16901},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA16901},
abstract = {We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second-step inference after first-step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now-common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed-width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression-type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing.},
year = {2021}
}



@inbook{qeclassify,
author="Quentin Batista and Chase Coleman and Spencer Lyon and Jesse Perla and Thomas Sargent and Paul Schrimpf and Natasha Watkins",
title="Classification",
bookTitle="QuantEcon DataScience: Introduction to Economic Modeling and Data Science",
year="2019",
url="https://datascience.quantecon.org/applications/classification.html"
}

@article{ciresan2010,
Author = {Dan Claudiu Ciresan and Ueli Meier and Luca Maria Gambardella and Juergen Schmidhuber},
Title = {Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition},
Year = {2010},
Eprint = {arXiv:1003.0358},
Howpublished = {Neural Computation, Volume 22, Number 12, December 2010},
Doi = {10.1162/NECO_a_00052},
}

@misc{rackauckas2019conv,
author={Chris Rackauckas},
title={PDEs, Convolutions, and the Mathematics of Locality},
year={2019},
url={https://mitmath.github.io/18337/lecture14/pdes_and_convolutions}
}

@article{siegelmann1991,
title = "Turing computability with neural nets",
journal = "Applied Mathematics Letters",
volume = "4",
number = "6",
pages = "77 - 80",
year = "1991",
issn = "0893-9659",
doi = "https://doi.org/10.1016/0893-9659(91)90080-F",
url = "http://www.sciencedirect.com/science/article/pii/089396599190080F",
author = "Hava T. Siegelmann and Eduardo D. Sontag",
abstract = "This paper shows the existence of a finite neural network, made up of sigmoidal neurons, which simulates a universal Turing machine. It is composed of less than 105 synchronously evolving processors, interconnected linearly. High-order connections are not required."
}

@inproceedings{siegelmann1992,
 author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
 title = {On the Computational Power of Neural Nets},
 booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
 series = {COLT '92},
 year = {1992},
 isbn = {0-89791-497-X},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {440--449},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/130385.130432},
 doi = {10.1145/130385.130432},
 acmid = {130432},
 publisher = {ACM},
 address = {New York, NY, USA},
}


@incollection{graves2011,
title = {Practical Variational Inference for Neural Networks},
author = {Graves, Alex},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
pages = {2348--2356},
year = {2011},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf}
}

@inproceedings{miao2016,
  title={Neural variational inference for text processing},
  author={Miao, Yishu and Yu, Lei and Blunsom, Phil},
  booktitle={International conference on machine learning},
  pages={1727--1736},
  year={2016},
  url={http://www.jmlr.org/proceedings/papers/v48/miao16.pdf}
}

@misc{ceinr2016,
Author = {Victor Chernozhukov and Juan Carlos Escanciano and Hidehiko Ichimura and Whitney K. Newey and James M. Robins},
Title = {Locally Robust Semiparametric Estimation},
Year = {2016},
url={https://arxiv.org/pdf/1608.00033.pdf},
Eprint = {arXiv:1608.00033},
}

@article{bhattamishra2020,
  author    = {Satwik Bhattamishra and
               Arkil Patel and
               Navin Goyal},
  title     = {On the Computational Power of Transformers and Its Implications in
               Sequence Modeling},
  journal   = {CoRR},
  volume    = {abs/2006.09286},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.09286},
  eprinttype = {arXiv},
  eprint    = {2006.09286},
  timestamp = {Wed, 17 Jun 2020 14:28:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-09286.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vaswani2017,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{jastrzebski2017,
  author    = {Stanislaw Jastrzebski and
               Devansh Arpit and
               Nicolas Ballas and
               Vikas Verma and
               Tong Che and
               Yoshua Bengio},
  title     = {Residual Connections Encourage Iterative Inference},
  journal   = {CoRR},
  volume    = {abs/1710.04773},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.04773},
  eprinttype = {arXiv},
  eprint    = {1710.04773},
  timestamp = {Mon, 13 Aug 2018 16:48:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-04773.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ba2016,
  doi = {10.48550/ARXIV.1607.06450},

  url = {https://arxiv.org/abs/1607.06450},

  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},

  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Layer Normalization},

  publisher = {arXiv},

  year = {2016},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{wu2020,
  author    = {Shijie Wu and
               Ryan Cotterell and
               Mans Hulden},
  title     = {Applying the Transformer to Character-level Transduction},
  journal   = {CoRR},
  volume    = {abs/2005.10213},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.10213},
  eprinttype = {arXiv},
  eprint    = {2005.10213},
  timestamp = {Fri, 22 May 2020 16:21:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-10213.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{al2019, title={Character-Level
                  Language Modeling with Deeper Self-Attention},
                  volume={33},
                  url={https://ojs.aaai.org/index.php/AAAI/article/view/4182},
                  DOI={10.1609/aaai.v33i01.33013159},
                  abstractNote={&lt;p&gt;LSTMs and other RNN variants
                  have shown strong performance on character-level
                  language modeling. These models are typically
                  trained using truncated backpropagation through
                  time, and it is common to assume that their success
                  stems from their ability to remember long-term
                  contexts. In this paper, we show that a deep
                  (64-layer) transformer model (Vaswani et al. 2017)
                  with fixed context outperforms RNN variants by a
                  large margin, achieving state of the art on two
                  popular benchmarks: 1.13 bits per character on text8
                  and 1.06 on enwik8. To get good results at this
                  depth, we show that it is important to add auxiliary
                  losses, both at intermediate network layers and
                  intermediate sequence positions.&lt;/p&gt;},
                  number={01}, journal={Proceedings of the AAAI
                  Conference on Artificial Intelligence},
                  author={Al-Rfou, Rami and Choe, Dokook and Constant,
                  Noah and Guo, Mandy and Jones, Llion}, year={2019},
                  month={Jul.}, pages={3159-3166}}