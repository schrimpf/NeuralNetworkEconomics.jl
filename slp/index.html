<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Introduction -  </title>
        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <link href="../assets/extra.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href=".."> </a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="..">Package Docs</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">ML in Economics <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../ml-intro/">Introduction</a>
</li>
                                    
<li >
    <a href="../ml-methods/">Methods</a>
</li>
                                    
<li >
    <a href="../ml-doubledebiased/">Inference</a>
</li>
                                    
<li >
    <a href="../mlExamplePKH/">Detecting heterogeneity</a>
</li>
                                    
<li >
    <a href="../ml-julia/">With Julia</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Neural Networks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li class="active">
    <a href="./">Introduction</a>
</li>
                                    
<li >
    <a href="../mlp/">Multi-Layer</a>
</li>
                                    
<li >
    <a href="../conv/">Convolutional</a>
</li>
                                    
<li >
    <a href="../rnn/">Recurrent</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../license/">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../ml-julia/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../mlp/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/schrimpf/NeuralNetworkEconomics.jl/edit/master/docs/slp.md"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#about-this-document">About this document</a></li>
        <li class="main "><a href="#introduction">Introduction</a></li>
            <li><a href="#additional-reading">Additional Reading</a></li>
        <li class="main "><a href="#single-layer-neural-networks">Single Layer Neural Networks</a></li>
        <li class="main "><a href="#training">Training</a></li>
            <li><a href="#low-level">Low level</a></li>
            <li><a href="#chain-interface">Chain interface</a></li>
            <li><a href="#initial-values">Initial values</a></li>
            <li><a href="#rectified-linear">Rectified linear</a></li>
            <li><a href="#stochastic-gradient-descent">Stochastic Gradient descent</a></li>
        <li class="main "><a href="#rate-of-convergence">Rate of convergence</a></li>
        <li class="main "><a href="#references-references">References [references]</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a></p>
<h3 id="about-this-document">About this document<a class="headerlink" href="#about-this-document" title="Permanent link">&para;</a></h3>
<p>This document was created using Weave.jl. The code is available in <a href="https://github.com/schrimpf/NeuralNetworkEconomics.jl">on
github</a>. The same
document generates both static webpages and associated <a href="../slp.ipynb">jupyter
notebook</a>.</p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
</script>
</p>
<h1 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<p>Neural networks, especially deep neural networks, have come to dominate
some areas of machine learning. Neural networks are especially prominent
in natural language processing, image classification, and reinforcement
learning. This documents gives a brief introduction to neural networks.</p>
<p>Examples in this document will use
<a href="https://fluxml.ai/Flux.jl/stable/"><code>Flux.jl</code></a>. An alternative Julia
package for deep learning is
<a href="https://denizyuret.github.io/Knet.jl/latest/"><code>Knet.jl</code></a>. There is a
good discussion comparing Flux and Knet <a href="https://discourse.julialang.org/t/state-of-deep-learning-in-julia/28049">on
discourse.</a>.
We will not have Knet examples here, but the documentation for Knet is
excellent and worth reading even if you plan to use Flux.</p>
<h2 id="additional-reading">Additional Reading<a class="headerlink" href="#additional-reading" title="Permanent link">&para;</a></h2>
<ul>
<li>Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow2016">2016</a>)
    <a href="http://www.deeplearningbook.org"><em>Deep Learning</em></a></li>
<li><a href="https://denizyuret.github.io/Knet.jl/latest/"><code>Knet.jl</code>
    documentation</a>
    especially the textbook</li>
<li>Klok and Nazarathy (<a href="#ref-klok2019">2019</a>) <em>Statistics with
    Julia:Fundamentals for Data Science, MachineLearning and Artificial
    Intelligence</em></li>
</ul>
<h1 id="single-layer-neural-networks">Single Layer Neural Networks<a class="headerlink" href="#single-layer-neural-networks" title="Permanent link">&para;</a></h1>
<p>We will describe neural networks from a perspective of nonparametric
estimation. Suppose we have a target function, $f: \R^p \to \R$. In many
applications the target function will be a conditional expectation,
$f(x) = \Er[y|x]$.</p>
<p>A single layer neural network approximates $f$ as follows <script type="math/tex; mode=display">
\hat{f}(x) = \sum_{j=1}^r \beta_j \psi(w_j'x + b_j)
</script> Here $r$ is the width of the layer. $\beta_j$ are scalars.
$\psi:\R \to \R$ is a nonlinear activation function. Common activation
functions include:</p>
<ul>
<li>
<p>Sigmoid $\psi(t) = 1/(1+e^{-t})$</p>
</li>
<li>
<p>Tanh $\psi(t) = \frac{e^t -e^{-t}}{e^t + e^{-t}}$</p>
</li>
<li>
<p>Rectified linear $\psi(t) = t 1(t\geq 0)$</p>
</li>
</ul>
<p>The $w_j \in \R^p$ are called weights and $b_j \in \R$ are biases.</p>
<p>You may have heard about the universal approximation theorem. This
refers to the fact that as $r$ increases, a neural network can
approximate any function. Mathematically, for some large class of
functions $\mathcal{F}$,</p>
<p>
<script type="math/tex; mode=display">
\sup_{f \in \mathcal{F}} \lim_{r \to \infty} \inf_{\beta, w, b} \Vert
f(x) - \sum_{j=1}^r \beta_j \psi(w_j'x+b_j) \Vert = 0
</script>
</p>
<p>Hornik, Stinchcombe, and White (<a href="#ref-hornik1989">1989</a>) contains one of
the earliest results along these lines. Some introductory texts mention
the universal approximation theorem as though it is something special
for neural networks. This is incorrect. In particular, the universal
approximation theorem does not explain why neural networks seem to be
unusually good at prediction. Most nonparametric estimation methods
(kernel, series, forests, etc) satisfy a similar conditions.</p>
<h1 id="training">Training<a class="headerlink" href="#training" title="Permanent link">&para;</a></h1>
<p>Models in <code>Flux.jl</code> all involve a differentiable loss function. The loss
function is minimized by a variant of gradient descent. Gradients are
usually calculated using reverse automatic differentiation
(backpropagation is a variant of reverse automatic differentiation
specialized for the structue of neural networks).</p>
<h2 id="low-level">Low level<a class="headerlink" href="#low-level" title="Permanent link">&para;</a></h2>
<p>A low level way to use <code>Flux.jl</code> is to write your loss function as a
typical Julia function, as in the following code block.</p>
<pre><code class="julia">using Plots, Flux, Statistics, ColorSchemes
# some function to estimate
f(x) = sin(x^x)/2^((x^x-π/2)/π)
function simulate(n,σ=1)
  x = rand(n,1).*π
  y = f.(x) .+ randn(n).*σ
  (x,y)
end

&quot;&quot;&quot;
   slp(r, activation=(t)-&gt; 1 ./ (1 .+ exp.(.-t)), dimx=1 )

Construct a single layer perception with width `r`. 
&quot;&quot;&quot;
function slp(r, activation=(t)-&gt; 1 ./ (1 .+ exp.(.-t)), dimx=1)
  # Parameters to be minimized wrt have to be declared for tracking
  # for reverse mode autodiff.
  w = param(randn(dimx,r))
  b = param(randn(1,r))
  β = param(randn(r))
  θ = Tracker.Params([β, w, b])
  pred(x) = activation(x*w.+b)*β
  loss(x,y) = mean((y.-pred(x)).^2)
  return(θ=θ, predict=pred,loss=loss)
end
x, y = simulate(1000, 0.5)
xg = 0:0.01:π
rs = [2, 3, 5, 7, 9]
cscheme = colorschemes[:BrBG_4];
</code></pre>

<pre><code class="julia">figs = Array{typeof(plot(0)),1}(undef,length(rs))
for r in eachindex(rs)
  m = slp(rs[r])
  figs[r]=plot(xg, f.(xg), lab=&quot;True f&quot;, title=&quot;$(rs[r]) units&quot;)
  figs[r]=scatter!(x,y, markeralpha=0.4, markersize=1, markerstrokewidth=0, lab=&quot;&quot;)
  maxiter = 5000
  @time for i = 1:maxiter
    Flux.train!(m.loss, m.θ, [(x, y)], Flux.AMSGrad())
    if (i % (maxiter ÷ 5))==0
      l=Tracker.data(m.loss(x,y))
      println(&quot;$i iteration, loss=$l&quot;)
      loc=Int64.(ceil(length(xg)*i/maxiter))
      yg = Tracker.data(m.predict(xg))
      figs[r]=plot!(xg,yg, lab=&quot;&quot;, color=get(cscheme, i/maxiter), alpha=1.0,
                    annotations=(xg[loc], yg[loc],
                                 Plots.text(&quot;i=$i&quot;, i&lt;maxiter/2 ? :left : :right, pointsize=10,
                                            color=get(cscheme, i/maxiter)) )
                    )
    end
  end
  display(figs[r])
end
</code></pre>

<pre><code>1000 iteration, loss=0.31287991801131293
2000 iteration, loss=0.27919155814182034
3000 iteration, loss=0.2664744507386843
4000 iteration, loss=0.2591087144969681
5000 iteration, loss=0.25554768594103433
 11.095482 seconds (26.86 M allocations: 2.326 GiB, 10.62% gc time)
1000 iteration, loss=0.29291415192156256
2000 iteration, loss=0.27347375110635963
3000 iteration, loss=0.2662186901730016
4000 iteration, loss=0.2620060980850178
5000 iteration, loss=0.2588918831520079
  1.919737 seconds (2.37 M allocations: 1.422 GiB, 13.19% gc time)
1000 iteration, loss=0.310489528030217
2000 iteration, loss=0.27693567372683353
3000 iteration, loss=0.26807096969551036
4000 iteration, loss=0.2643358828827099
5000 iteration, loss=0.26195455598373163
  2.699893 seconds (2.37 M allocations: 2.094 GiB, 13.17% gc time)
1000 iteration, loss=0.2918806449645754
2000 iteration, loss=0.2762274337725736
3000 iteration, loss=0.2680434574226006
4000 iteration, loss=0.26297845594471586
5000 iteration, loss=0.2597747862202985
  3.465996 seconds (2.37 M allocations: 2.767 GiB, 12.57% gc time)
1000 iteration, loss=0.3077135211105479
2000 iteration, loss=0.29601847497562433
3000 iteration, loss=0.2840659467916058
4000 iteration, loss=0.27586835422190475
5000 iteration, loss=0.27057958564677087
  4.163962 seconds (2.37 M allocations: 3.439 GiB, 12.89% gc time)
</code></pre>
<p><img alt="" src="../figures/slp_3_1.png" /> <img alt="" src="../figures/slp_3_2.png" />
<img alt="" src="../figures/slp_3_3.png" /> <img alt="" src="../figures/slp_3_4.png" />
<img alt="" src="../figures/slp_3_5.png" /></p>
<p>Notice how even though a wider network can approximate $f$ better, wider
networks also take more training iterations to minimize the loss. This
is typical of any minimization algorithm — the number of iterations
increases with the problem size.</p>
<p>Each invocation of <code>Flux.train!</code> completes one iteration of gradient
descent. As you might guess from this API, it is common to train neural
networks for a fixed number of iterations instead of until convergence
to a local minimum. The number of training iterations can act as a
regularization parameter.</p>
<h2 id="chain-interface">Chain interface<a class="headerlink" href="#chain-interface" title="Permanent link">&para;</a></h2>
<p><code>Flux.jl</code> also contains some higher level functions for creating loss
functions for neural networks. Here is the same network as in the
previous code block, but using the higher level API.</p>
<pre><code class="julia">dimx = 1
figs = Array{typeof(plot(0)),1}(undef,length(rs))
initmfigs = Array{typeof(plot(0)),1}(undef,length(rs))
xt = reshape(Float32.(x), 1, length(x))
yt = reshape(Float32.(y), 1, length(y))
for r in eachindex(rs)
  l = rs[r]
  m = Chain(x-&gt;Flux.normalise(x, dims=2), Dense(dimx, l, Flux.σ), Dense(rs[r], 1))
  initmfigs[r] = plot(xg, Tracker.data(m[1:(end-1)](xg'))', lab=&quot;&quot;, legend=false)
  figs[r]=plot(xg, f.(xg), lab=&quot;&quot;, title=&quot;$(rs[r]) units&quot;, color=:red)
  figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=&quot;&quot;)
  maxiter = 3000
  @time for i = 1:maxiter
    Flux.train!((x,y)-&gt;Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], Flux.AMSGrad() ) #,
                #cb = Flux.throttle(()-&gt;@show(Flux.mse(m(xt),yt)),100))
    if i==1 || (i % (maxiter ÷ 5))==0
      l=Tracker.data(Flux.mse(m(xt), yt))
      println(&quot;$(rs[r]) units, $i iterations, loss=$l&quot;)
      yg = Tracker.data(m(xg'))'
      loc=Int64.(ceil(length(xg)*i/maxiter))
      figs[r]=plot!(xg,yg, lab=&quot;&quot;, color=get(cscheme, i/maxiter), alpha=1.0,
                    annotations=(xg[loc], yg[loc],
                                 Plots.text(&quot;i=$i&quot;, i&lt;maxiter/2 ? :left : :right, pointsize=10,
                                            color=get(cscheme, i/maxiter)) )
                    )
    end
  end
  display(figs[r])
end
</code></pre>

<pre><code>2 units, 1 iterations, loss=0.9574206
2 units, 600 iterations, loss=0.30447912
2 units, 1200 iterations, loss=0.2659822
2 units, 1800 iterations, loss=0.25434485
2 units, 2400 iterations, loss=0.25087446
2 units, 3000 iterations, loss=0.24971141
  6.771462 seconds (17.30 M allocations: 1.101 GiB, 10.32% gc time)
3 units, 1 iterations, loss=0.825582
3 units, 600 iterations, loss=0.28322142
3 units, 1200 iterations, loss=0.2600006
3 units, 1800 iterations, loss=0.25268567
3 units, 2400 iterations, loss=0.25033832
3 units, 3000 iterations, loss=0.24962674
  1.279441 seconds (1.62 M allocations: 382.235 MiB, 6.08% gc time)
5 units, 1 iterations, loss=0.9207832
5 units, 600 iterations, loss=0.293824
5 units, 1200 iterations, loss=0.264771
5 units, 1800 iterations, loss=0.25713927
5 units, 2400 iterations, loss=0.25334862
5 units, 3000 iterations, loss=0.25147322
  1.590898 seconds (1.63 M allocations: 496.933 MiB, 6.20% gc time)
7 units, 1 iterations, loss=1.5818428
7 units, 600 iterations, loss=0.30069882
7 units, 1200 iterations, loss=0.26719385
7 units, 1800 iterations, loss=0.25823155
7 units, 2400 iterations, loss=0.2537658
7 units, 3000 iterations, loss=0.2517068
  1.867614 seconds (1.63 M allocations: 611.906 MiB, 6.23% gc time)
9 units, 1 iterations, loss=0.5171113
9 units, 600 iterations, loss=0.28988507
9 units, 1200 iterations, loss=0.2667996
9 units, 1800 iterations, loss=0.26013446
9 units, 2400 iterations, loss=0.25565305
9 units, 3000 iterations, loss=0.25305483
  2.025408 seconds (1.63 M allocations: 727.292 MiB, 5.97% gc time)
</code></pre>
<p><img alt="" src="../figures/slp_4_1.png" /> <img alt="" src="../figures/slp_4_2.png" />
<img alt="" src="../figures/slp_4_3.png" /> <img alt="" src="../figures/slp_4_4.png" />
<img alt="" src="../figures/slp_4_5.png" /></p>
<p>The figures do not appear identical to the first example since the
initial values differ, and the above code first normalises the $x$s.</p>
<h2 id="initial-values">Initial values<a class="headerlink" href="#initial-values" title="Permanent link">&para;</a></h2>
<p>Initial values are especially important with neural networks because
activation functions tend to be flat at the extremes. This causes the
gradient of the loss function to vanish in some regions of the parameter
space. For gradient descent to be successful, it is important to avoid
regions with vanishing gradients. The default initial values of $w$ and
$b$ used by Flux tend to work better with normalised $x$. The initial
activation are shown below.</p>
<pre><code class="julia">plot(initmfigs..., legend=false)
</code></pre>

<p><img alt="" src="../figures/slp_5_1.png" /></p>
<p>At these initial values, $w&rsquo;x + b$, does change sign for each
activation, but $w&rsquo;x$ is small enough that $\psi(w&rsquo;x + b)$ is
approximately linear. This will make it initially difficult to
distinguish $\beta \psi&rsquo;$ from $w$,</p>
<p>We can improve the fit by choosing initial values even more carefully.
The following code choses initial $w$ and $b$ to make sure the
activation functions vary nonlinearly in the support of $x$. The initial
activations functions are plotted below.</p>
<pre><code class="julia">dimx = 1
figs = Array{typeof(plot(0)),1}(undef,length(rs))
initmfigs = Array{typeof(plot(0)),1}(undef,length(rs))
xt = reshape(Float32.(x), 1, length(x))
yt = reshape(Float32.(y), 1, length(y))
for r in eachindex(rs)
  l = rs[r]
  m = Chain(Dense(dimx, l, Flux.σ), Dense(rs[r], 1))
  # adjust initial weights to make sure each node is nonlinear in support of X
  Tracker.update!(m[1].W, -m[1].W .+ sign.(Tracker.data(m[1].W))*2*π)
  # adjust initial intercepts to be in the support of w*x  
  Tracker.update!(m[1].b, -m[1].b .- m[1].W[:].*Float32.(π/(l+1):π/(l+1):π*l/(l+1)))
  # make initial output weights optimal given first layer
  X = vcat(1, Tracker.data(m[1](xt)))
  bols = (X*X') \ (X*Float32.(y))
  Tracker.update!(m[2].W, -m[2].W .+ bols[2:end]')
  Tracker.update!(m[2].b, -m[2].b .- Float32(Tracker.data(mean(m(xt) .- yt))))
  initmfigs[r] = plot(xg, Tracker.data(m[1](xg'))', lab=&quot;&quot;, legend=false)
  figs[r]=plot(xg, f.(xg), lab=&quot;&quot;, title=&quot;$(rs[r]) units&quot;, color=:red)
  figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=&quot;&quot;)
  maxiter = 3000
  @time for i = 1:maxiter
    Flux.train!((x,y)-&gt;Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], Flux.AMSGrad() ) #,
    #cb = Flux.throttle(()-&gt;@show(Flux.mse(m(xt),yt)),100))
    if i==1 || (i % (maxiter ÷ 5))==0
      l=Tracker.data(Flux.mse(m(xt), yt))
      println(&quot;$(rs[r]) units, $i iterations, loss=$l&quot;)
      yg = Tracker.data(m(xg'))'
      loc=Int64.(ceil(length(xg)*i/maxiter))
      figs[r]=plot!(xg,yg, lab=&quot;&quot;, color=get(cscheme, i/maxiter), alpha=1.0,
                    annotations=(xg[loc], yg[loc],
                                 Plots.text(&quot;i=$i&quot;, i&lt;maxiter/2 ? :left : :right, pointsize=10,
                                            color=get(cscheme, i/maxiter)) )
                    )
    end
  end
end
</code></pre>

<pre><code>2 units, 1 iterations, loss=0.32728884
2 units, 600 iterations, loss=0.26378527
2 units, 1200 iterations, loss=0.26310408
2 units, 1800 iterations, loss=0.26202366
2 units, 2400 iterations, loss=0.25056323
2 units, 3000 iterations, loss=0.24954805
  1.160733 seconds (1.63 M allocations: 314.262 MiB, 8.18% gc time)
3 units, 1 iterations, loss=0.26246357
3 units, 600 iterations, loss=0.24973491
3 units, 1200 iterations, loss=0.24814619
3 units, 1800 iterations, loss=0.24734117
3 units, 2400 iterations, loss=0.24683838
3 units, 3000 iterations, loss=0.24648328
  1.260519 seconds (1.55 M allocations: 367.629 MiB, 7.17% gc time)
5 units, 1 iterations, loss=0.25790077
5 units, 600 iterations, loss=0.24976541
5 units, 1200 iterations, loss=0.24819629
5 units, 1800 iterations, loss=0.2473083
5 units, 2400 iterations, loss=0.24671915
5 units, 3000 iterations, loss=0.24627346
  1.556834 seconds (1.56 M allocations: 482.326 MiB, 7.14% gc time)
7 units, 1 iterations, loss=0.2515118
7 units, 600 iterations, loss=0.24660046
7 units, 1200 iterations, loss=0.24575815
7 units, 1800 iterations, loss=0.24524012
7 units, 2400 iterations, loss=0.24487147
7 units, 3000 iterations, loss=0.24458885
  1.885652 seconds (1.56 M allocations: 597.300 MiB, 7.07% gc time)
9 units, 1 iterations, loss=0.24346104
9 units, 600 iterations, loss=0.24281342
9 units, 1200 iterations, loss=0.24277252
9 units, 1800 iterations, loss=0.24274462
9 units, 2400 iterations, loss=0.24272026
9 units, 3000 iterations, loss=0.24269772
  2.165591 seconds (1.56 M allocations: 712.686 MiB, 7.15% gc time)
</code></pre>
<pre><code class="julia">nothing
</code></pre>

<pre><code class="julia">display(plot(initmfigs..., legend=false))
</code></pre>

<p><img alt="" src="../figures/slp_7_1.png" /></p>
<p>And the fit figures.</p>
<pre><code class="julia">for f in figs
  display(f)
end
</code></pre>

<p><img alt="" src="../figures/slp_8_1.png" /> <img alt="" src="../figures/slp_8_2.png" />
<img alt="" src="../figures/slp_8_3.png" /> <img alt="" src="../figures/slp_8_4.png" />
<img alt="" src="../figures/slp_8_5.png" /></p>
<h2 id="rectified-linear">Rectified linear<a class="headerlink" href="#rectified-linear" title="Permanent link">&para;</a></h2>
<p>Large applications of neural networks often use rectified linear
activation for efficiency. Let’s see how the same example behaves with
(leaky) rectified linear activation.</p>
<pre><code class="julia">dimx = 1
figs = Array{typeof(plot(0)),1}(undef,length(rs))
for r in eachindex(rs)
  l = rs[r]
  m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change
  # adjust initial weights to make sure each node is nonlinear in support of X
  Tracker.update!(m[1].W, -m[1].W .+ sign.(Tracker.data(m[1].W))*2*π)
  # adjust initial intercepts to be in the support of w*x  
  Tracker.update!(m[1].b, -m[1].b .- m[1].W[:].*Float32.(π/(l+1):π/(l+1):π*l/(l+1)))
  # make initial output weights optimal given first layer
  X = vcat(1, Tracker.data(m[1](xt)))
  bols = (X*X') \ (X*Float32.(y))
  Tracker.update!(m[2].W, -m[2].W .+ bols[2:end]')
  Tracker.update!(m[2].b, -m[2].b .- Float32(Tracker.data(mean(m(xt) .- yt))))
  initmfigs[r] = plot(xg, Tracker.data(m[1:(end-1)](xg'))', lab=&quot;&quot;, legend=false)
  figs[r]=plot(xg, f.(xg), lab=&quot;&quot;, title=&quot;$(rs[r]) units&quot;, color=:red)
  figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=&quot;&quot;)
  maxiter = 3000
  @time for i = 1:maxiter
    Flux.train!((x,y)-&gt;Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], Flux.AMSGrad() ) #,
                #cb = Flux.throttle(()-&gt;@show(Flux.mse(m(xt),yt)),100))
    if i==1 || (i % (maxiter ÷ 5))==0
      l=Tracker.data(Flux.mse(m(xt), yt))
      println(&quot;$(rs[r]) units, $i iterations, loss=$l&quot;)
      yg = Tracker.data(m(xg'))'
      loc=Int64.(ceil(length(xg)*i/maxiter))
      figs[r]=plot!(xg,yg, lab=&quot;&quot;, color=get(cscheme, i/maxiter), alpha=1.0,
                    annotations=(xg[loc], yg[loc],
                                 Plots.text(&quot;i=$i&quot;, i&lt;maxiter/2 ? :left : :right, pointsize=10,
                                            color=get(cscheme, i/maxiter)) )
                    )
    end
  end
end
</code></pre>

<pre><code>2 units, 1 iterations, loss=0.39443994
2 units, 600 iterations, loss=0.31757522
2 units, 1200 iterations, loss=0.31293875
2 units, 1800 iterations, loss=0.3128002
2 units, 2400 iterations, loss=0.31272814
2 units, 3000 iterations, loss=0.31268603
  1.505653 seconds (3.06 M allocations: 386.479 MiB, 9.57% gc time)
3 units, 1 iterations, loss=0.31972623
3 units, 600 iterations, loss=0.29133326
3 units, 1200 iterations, loss=0.27709955
3 units, 1800 iterations, loss=0.27327424
3 units, 2400 iterations, loss=0.27085584
3 units, 3000 iterations, loss=0.2697004
  1.014758 seconds (1.54 M allocations: 367.492 MiB, 9.31% gc time)
5 units, 1 iterations, loss=0.2658342
5 units, 600 iterations, loss=0.26712254
5 units, 1200 iterations, loss=0.26643205
5 units, 1800 iterations, loss=0.26587254
5 units, 2400 iterations, loss=0.26543647
5 units, 3000 iterations, loss=0.26504025
  1.130329 seconds (1.55 M allocations: 482.189 MiB, 10.40% gc time)
7 units, 1 iterations, loss=0.24869746
7 units, 600 iterations, loss=0.24956109
7 units, 1200 iterations, loss=0.24936324
7 units, 1800 iterations, loss=0.24914628
7 units, 2400 iterations, loss=0.24899112
7 units, 3000 iterations, loss=0.24890159
  1.453316 seconds (1.55 M allocations: 597.162 MiB, 22.52% gc time)
9 units, 1 iterations, loss=0.26039335
9 units, 600 iterations, loss=0.24953708
9 units, 1200 iterations, loss=0.24872662
9 units, 1800 iterations, loss=0.24806042
9 units, 2400 iterations, loss=0.24774258
9 units, 3000 iterations, loss=0.24747035
  1.147302 seconds (1.55 M allocations: 712.548 MiB, 11.40% gc time)
</code></pre>
<pre><code class="julia">display(plot(initmfigs..., legend=false) )
</code></pre>

<p><img alt="" src="../figures/slp_9_1.png" /></p>
<pre><code class="julia">
for f in figs
  display(f)
end
</code></pre>

<p><img alt="" src="../figures/slp_9_2.png" /> <img alt="" src="../figures/slp_9_3.png" />
<img alt="" src="../figures/slp_9_4.png" /> <img alt="" src="../figures/slp_9_5.png" />
<img alt="" src="../figures/slp_9_6.png" /></p>
<h2 id="stochastic-gradient-descent">Stochastic Gradient descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permanent link">&para;</a></h2>
<p>The above examples all used the full data in each iteration of gradient
descent. Computation can be reduced and the parameter space can possibly
be explored more by using stochastic gradient descent. In stochastic
gradient descent, a subset (possibly even of size 1) of the data is used
to compute the gradient for each iteration. To accomplish this in Flux,
we should give the <code>Flux.train!</code> function an array of tuples of data
consisting of the subsets to be used in iteration. Each call to
<code>Flux.train!</code> loops over all tuples of data, doing one gradient descent
iteration for each. This whole process is referred to as a training
epoch. You could use (the below does not) Flux’s <code>@epochs</code> macro for
running multiple training epochs without writing a loop.</p>
<pre><code class="julia">dimx = 1
figs = Array{typeof(plot(0)),1}(undef,length(rs))
for r in eachindex(rs)
  l = rs[r]
  m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change
  # adjust initial weights to make sure each node is nonlinear in support of X
  Tracker.update!(m[1].W, -m[1].W .+ sign.(Tracker.data(m[1].W))*2*π)
  # adjust initial intercepts to be in the support of w*x  
  Tracker.update!(m[1].b, -m[1].b .- m[1].W[:].*Float32.(π/(l+1):π/(l+1):π*l/(l+1)))
  # make initial output weights optimal given first layer
  X = vcat(1, Tracker.data(m[1](xt)))
  bols = (X*X') \ (X*Float32.(y))
  Tracker.update!(m[2].W, -m[2].W .+ bols[2:end]')
  Tracker.update!(m[2].b, -m[2].b .- Float32(Tracker.data(mean(m(xt) .- yt))))
  initmfigs[r] = plot(xg, Tracker.data(m[1:(end-1)](xg'))', lab=&quot;&quot;, legend=false)
  figs[r]=plot(xg, f.(xg), lab=&quot;&quot;, title=&quot;$(rs[r]) units&quot;, color=:red)
  figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=&quot;&quot;)
  maxiter = 3000
  @time for i = 1:maxiter
    Flux.train!((x,y)-&gt;Flux.mse(m(x),y), Flux.params(m), 
                # partition data into 100 batches
                [(xt[:,p], yt[:,p]) for p in Base.Iterators.partition(1:length(y), 100)], 
                Flux.AMSGrad() ) #,
    if i==1 || (i % (maxiter ÷ 5))==0
      l=Tracker.data(Flux.mse(m(xt), yt))
      println(&quot;$(rs[r]) units, $i iterations, loss=$l&quot;)
      yg = Tracker.data(m(xg'))'
      loc=Int64.(ceil(length(xg)*i/maxiter))
      figs[r]=plot!(xg,yg, lab=&quot;&quot;, color=get(cscheme, i/maxiter), alpha=1.0,
                    annotations=(xg[loc], yg[loc],
                                 Plots.text(&quot;i=$i&quot;, i&lt;maxiter/2 ? :left : :right, pointsize=10,
                                            color=get(cscheme, i/maxiter)) )
                    )
    end
  end
end
</code></pre>

<pre><code>2 units, 1 iterations, loss=0.28042743
2 units, 600 iterations, loss=0.2561532
2 units, 1200 iterations, loss=0.25613663
2 units, 1800 iterations, loss=0.25611833
2 units, 2400 iterations, loss=0.25609887
2 units, 3000 iterations, loss=0.25608727
  4.317296 seconds (14.94 M allocations: 803.055 MiB, 5.41% gc time)
3 units, 1 iterations, loss=0.29548076
3 units, 600 iterations, loss=0.2640844
3 units, 1200 iterations, loss=0.2613922
3 units, 1800 iterations, loss=0.26100206
3 units, 2400 iterations, loss=0.2607383
3 units, 3000 iterations, loss=0.2605288
  4.342378 seconds (14.66 M allocations: 859.633 MiB, 5.58% gc time)
5 units, 1 iterations, loss=0.25648135
5 units, 600 iterations, loss=0.24122679
5 units, 1200 iterations, loss=0.2427018
5 units, 1800 iterations, loss=0.24278502
5 units, 2400 iterations, loss=0.245291
5 units, 3000 iterations, loss=0.24250484
  4.537362 seconds (14.66 M allocations: 984.721 MiB, 6.43% gc time)
7 units, 1 iterations, loss=0.25698918
7 units, 600 iterations, loss=0.24852635
7 units, 1200 iterations, loss=0.2531924
7 units, 1800 iterations, loss=0.25390106
7 units, 2400 iterations, loss=0.24680556
7 units, 3000 iterations, loss=0.25356048
  4.591942 seconds (14.66 M allocations: 1.073 GiB, 6.37% gc time)
9 units, 1 iterations, loss=0.26235273
9 units, 600 iterations, loss=0.249976
9 units, 1200 iterations, loss=0.24925886
9 units, 1800 iterations, loss=0.24445722
9 units, 2400 iterations, loss=0.24422652
9 units, 3000 iterations, loss=0.24371915
  4.611709 seconds (14.66 M allocations: 1.197 GiB, 6.95% gc time)
</code></pre>
<pre><code class="julia">
for f in figs
  display(f)
end
</code></pre>

<p><img alt="" src="../figures/slp_10_1.png" /> <img alt="" src="../figures/slp_10_2.png" />
<img alt="" src="../figures/slp_10_3.png" /> <img alt="" src="../figures/slp_10_4.png" />
<img alt="" src="../figures/slp_10_5.png" /></p>
<p>Here we see an in sample MSE about as low as the best previous result.
Note, however, the longer training time. Each “iteration” above is an
epoch, which consists of 10 gradient descent steps using the 10
different subsets (or batches) of data of size 100.</p>
<h1 id="rate-of-convergence">Rate of convergence<a class="headerlink" href="#rate-of-convergence" title="Permanent link">&para;</a></h1>
<ul>
<li>Chen and White (<a href="#ref-chen1999">1999</a>)</li>
<li>$f(x) = \Er[y|x]$ with Fourier representation
    <script type="math/tex; mode=display"> f(x) = \int e^{i a'x} d\sigma_f(a) </script> where
    $\int (\sqrt{a&rsquo;a} \vee 1) d|\sigma_f|(a) &lt; \infty$</li>
<li>Network sieve : <script type="math/tex; mode=display"> \begin{align*}
    \mathcal{G}_n = \{ &
    g: g(x) = \sum_{j=1}^{r_n} \beta_j (a_j'a_j \vee 1)^{-1}
    \psi(a_j'x + b_j), \\ & \norm{\beta}_1 \leq B_n \}
    \end{align*}
    </script>
</li>
</ul>
<p>The setup in Chen and White (<a href="#ref-chen1999">1999</a>) is more general.
They consider estimating both $f$ and its first $m$ derivatives. Here,
we focus on the case of just estimating $f$. Chen and White
(<a href="#ref-chen1999">1999</a>) also consider estimation of functions other than
conditional expectations.</p>
<p>The restriction on $f$ in the second bullet is used to control
approximation error. The second bullet says that $f$ is the inverse
Fourier transform of measure $\sigma_f$. The bite of the restriction on
$f$ comes from the requirement that $\sigma_f$ be absolutely integral,
$\int (\sqrt{a&rsquo;a} \vee 1) d|\sigma_f|(a) &lt; \infty$. It would be a good
exercise to check whether this restriction is satisfied by some familiar
types of functions. Barron (<a href="#ref-barron1993">1993</a>) first showed that
neural networks approximate this class of functions well, and compares
the approximation rate of neural networks to other function
approximation results.</p>
<p>TODO: - clean up this section - add simulations demonstrating
convergence rate?</p>
<h1 id="references-references">References [references]<a class="headerlink" href="#references-references" title="Permanent link">&para;</a></h1>
<div class="references" id="refs">
<div id="ref-barron1993">
<p>Barron, A. R. 1993. “Universal Approximation Bounds for Superpositions
of a Sigmoidal Function.” <em>IEEE Transactions on Information Theory</em> 39
(3): 930–45. <a href="https://doi.org/10.1109/18.256500">https://doi.org/10.1109/18.256500</a>.</p>
</div>
<div id="ref-chen1999">
<p>Chen, Xiaohong, and H. White. 1999. “Improved Rates and Asymptotic
Normality for Nonparametric Neural Network Estimators.” <em>IEEE
Transactions on Information Theory</em> 45 (2): 682–91.
<a href="https://doi.org/10.1109/18.749011">https://doi.org/10.1109/18.749011</a>.</p>
</div>
<div id="ref-goodfellow2016">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep
Learning</em>. MIT Press. <a href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>.</p>
</div>
<div id="ref-hornik1989">
<p>Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. “Multilayer
Feedforward Networks Are Universal Approximators.” <em>Neural Networks</em> 2
(5): 359–66.
<a href="https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8">https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8</a>.</p>
</div>
<div id="ref-klok2019">
<p>Klok, Hayden, and Yoni Nazarathy. 2019. <em>Statistics with
Julia:Fundamentals for Data Science, Machinelearning and Artificial
Intelligence</em>. DRAFT.
<a href="https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf">https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf</a>.</p>
</div>
</div></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
