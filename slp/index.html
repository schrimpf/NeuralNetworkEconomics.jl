<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Introduction -  </title>
        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <link href="../assets/extra.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href=".."> </a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="..">Package Docs</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">ML in Economics <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../ml-intro/">Introduction</a>
</li>
                                    
<li >
    <a href="../ml-methods/">Methods</a>
</li>
                                    
<li >
    <a href="../ml-doubledebiased/">Inference</a>
</li>
                                    
<li >
    <a href="../mlExamplePKH/">Detecting heterogeneity</a>
</li>
                                    
<li >
    <a href="../ml-julia/">With Julia</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Neural Networks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li class="active">
    <a href="./">Introduction</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../license/">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../ml-julia/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../license/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/schrimpf/NeuralNetworkEconomics.jl/edit/master/docs/slp.md"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#about-this-document">About this document</a></li>
        <li class="main "><a href="#introduction">Introduction</a></li>
            <li><a href="#additional-reading">Additional Reading</a></li>
        <li class="main "><a href="#single-layer-neural-networks">Single Layer Neural Networks</a></li>
        <li class="main "><a href="#training">Training</a></li>
            <li><a href="#low-level">Low level</a></li>
            <li><a href="#chain-interface">Chain interface</a></li>
            <li><a href="#initial-values">Initial values</a></li>
            <li><a href="#rectified-linear">Rectified linear</a></li>
            <li><a href="#stochastic-gradient-descent">Stochastic Gradient descent</a></li>
        <li class="main "><a href="#rate-of-convergence">Rate of convergence</a></li>
        <li class="main "><a href="#references-references">References [references]</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a></p>
<h3 id="about-this-document">About this document<a class="headerlink" href="#about-this-document" title="Permanent link">&para;</a></h3>
<p>This document was created using Weave.jl. The code is available in <a href="https://github.com/schrimpf/NeuralNetworkEconomics.jl">on
github</a>. The same
document generates both static webpages and associated <a href="../slp.ipynb">jupyter
notebook</a>.</p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
</script>
</p>
<h1 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<p>Neural networks, especially deep neural networks, have come to dominate
some areas of machine learning. Neural networks are especially prominent
in natural language processing, image classification, and reinforcement
learning. This documents gives a brief introduction to neural networks.</p>
<p>Examples in this document will use
<a href="https://fluxml.ai/Flux.jl/stable/"><code>Flux.jl</code></a>. An alternative Julia
package for deep learning is
<a href="https://denizyuret.github.io/Knet.jl/latest/"><code>Knet.jl</code></a>. There is a
good discussion comparing Flux and Knet <a href="https://discourse.julialang.org/t/state-of-deep-learning-in-julia/28049">on
discourse.</a>.
We will not have Knet examples here, but the documentation for Knet is
excellent and worth reading even if you plan to use Flux.</p>
<h2 id="additional-reading">Additional Reading<a class="headerlink" href="#additional-reading" title="Permanent link">&para;</a></h2>
<ul>
<li>Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow2016">2016</a>)
    <a href="http://www.deeplearningbook.org"><em>Deep Learning</em></a></li>
<li><a href="https://denizyuret.github.io/Knet.jl/latest/"><code>Knet.jl</code>
    documentation</a>
    especially the textbook</li>
<li>Klok and Nazarathy (<a href="#ref-klok2019">2019</a>) <em>Statistics with
    Julia:Fundamentals for Data Science, MachineLearning and Artificial
    Intelligence</em></li>
</ul>
<h1 id="single-layer-neural-networks">Single Layer Neural Networks<a class="headerlink" href="#single-layer-neural-networks" title="Permanent link">&para;</a></h1>
<p>We will describe neural networks from a perspective of nonparametric
estimation. Suppose we have a target function, $f: \R^p \to \R$. In many
applications the target function will be a conditional expectation,
$f(x) = \Er[y|x]$.</p>
<p>A single layer neural network approximates $f$ as follows <script type="math/tex; mode=display">
\hat{f}(x) = \sum_{j=1}^r \beta_j 
\psi(w_j'x + b_j)
</script> Here $r$ is the width of the layer. $\beta_j$ are scalars.
$\psi:\R \to \R$ is a nonlinear activation function. Common activation
functions include:</p>
<ul>
<li>
<p>Sigmoid $\psi(t) = 1/(1+e^{-t})$</p>
</li>
<li>
<p>Tanh $\psi(t) = \frac{e^t -e^{-t}}{e^t + e^{-t}}$</p>
</li>
<li>
<p>Rectified linear $\psi(t) = t 1(t\geq 0)$</p>
</li>
</ul>
<p>The $w_j \in \R^p$ are called weights and $b_j \in \R$ are biases.</p>
<p>You may have heard about the universal approximation theorem. This
refers to the fact that as $r$ increases, a neural network can
approximate any function. Mathematically, for some large class of
functions $\mathcal{F}$,</p>
<p>
<script type="math/tex; mode=display">
\sup_{f \in \mathcal{F}} \lim_{r \to \infty} \inf_{\beta, w, b} \Vert
f(x) - \sum_{j=1}^r \beta_j \psi(w_j'x+b_j) \Vert = 0
</script>
</p>
<p>Hornik, Stinchcombe, and White (<a href="#ref-hornik1989">1989</a>) contains one of
the earliest results along these lines. Some introductory texts mention
the universal approximation theorem as though it is something special
for neural networks. This is incorrect. In particular, the universal
approximation theorem does not explain why neural networks seem to be
unusually good at prediction. Most nonparametric estimation methods
(kernel, series, forests, etc) satisfy a similar conditions.</p>
<h1 id="training">Training<a class="headerlink" href="#training" title="Permanent link">&para;</a></h1>
<p>Models in <code>Flux.jl</code> all involve a differentiable loss function. The loss
function is minimized by a variant of gradient descent. Gradients are
usually calculated using reverse automatic differentiation
(backpropagation is a variant of reverse automatic differentiation
specialized for the structue of neural networks).</p>
<h2 id="low-level">Low level<a class="headerlink" href="#low-level" title="Permanent link">&para;</a></h2>
<p>A low level way to use <code>Flux.jl</code> is to write your loss function as a
typical Julia function, as in the following code block.</p>
<pre><code class="julia">using Plots, Flux, Statistics, ColorSchemes
# some function to estimate
f(x) = sin(x^x)/2^((x^x-π/2)/π)
function simulate(n,σ=1)
  x = rand(n,1).*π
  y = f.(x) .+ randn(n).*σ
  (x,y)
end

&quot;&quot;&quot;
   slp(r, activation=(t)-&gt; 1 ./ (1 .+ exp.(.-t)), dimx=1 )

Construct a single layer perception with width `r`. 
&quot;&quot;&quot;
function slp(r, activation=(t)-&gt; 1 ./ (1 .+ exp.(.-t)), dimx=1)
  # Parameters to be minimized wrt have to be declared for tracking
  # for reverse mode autodiff.
  w = param(randn(dimx,r))
  b = param(randn(1,r))
  β = param(randn(r))
  θ = Tracker.Params([β, w, b])
  pred(x) = activation(x*w.+b)*β
  loss(x,y) = mean((y.-pred(x)).^2)
  return(θ=θ, predict=pred,loss=loss)
end
x, y = simulate(1000, 0.5)
xg = 0:0.01:π
rs = [2, 3, 5, 7, 9]
cscheme = colorschemes[:BrBG_4];
</code></pre>

<pre><code class="julia">figs = Array{typeof(plot(0)),1}(undef,length(rs))
for r in eachindex(rs)
  m = slp(rs[r])
  figs[r]=plot(xg, f.(xg), lab=&quot;True f&quot;, title=&quot;$(rs[r]) units&quot;)
  figs[r]=scatter!(x,y, markeralpha=0.4, markersize=1, markerstrokewidth=0, lab=&quot;&quot;)
  maxiter = 5000
  for i = 1:maxiter
    Flux.train!(m.loss, m.θ, [(x, y)], Flux.AMSGrad())
    if (i % (maxiter ÷ 5))==0
      l=Tracker.data(m.loss(x,y))
      println(&quot;$i iteration, loss=$l&quot;)
      yg = Tracker.data(m.predict(xg))
      figs[r]=plot!(xg,yg, lab=&quot;&quot;, color=get(cscheme, i/maxiter), alpha=1.0,
                    annotations=(xg[loc], yg[loc],
                                 Plots.text(&quot;i=$i&quot;, i&lt;maxiter/2 ? :left : :right, pointsize=10,
                                            color=get(cscheme, i/maxiter)) )
                    )
    end
  end
  display(figs[r])
end
</code></pre>

<pre><code>1000 iteration, loss=0.3384777887417437
Error: UndefVarError: loc not defined
</code></pre>
<p>Notice how even though a wider network can approximate $f$ better, wider
networks also take more training iterations to minimize the loss. This
is typical of any minimization algorithm — the number of iterations
increases with the problem size.</p>
<p>Each invocation of <code>Flux.train!</code> completes one iteration of gradient
descent. As you might guess from this API, it is common to train neural
networks for a fixed number of iterations instead of until convergence
to a local minimum. The number of training iterations can act as a
regularization parameter.</p>
<h2 id="chain-interface">Chain interface<a class="headerlink" href="#chain-interface" title="Permanent link">&para;</a></h2>
<p><code>Flux.jl</code> also contains some higher level functions for creating loss
functions for neural networks. Here is the same network as in the
previous code block, but using the higher level API.</p>
<pre><code class="julia">dimx = 1
figs = Array{typeof(plot(0)),1}(undef,length(rs))
initmfigs = Array{typeof(plot(0)),1}(undef,length(rs))
xt = reshape(Float32.(x), 1, length(x))
yt = reshape(Float32.(y), 1, length(y))
for r in eachindex(rs)
  l = rs[r]
  m = Chain(x-&gt;Flux.normalise(x, dims=2), Dense(dimx, l, Flux.σ), Dense(rs[r], 1))
  initmfigs[r] = plot(xg, Tracker.data(m[1:(end-1)](xg'))', lab=&quot;&quot;, legend=false)
  figs[r]=plot(xg, f.(xg), lab=&quot;&quot;, title=&quot;$(rs[r]) units&quot;, color=:red)
  figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=&quot;&quot;)
  maxiter = 3000
  for i = 1:maxiter
    Flux.train!((x,y)-&gt;Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], Flux.AMSGrad() ) #,
                #cb = Flux.throttle(()-&gt;@show(Flux.mse(m(xt),yt)),100))
    if i==1 || (i % (maxiter ÷ 5))==0
      l=Tracker.data(Flux.mse(m(xt), yt))
      println(&quot;$(rs[r]) units, $i iterations, loss=$l&quot;)
      yg = Tracker.data(m(xg'))'
      loc=Int64.(ceil(length(xg)*i/maxiter))
      figs[r]=plot!(xg,yg, lab=&quot;&quot;, color=get(cscheme, i/maxiter), alpha=1.0,
                    annotations=(xg[loc], yg[loc],
                                 Plots.text(&quot;i=$i&quot;, i&lt;maxiter/2 ? :left : :right, pointsize=10,
                                            color=get(cscheme, i/maxiter)) )
                    )
    end
  end
  display(figs[r])
end
</code></pre>

<pre><code>2 units, 1 iterations, loss=1.0369217
2 units, 600 iterations, loss=0.3163922
2 units, 1200 iterations, loss=0.2810491
2 units, 1800 iterations, loss=0.26917544
2 units, 2400 iterations, loss=0.26473504
2 units, 3000 iterations, loss=0.26302978
3 units, 1 iterations, loss=0.7069125
3 units, 600 iterations, loss=0.32422245
3 units, 1200 iterations, loss=0.2859918
3 units, 1800 iterations, loss=0.27349538
3 units, 2400 iterations, loss=0.2671763
3 units, 3000 iterations, loss=0.26457357
5 units, 1 iterations, loss=3.5714097
5 units, 600 iterations, loss=0.31072095
5 units, 1200 iterations, loss=0.28445798
5 units, 1800 iterations, loss=0.27327633
5 units, 2400 iterations, loss=0.26768163
5 units, 3000 iterations, loss=0.26503065
7 units, 1 iterations, loss=1.6327342
7 units, 600 iterations, loss=0.306045
7 units, 1200 iterations, loss=0.28370106
7 units, 1800 iterations, loss=0.27319804
7 units, 2400 iterations, loss=0.26775336
7 units, 3000 iterations, loss=0.2653656
9 units, 1 iterations, loss=2.3672042
9 units, 600 iterations, loss=0.316754
9 units, 1200 iterations, loss=0.2887455
9 units, 1800 iterations, loss=0.27853328
9 units, 2400 iterations, loss=0.2717537
9 units, 3000 iterations, loss=0.2678932
</code></pre>
<p><img alt="" src="../figures/slp_4_1.png" /> <img alt="" src="../figures/slp_4_2.png" />
<img alt="" src="../figures/slp_4_3.png" /> <img alt="" src="../figures/slp_4_4.png" />
<img alt="" src="../figures/slp_4_5.png" /></p>
<p>The figures do not appear identical to the first example since the
initial values differ, and the above code first normalises the $x$s.</p>
<h2 id="initial-values">Initial values<a class="headerlink" href="#initial-values" title="Permanent link">&para;</a></h2>
<p>Initial values are especially important with neural networks because
activation functions tend to be flat at the extremes. This causes the
gradient of the loss function to vanish in some regions of the parameter
space. For gradient descent to be successful, it is important to avoid
regions with vanishing gradients. The default initial values of $w$ and
$b$ used by Flux tend to work better with normalised $x$. The initial
activation are shown below.</p>
<pre><code class="julia">plot(initmfigs..., legend=false)
</code></pre>

<p><img alt="" src="../figures/slp_5_1.png" /></p>
<p>At these initial values, $w&rsquo;x + b$, does change sign for each
activation, but $w&rsquo;x$ is small enough that $\psi(w&rsquo;x + b)$ is
approximately linear. This will make it initially difficult to
distinguish $\beta \psi&rsquo;$ from $w$,</p>
<p>We can improve the fit by choosing initial values even more carefully.
The following code choses initial $w$ and $b$ to make sure the
activation functions vary nonlinearly in the support of $x$. The initial
activations functions are plotted below.</p>
<pre><code class="julia">dimx = 1
figs = Array{typeof(plot(0)),1}(undef,length(rs))
initmfigs = Array{typeof(plot(0)),1}(undef,length(rs))
xt = reshape(Float32.(x), 1, length(x))
yt = reshape(Float32.(y), 1, length(y))
for r in eachindex(rs)
  l = rs[r]
  m = Chain(Dense(dimx, l, Flux.σ), Dense(rs[r], 1))
  # adjust initial weights to make sure each node is nonlinear in support of X
  Tracker.update!(m[1].W, -m[1].W .+ sign.(Tracker.data(m[1].W))*2*π)
  # adjust initial intercepts to be in the support of w*x  
  Tracker.update!(m[1].b, -m[1].b .- m[1].W[:].*Float32.(π/(l+1):π/(l+1):π*l/(l+1)))
  # make initial output weights optimal given first layer
  X = vcat(1, Tracker.data(m[1](xt)))
  bols = (X*X') \ (X*Float32.(y))
  Tracker.update!(m[2].W, -m[2].W .+ bols[2:end]')
  Tracker.update!(m[2].b, -m[2].b .- Float32(Tracker.data(mean(m(xt) .- yt))))
  initmfigs[r] = plot(xg, Tracker.data(m[1](xg'))', lab=&quot;&quot;, legend=false)
  figs[r]=plot(xg, f.(xg), lab=&quot;&quot;, title=&quot;$(rs[r]) units&quot;, color=:red)
  figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=&quot;&quot;)
  maxiter = 3000
  for i = 1:maxiter
    Flux.train!((x,y)-&gt;Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], Flux.AMSGrad() ) #,
    #cb = Flux.throttle(()-&gt;@show(Flux.mse(m(xt),yt)),100))
    if i==1 || (i % (maxiter ÷ 5))==0
      l=Tracker.data(Flux.mse(m(xt), yt))
      println(&quot;$(rs[r]) units, $i iterations, loss=$l&quot;)
      yg = Tracker.data(m(xg'))'
      loc=Int64.(ceil(length(xg)*i/maxiter))
      figs[r]=plot!(xg,yg, lab=&quot;&quot;, color=get(cscheme, i/maxiter), alpha=1.0,
                    annotations=(xg[loc], yg[loc],
                                 Plots.text(&quot;i=$i&quot;, i&lt;maxiter/2 ? :left : :right, pointsize=10,
                                            color=get(cscheme, i/maxiter)) )
                    )
    end
  end
end
</code></pre>

<pre><code>2 units, 1 iterations, loss=0.3366038
2 units, 600 iterations, loss=0.27992913
2 units, 1200 iterations, loss=0.27875888
2 units, 1800 iterations, loss=0.27578646
2 units, 2400 iterations, loss=0.2642159
2 units, 3000 iterations, loss=0.26335862
3 units, 1 iterations, loss=0.28392342
3 units, 600 iterations, loss=0.26435375
3 units, 1200 iterations, loss=0.26197103
3 units, 1800 iterations, loss=0.26100686
3 units, 2400 iterations, loss=0.26044744
3 units, 3000 iterations, loss=0.26004958
5 units, 1 iterations, loss=0.27672693
5 units, 600 iterations, loss=0.26504174
5 units, 1200 iterations, loss=0.26310754
5 units, 1800 iterations, loss=0.2621695
5 units, 2400 iterations, loss=0.26154354
5 units, 3000 iterations, loss=0.26107207
7 units, 1 iterations, loss=0.26915398
7 units, 600 iterations, loss=0.25981954
7 units, 1200 iterations, loss=0.2588425
7 units, 1800 iterations, loss=0.25818548
7 units, 2400 iterations, loss=0.257687
7 units, 3000 iterations, loss=0.25729224
9 units, 1 iterations, loss=0.25631088
9 units, 600 iterations, loss=0.25555292
9 units, 1200 iterations, loss=0.2554833
9 units, 1800 iterations, loss=0.2554464
9 units, 2400 iterations, loss=0.25541702
9 units, 3000 iterations, loss=0.2553905
</code></pre>
<pre><code class="julia">nothing
</code></pre>

<pre><code class="julia">display(plot(initmfigs..., legend=false))
</code></pre>

<p><img alt="" src="../figures/slp_7_1.png" /></p>
<p>And the fit figures.</p>
<pre><code class="julia">for f in figs
  display(f)
end
</code></pre>

<p><img alt="" src="../figures/slp_8_1.png" /> <img alt="" src="../figures/slp_8_2.png" />
<img alt="" src="../figures/slp_8_3.png" /> <img alt="" src="../figures/slp_8_4.png" />
<img alt="" src="../figures/slp_8_5.png" /></p>
<h2 id="rectified-linear">Rectified linear<a class="headerlink" href="#rectified-linear" title="Permanent link">&para;</a></h2>
<p>Large applications of neural networks often use rectified linear
activation for efficiency. Let’s see how the same example behaves with
(leaky) rectified linear activation.</p>
<pre><code class="julia">dimx = 1
figs = Array{typeof(plot(0)),1}(undef,length(rs))
for r in eachindex(rs)
  l = rs[r]
  m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change
  # adjust initial weights to make sure each node is nonlinear in support of X
  Tracker.update!(m[1].W, -m[1].W .+ sign.(Tracker.data(m[1].W))*2*π)
  # adjust initial intercepts to be in the support of w*x  
  Tracker.update!(m[1].b, -m[1].b .- m[1].W[:].*Float32.(π/(l+1):π/(l+1):π*l/(l+1)))
  # make initial output weights optimal given first layer
  X = vcat(1, Tracker.data(m[1](xt)))
  bols = (X*X') \ (X*Float32.(y))
  Tracker.update!(m[2].W, -m[2].W .+ bols[2:end]')
  Tracker.update!(m[2].b, -m[2].b .- Float32(Tracker.data(mean(m(xt) .- yt))))
  initmfigs[r] = plot(xg, Tracker.data(m[1:(end-1)](xg'))', lab=&quot;&quot;, legend=false)
  figs[r]=plot(xg, f.(xg), lab=&quot;&quot;, title=&quot;$(rs[r]) units&quot;, color=:red)
  figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=&quot;&quot;)
  maxiter = 3000
  for i = 1:maxiter
    Flux.train!((x,y)-&gt;Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], Flux.AMSGrad() ) #,
                #cb = Flux.throttle(()-&gt;@show(Flux.mse(m(xt),yt)),100))
    if i==1 || (i % (maxiter ÷ 5))==0
      l=Tracker.data(Flux.mse(m(xt), yt))
      println(&quot;$(rs[r]) units, $i iterations, loss=$l&quot;)
      yg = Tracker.data(m(xg'))'
      loc=Int64.(ceil(length(xg)*i/maxiter))
      figs[r]=plot!(xg,yg, lab=&quot;&quot;, color=get(cscheme, i/maxiter), alpha=1.0,
                    annotations=(xg[loc], yg[loc],
                                 Plots.text(&quot;i=$i&quot;, i&lt;maxiter/2 ? :left : :right, pointsize=10,
                                            color=get(cscheme, i/maxiter)) )
                    )
    end
  end
end
</code></pre>

<pre><code>2 units, 1 iterations, loss=0.29702237
2 units, 600 iterations, loss=0.28908145
2 units, 1200 iterations, loss=0.2883219
2 units, 1800 iterations, loss=0.28762406
2 units, 2400 iterations, loss=0.28705677
2 units, 3000 iterations, loss=0.28651312
3 units, 1 iterations, loss=0.30765957
3 units, 600 iterations, loss=0.286267
3 units, 1200 iterations, loss=0.283767
3 units, 1800 iterations, loss=0.282318
3 units, 2400 iterations, loss=0.2816515
3 units, 3000 iterations, loss=0.28122678
5 units, 1 iterations, loss=0.28393695
5 units, 600 iterations, loss=0.2731749
5 units, 1200 iterations, loss=0.27230558
5 units, 1800 iterations, loss=0.2719356
5 units, 2400 iterations, loss=0.27168837
5 units, 3000 iterations, loss=0.27141717
7 units, 1 iterations, loss=0.2570676
7 units, 600 iterations, loss=0.25794896
7 units, 1200 iterations, loss=0.25785723
7 units, 1800 iterations, loss=0.25780305
7 units, 2400 iterations, loss=0.25776556
7 units, 3000 iterations, loss=0.25772992
9 units, 1 iterations, loss=0.26243994
9 units, 600 iterations, loss=0.26901057
9 units, 1200 iterations, loss=0.26791033
9 units, 1800 iterations, loss=0.2674495
9 units, 2400 iterations, loss=0.26714712
9 units, 3000 iterations, loss=0.2668494
</code></pre>
<pre><code class="julia">display(plot(initmfigs..., legend=false) )
</code></pre>

<p><img alt="" src="../figures/slp_9_1.png" /></p>
<pre><code class="julia">
for f in figs
  display(f)
end
</code></pre>

<p><img alt="" src="../figures/slp_9_2.png" /> <img alt="" src="../figures/slp_9_3.png" />
<img alt="" src="../figures/slp_9_4.png" /> <img alt="" src="../figures/slp_9_5.png" />
<img alt="" src="../figures/slp_9_6.png" /></p>
<h2 id="stochastic-gradient-descent">Stochastic Gradient descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permanent link">&para;</a></h2>
<p>The above examples all used the full data in each iteration of gradient
descent. Computation can be reduced and the parameter space can possibly
be explored more by using stochastic gradient descent. In stochastic
gradient descent, a subset (possibly even of size 1) of the data is used
to compute the gradient for each iteration. To accomplish this in Flux,
we should give the <code>Flux.train!</code> function an array of tuples of data
consisting of the subsets to be used in iteration. Each call to
<code>Flux.train!</code> loops over all tuples of data, doing one gradient descent
iteration for each. This whole process is referred to as a training
epoch. You could use (the below does not) Flux’s <code>@epochs</code> macro for
running multiple training epochs without writing a loop.</p>
<pre><code class="julia">dimx = 1
figs = Array{typeof(plot(0)),1}(undef,length(rs))
for r in eachindex(rs)
  l = rs[r]
  m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change
  # adjust initial weights to make sure each node is nonlinear in support of X
  Tracker.update!(m[1].W, -m[1].W .+ sign.(Tracker.data(m[1].W))*2*π)
  # adjust initial intercepts to be in the support of w*x  
  Tracker.update!(m[1].b, -m[1].b .- m[1].W[:].*Float32.(π/(l+1):π/(l+1):π*l/(l+1)))
  # make initial output weights optimal given first layer
  X = vcat(1, Tracker.data(m[1](xt)))
  bols = (X*X') \ (X*Float32.(y))
  Tracker.update!(m[2].W, -m[2].W .+ bols[2:end]')
  Tracker.update!(m[2].b, -m[2].b .- Float32(Tracker.data(mean(m(xt) .- yt))))
  initmfigs[r] = plot(xg, Tracker.data(m[1:(end-1)](xg'))', lab=&quot;&quot;, legend=false)
  figs[r]=plot(xg, f.(xg), lab=&quot;&quot;, title=&quot;$(rs[r]) units&quot;, color=:red)
  figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=&quot;&quot;)
  maxiter = 3000
  for i = 1:maxiter
    Flux.train!((x,y)-&gt;Flux.mse(m(x),y), Flux.params(m), 
                # partition data into 100 batches
                [(xt[:,p], yt[:,p]) for p in Base.Iterators.partition(1:length(y), 100)], 
                Flux.AMSGrad() ) #,
    if i==1 || (i % (maxiter ÷ 5))==0
      l=Tracker.data(Flux.mse(m(xt), yt))
      println(&quot;$(rs[r]) units, $i iterations, loss=$l&quot;)
      yg = Tracker.data(m(xg'))'
      loc=Int64.(ceil(length(xg)*i/maxiter))
      figs[r]=plot!(xg,yg, lab=&quot;&quot;, color=get(cscheme, i/maxiter), alpha=1.0,
                    annotations=(xg[loc], yg[loc],
                                 Plots.text(&quot;i=$i&quot;, i&lt;maxiter/2 ? :left : :right, pointsize=10,
                                            color=get(cscheme, i/maxiter)) )
                    )
    end
  end
end
</code></pre>

<pre><code>2 units, 1 iterations, loss=0.29751676
2 units, 600 iterations, loss=0.27500233
2 units, 1200 iterations, loss=0.27498618
2 units, 1800 iterations, loss=0.2749766
2 units, 2400 iterations, loss=0.27497485
2 units, 3000 iterations, loss=0.27501076
3 units, 1 iterations, loss=0.3140766
3 units, 600 iterations, loss=0.27320027
3 units, 1200 iterations, loss=0.27320513
3 units, 1800 iterations, loss=0.27321768
3 units, 2400 iterations, loss=0.273258
3 units, 3000 iterations, loss=0.27322558
5 units, 1 iterations, loss=0.26850498
5 units, 600 iterations, loss=0.27048385
5 units, 1200 iterations, loss=0.26997226
5 units, 1800 iterations, loss=0.27007732
5 units, 2400 iterations, loss=0.2693555
5 units, 3000 iterations, loss=0.26938492
7 units, 1 iterations, loss=0.262361
7 units, 600 iterations, loss=0.26100615
7 units, 1200 iterations, loss=0.26014635
7 units, 1800 iterations, loss=0.25989655
7 units, 2400 iterations, loss=0.25976866
7 units, 3000 iterations, loss=0.25972375
9 units, 1 iterations, loss=0.26405096
9 units, 600 iterations, loss=0.25855294
9 units, 1200 iterations, loss=0.2617295
9 units, 1800 iterations, loss=0.26226482
9 units, 2400 iterations, loss=0.26251462
9 units, 3000 iterations, loss=0.26008978
</code></pre>
<pre><code class="julia">
for f in figs
  display(f)
end
</code></pre>

<p><img alt="" src="../figures/slp_10_1.png" /> <img alt="" src="../figures/slp_10_2.png" />
<img alt="" src="../figures/slp_10_3.png" /> <img alt="" src="../figures/slp_10_4.png" />
<img alt="" src="../figures/slp_10_5.png" /></p>
<h1 id="rate-of-convergence">Rate of convergence<a class="headerlink" href="#rate-of-convergence" title="Permanent link">&para;</a></h1>
<ul>
<li>Chen and White (<a href="#ref-chen1999">1999</a>)</li>
<li>$f(x) = \Er[y|x]$ with Fourier representation
    <script type="math/tex; mode=display"> f(x) = \int e^{i a'x} d\sigma_f(a) </script> where
    $\int (\sqrt{a&rsquo;a} \vee 1) d|\sigma_f|(a) &lt; \infty$</li>
<li>Network sieve : <script type="math/tex; mode=display"> \begin{align*}
    \mathcal{G}_n = \{ &
    g: g(x) = \sum_{j=1}^{r_n} \beta_j (a_j'a_j \vee 1)^{-1}
    \psi(a_j'x + b_j), \\ & \norm{\beta}_1 \leq B_n \}
    \end{align*}
    </script>
</li>
</ul>
<p>The setup in Chen and White (<a href="#ref-chen1999">1999</a>) is more general.
They consider estimating both $f$ and its first $m$ derivatives. Here,
we focus on the case of just estimating $f$. Chen and White
(<a href="#ref-chen1999">1999</a>) also consider estimation of functions other than
conditional expectations.</p>
<p>The restriction on $f$ in the second bullet is used to control
approximation error. The second bullet says that $f$ is the inverse
Fourier transform of measure $\sigma_f$. The bite of the restriction on
$f$ comes from the requirement that $\sigma_f$ be absolutely integral,
$\int (\sqrt{a&rsquo;a} \vee 1) d|\sigma_f|(a) &lt; \infty$. It would be a good
exercise to check whether this restriction is satisfied by some familiar
types of functions. Barron (<a href="#ref-barron1993">1993</a>) first showed that
neural networks approximate this class of functions well, and compares
the approximation rate of neural networks to other function
approximation results.</p>
<p>TODO: - clean up this section - add simulations demonstrating
convergence rate?</p>
<h1 id="references-references">References [references]<a class="headerlink" href="#references-references" title="Permanent link">&para;</a></h1>
<div class="references" id="refs">
<div id="ref-barron1993">
<p>Barron, A. R. 1993. “Universal Approximation Bounds for Superpositions
of a Sigmoidal Function.” <em>IEEE Transactions on Information Theory</em> 39
(3): 930–45. <a href="https://doi.org/10.1109/18.256500">https://doi.org/10.1109/18.256500</a>.</p>
</div>
<div id="ref-chen1999">
<p>Chen, Xiaohong, and H. White. 1999. “Improved Rates and Asymptotic
Normality for Nonparametric Neural Network Estimators.” <em>IEEE
Transactions on Information Theory</em> 45 (2): 682–91.
<a href="https://doi.org/10.1109/18.749011">https://doi.org/10.1109/18.749011</a>.</p>
</div>
<div id="ref-goodfellow2016">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep
Learning</em>. MIT Press. <a href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>.</p>
</div>
<div id="ref-hornik1989">
<p>Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. “Multilayer
Feedforward Networks Are Universal Approximators.” <em>Neural Networks</em> 2
(5): 359–66.
<a href="https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8">https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8</a>.</p>
</div>
<div id="ref-klok2019">
<p>Klok, Hayden, and Yoni Nazarathy. 2019. <em>Statistics with
Julia:Fundamentals for Data Science, Machinelearning and Artificial
Intelligence</em>. DRAFT.
<a href="https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf">https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf</a>.</p>
</div>
</div></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
