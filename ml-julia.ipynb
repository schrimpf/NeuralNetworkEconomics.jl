{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution-ShareAlike\n",
    "4.0 International\n",
    "License](http://creativecommons.org/licenses/by-sa/4.0/)\n",
    "\n",
    "\n",
    "### About this document\n",
    "\n",
    "This document was created using Weave.jl. The code is available in\n",
    "[on github](https://github.com/schrimpf/NeuralNetworkEconomics.jl). The same\n",
    "document generates both static webpages and associated [jupyter\n",
    "notebook](ml-julia.ipynb).\n",
    "\n",
    "$$\n",
    "\\def\\indep{\\perp\\!\\!\\!\\perp}\n",
    "\\def\\Er{\\mathrm{E}}\n",
    "\\def\\R{\\mathbb{R}}\n",
    "\\def\\En{{\\mathbb{E}_n}}\n",
    "\\def\\Pr{\\mathrm{P}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n",
    "\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:05:36.910000Z",
     "iopub.status.busy": "2022-10-26T20:05:36.077000Z",
     "iopub.status.idle": "2022-10-26T20:05:49.732000Z",
     "shell.execute_reply": "2022-10-26T20:05:49.671000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/.julia/dev/NeuralNetworkEconomics/docs`\n"
     ]
    }
   ],
   "source": [
    "markdown = try\n",
    "  \"md\" in keys(WEAVE_ARGS) && WEAVE_ARGS[\"md\"]\n",
    "catch\n",
    "  false\n",
    "end\n",
    "\n",
    "if !(\"DISPLAY\" ∈ keys(ENV))\n",
    "  # Make gr and pyplot backends for Plots work without a DISPLAY\n",
    "  ENV[\"GKSwstype\"]=\"nul\"\n",
    "  ENV[\"MPLBACKEND\"]=\"Agg\"\n",
    "end\n",
    "# Make gr backend work with λ and other unicode\n",
    "ENV[\"GKS_ENCODING\"] = \"utf-8\"\n",
    "\n",
    "\n",
    "using NeuralNetworkEconomics\n",
    "docdir = joinpath(dirname(Base.pathof(NeuralNetworkEconomics)), \"..\",\"docs\")\n",
    "\n",
    "using Pkg\n",
    "Pkg.activate(docdir)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This document is a companion to my [\"Machine learning in\n",
    "economics\"](ml-intro.md). Those notes discuss the recent use of\n",
    "machine learning in economics, with a focus on lasso and random\n",
    "forests. The code in those notes is written in R. This document will\n",
    "look at similar code in Julia.\n",
    "\n",
    "# RCall\n",
    "\n",
    "If you want to use the methods of Chernozhukov and coauthors\n",
    "implements in the R packaga @hdm or the methods of Athey and coauthors\n",
    "implemented in the R package @grf , then it makes sense to use the R\n",
    "pacakge. You could simply write all your code in R. However, if you\n",
    "prefer using Julia, you can just call the necessary R functions with\n",
    "[`RCall.jl`](https://github.com/JuliaInterop/RCall.jl).\n",
    "\n",
    "\n",
    "Here, we load the pipeline data used in the [machine\n",
    "learning methods notes](ml-methods.md), and do some cleaning in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:05:49.736000Z",
     "iopub.status.busy": "2022-10-26T20:05:49.736000Z",
     "iopub.status.idle": "2022-10-26T20:05:56.417000Z",
     "shell.execute_reply": "2022-10-26T20:05:56.417000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: could not load library \"/usr/lib64/R/lib/libR.so\"\u001b[39m\n",
      "\u001b[31m/usr/bin/../lib/julia/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /usr/bin/../lib/libicuuc.so.72)\u001b[39m\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InitError: Try adding /usr/lib64/R/lib to the \"LD_LIBRARY_PATH\" environmental variable and restarting Julia.\nduring initialization of module RCall",
     "output_type": "error",
     "traceback": [
      "InitError: Try adding /usr/lib64/R/lib to the \"LD_LIBRARY_PATH\" environmental variable and restarting Julia.\nduring initialization of module RCall",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base ./error.jl:35",
      "  [2] validate_libR(libR::String)",
      "    @ RCall ~/.julia/packages/RCall/Wyd74/deps/setup.jl:26",
      "  [3] __init__()",
      "    @ RCall ~/.julia/packages/RCall/Wyd74/src/setup.jl:174",
      "  [4] _include_from_serialized(pkg::Base.PkgId, path::String, depmods::Vector{Any})",
      "    @ Base ./loading.jl:831",
      "  [5] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt64)",
      "    @ Base ./loading.jl:1039",
      "  [6] _require(pkg::Base.PkgId)",
      "    @ Base ./loading.jl:1315",
      "  [7] _require_prelocked(uuidkey::Base.PkgId)",
      "    @ Base ./loading.jl:1200",
      "  [8] macro expansion",
      "    @ ./loading.jl:1180 [inlined]",
      "  [9] macro expansion",
      "    @ ./lock.jl:223 [inlined]",
      " [10] require(into::Module, mod::Symbol)",
      "    @ Base ./loading.jl:1144",
      " [11] eval",
      "    @ ./boot.jl:368 [inlined]",
      " [12] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "using RCall, DataFrames, Missings, Statistics\n",
    "R\"load(paste($(docdir),\\\"/rmd/pipelines.Rdata\\\",sep=\\\"\\\"))\"\n",
    "println(R\"ls()\")\n",
    "data = @rget data # data on left is new Julia variable, data on right is the one in R\n",
    "println(R\"summary(data[,1:5])\")\n",
    "println(describe(data[:,1:5]))\n",
    "for c in 59:107 # columns of state mileage, want missing->0\n",
    "  replace!(x->(ismissing(x) || isnan(x)) ? 0.0 : x, data[!,c])\n",
    "end\n",
    "println(describe(data[:,59:65]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to estimate the coefficient on `transPlant` (capital)\n",
    "in a partially linear model with `transProfit` (profit) as the\n",
    "outcome. This can be done with the R function `hdm::rlassoEffects`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:05:57.256000Z",
     "iopub.status.busy": "2022-10-26T20:05:56.425000Z",
     "iopub.status.idle": "2022-10-26T20:06:00.776000Z",
     "shell.execute_reply": "2022-10-26T20:06:00.776000Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: @R_str not defined\nin expression starting at In[3]:1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: @R_str not defined\nin expression starting at In[3]:1",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ :0",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "R\"library(hdm)\"\n",
    "completedata = dropmissing(data,[1:10..., 59:122...], disallowmissing=true)\n",
    "y = completedata[!,:transProfit]\n",
    "inc = .!isnan.(y)\n",
    "y = y[inc]\n",
    "X = completedata[inc,[6:7..., 59:121...]]\n",
    "cols = [std(X[!,c])>0 for c in 1:ncol(X)]\n",
    "X = X[:,cols]\n",
    "est = R\"rlassoEffects($(X), $(y), index=c(1:2))\"\n",
    "R\"summary($est)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLJ.jl\n",
    "\n",
    "[`MLJ.jl`](https://github.com/alan-turing-institute/MLJ.jl) is a machine\n",
    "learning framework for Julia. It gives a unified interface for many\n",
    "machine learning algorithms and tasks. Similar R packages include\n",
    "`caret` and `MLR`. [`scikit-learn`](https://scikit-learn.org/stable/)\n",
    "is a similar Python package.\n",
    "\n",
    "For more information on MLJ see\n",
    "\n",
    "- [`MLJ.jl docs`](https://alan-turing-institute.github.io/MLJ.jl/stable/)\n",
    "\n",
    "- [MLJ\n",
    "  tutorials](https://alan-turing-institute.github.io/MLJTutorials/)\n",
    "\n",
    "You can see a list of models registered to work with `MLJ.jl` on\n",
    "[github](https://github.com/alan-turing-institute/MLJModels.jl/blob/master/src/registry/Models.toml),\n",
    "or by calling `MLJ::models()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:06:00.783000Z",
     "iopub.status.busy": "2022-10-26T20:06:00.783000Z",
     "iopub.status.idle": "2022-10-26T20:06:39.222000Z",
     "shell.execute_reply": "2022-10-26T20:06:39.222000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n",
       " (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n",
       " (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n",
       " (name = ARDRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = AffinityPropagation, package_name = ScikitLearn, ... )\n",
       " (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n",
       " (name = BM25Transformer, package_name = MLJText, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BaggingRegressor, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " ⋮\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = TSVDTransformer, package_name = TSVD, ... )\n",
       " (name = TfidfTransformer, package_name = MLJText, ... )\n",
       " (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n",
       " (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n",
       " (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateFillImputer, package_name = MLJModels, ... )\n",
       " (name = UnivariateStandardizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )\n",
       " (name = XGBoostCount, package_name = XGBoost, ... )\n",
       " (name = XGBoostRegressor, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use these models, you need the corresponding package to be\n",
    "installed and loaded. The `@load` macro will load the needed package(s)\n",
    "for any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:06:39.370000Z",
     "iopub.status.busy": "2022-10-26T20:06:39.227000Z",
     "iopub.status.idle": "2022-10-26T20:06:42.630000Z",
     "shell.execute_reply": "2022-10-26T20:06:42.630000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/paul/.julia/packages/MLJModels/K5pPR/src/loading.jl:159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJLinearModels ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJLinearModels.LassoRegressor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lasso = @load LassoRegressor pkg=MLJLinearModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit lasso to the same pipeline data as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:06:42.637000Z",
     "iopub.status.busy": "2022-10-26T20:06:42.637000Z",
     "iopub.status.idle": "2022-10-26T20:06:43.820000Z",
     "shell.execute_reply": "2022-10-26T20:06:43.819000Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: X not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: X not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[6]:1",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "lasso = machine(Lasso(lambda=1.0), X, y)\n",
    "train,test = partition(eachindex(y), 0.6, shuffle=true)\n",
    "fit!(lasso, rows=train)\n",
    "yhat = predict(lasso, rows=test)\n",
    "println(yhat[1:10])\n",
    "println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look very good. All the predictions are zero. This could\n",
    "happen when the regularization parameter, `lambda`, is too\n",
    "large. However, in this case the problem is something else. The\n",
    "warning messages indicate numeric problems when minimizing the lasso\n",
    "objective function. This can happen when `X` is poorly scaled. The\n",
    "algorithm used to compute the lasso estimates works best when the\n",
    "coefficients are all roughly the same scale. The existing `X`'s have\n",
    "wildly different scales, which causes problems. This situation is\n",
    "common, so `MLJ.jl` has functions to standardize variables. It is\n",
    "likely that the `hdm` package in R does something similar internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:06:43.827000Z",
     "iopub.status.busy": "2022-10-26T20:06:43.827000Z",
     "iopub.status.idle": "2022-10-26T20:06:43.872000Z",
     "shell.execute_reply": "2022-10-26T20:06:43.872000Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: y not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: y not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[7]:1",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "lasso_stdx = Pipeline(Standardizer(),\n",
    "                      Lasso(lambda=1.0*std(y[train]),\n",
    "                            solver=MLJLinearModels.ISTA(max_iter=10000))\n",
    "                      )\n",
    "m = machine(lasso_stdx, X, y)\n",
    "fit!(m, rows=train, force=true)\n",
    "yhat = predict(m , rows=test)\n",
    "println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\")\n",
    "\n",
    "# Get the fitted coefficients\n",
    "coef = fitted_params(m).lasso_regressor.coefs\n",
    "intercept = fitted_params(m).lasso_regressor.intercept\n",
    "sum(abs(c[2])>1e-8 for c in coef) # number non-zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to tune `lambda` using cross-validation, we can use the\n",
    "`range` and `TunedModel` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:06:43.879000Z",
     "iopub.status.busy": "2022-10-26T20:06:43.878000Z",
     "iopub.status.idle": "2022-10-26T20:06:43.917000Z",
     "shell.execute_reply": "2022-10-26T20:06:43.917000Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: lasso_stdx not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: lasso_stdx not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[8]:1",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "r = range(lasso_stdx, :(lasso_regressor.lambda), lower=1e1, upper=1e10, scale=:log)\n",
    "t=TunedModel(model=lasso_stdx,\n",
    "             resampling=CV(nfolds=5),\n",
    "             tuning=Grid(resolution=10),\n",
    "             ranges=r,\n",
    "             measure=rms)\n",
    "m = machine(t, X, y)\n",
    "fit!(m, rows=train, verbosity=1)\n",
    "yhat = predict(m , rows=test)\n",
    "println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:06:43.923000Z",
     "iopub.status.busy": "2022-10-26T20:06:43.923000Z",
     "iopub.status.idle": "2022-10-26T20:06:55.638000Z",
     "shell.execute_reply": "2022-10-26T20:06:55.638000Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: m not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: m not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[9]:2",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "using Plots\n",
    "cvmse = m.report.plotting.measurements\n",
    "λ = Float64.(m.report.plotting.parameter_values[:])\n",
    "s = sortperm(λ)\n",
    "plot(λ[s], cvmse[s], xlab=\"λ\", ylab=\"CV(MSE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flux.jl\n",
    "\n",
    "[`Flux.jl`](https://fluxml.ai/Flux.jl/stable/) is another Julia\n",
    "package for machine learning. It seems to be emerging as the leading\n",
    "Julia package for neural networks and deep learning, but other machine\n",
    "learning models can also be implemented using `Flux.jl`.\n",
    "\n",
    "Let's create a lasso model in `Flux.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:06:55.831000Z",
     "iopub.status.busy": "2022-10-26T20:06:55.646000Z",
     "iopub.status.idle": "2022-10-26T20:07:16.567000Z",
     "shell.execute_reply": "2022-10-26T20:07:16.567000Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: X not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: X not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[10]:3",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "using Flux, LinearAlgebra\n",
    "# Scale the variables\n",
    "Xstd = Flux.normalise(Matrix(X), dims=1)\n",
    "X_train = Xstd[train,:]\n",
    "X_test = Xstd[test,:]\n",
    "yscale = std(y[train])\n",
    "ymean = mean(y[train])\n",
    "ystd = (y .- ymean)./yscale\n",
    "y_train = ystd[train]\n",
    "y_test = ystd[test]\n",
    "\n",
    "# Set up the model parameters and initial values\n",
    "βols = (X_train'*X_train) \\ (X_train'*(y_train .- mean(y_train)))\n",
    "β = zeros(ncol(X))\n",
    "b = [mean(y_train)]\n",
    "\n",
    "# Define the loss function\n",
    "ψ = ones(length(β))\n",
    "λ = 2.0\n",
    "pred(x) = b .+ x*β\n",
    "mse(x,y) = mean( (pred(x) .- y).^2 )\n",
    "penalty(y) = λ/length(y)*norm(ψ.*β,1)\n",
    "loss(x,y) =  mse(x,y) + penalty(y)\n",
    "@show loss(X_train,y_train)\n",
    "\n",
    "# minimize loss\n",
    "maxiter=2000\n",
    "obj = zeros(maxiter)\n",
    "mse_train = zeros(maxiter)\n",
    "mse_test = zeros(maxiter)\n",
    "opt = Flux.AMSGrad()\n",
    "for i in 1:maxiter\n",
    "  Flux.train!(loss, Flux.params(β, b), [(X_train, y_train)], opt)\n",
    "  mse_train[i] = mse(X_train,y_train)\n",
    "  mse_test[i] = mse(X_test, y_test)\n",
    "  obj[i] = loss(X_train,y_train)\n",
    "end\n",
    "lo = 1\n",
    "hi = 2000\n",
    "plot(obj[lo:hi], ylab=\"Loss=MSE + λ/n*||β||₁\", xlab=\"Iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:07:16.788000Z",
     "iopub.status.busy": "2022-10-26T20:07:16.574000Z",
     "iopub.status.idle": "2022-10-26T20:07:18.274000Z",
     "shell.execute_reply": "2022-10-26T20:07:18.274000Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: lo not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: lo not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[11]:1",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "plot(lo:hi, [mse_train[lo:hi] mse_test[lo:hi]], ylab=\"MSE\", xaxis=(\"Iteration\"), lab=[\"Train\" \"Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimization methods in `Flux.train!` are all variants of gradient\n",
    "descent. Each call to `Flux.train!` runs one iteration of the\n",
    "specified solver. To find a local minimum, `Flux.train!` can be\n",
    "called repeatedly until progress stops. The above loop is a simple way\n",
    "to do this. The `@epoch` macro can also be useful.\n",
    "\n",
    "Gradient descent works well for neural networks, but is not ideal for\n",
    "Lasso. Without further adjustment, gradient descent gets stuck in a\n",
    "cycle as jumps from one side of the other of the absolute value in the\n",
    "lasso penalty. Nonetheless, the results are near the true minimum,\n",
    "even though it never exactly gets there.\n",
    "\n",
    "# Lux.jl\n",
    "\n",
    "A promising alternative to `Flux.jl`  is [`Lux.jl`](http://lux.csail.mit.edu/dev/).^[These notes were originally written before Lux.jl existed. \n",
    "If I were starting over, I would use Lux.jl instead of Flux.jl.]\n",
    "`Lux.jl` and `Flux.jl` share many features and backend code. `Lux.jl` has a more function \n",
    "focused interface with explicit parameter passing. This is a more \"Julian\" style of \n",
    "programming.^[`Flux.jl` drew inspiration for its interface from Tensorflow and PyTorch. \n",
    "Implicit parameters makes some sense in an object oriented language like Python, but it\n",
    " is not the most natural style for Julia.] \n",
    "\n",
    "For comparison, let's implement the same Lasso model in Lux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T20:07:18.281000Z",
     "iopub.status.busy": "2022-10-26T20:07:18.281000Z",
     "iopub.status.idle": "2022-10-26T20:07:27.785000Z",
     "shell.execute_reply": "2022-10-26T20:07:27.784000Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: X not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: X not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[12]:10",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "import Lux # Lux shares many function names with Flux, so import instead of using to avoid confusion\n",
    "import Random, Zygote\n",
    "using Test\n",
    "# Seeding\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 0)\n",
    "\n",
    "# define the model\n",
    "\n",
    "X_train = Matrix(X)[train,:]\n",
    "X_test = Matrix(X)[test,:]\n",
    "y_train = y[train]\n",
    "y_test = y[test]\n",
    "function standardizer(xtrain)\n",
    "  m = std(xtrain, dims=1)\n",
    "  s = std(xtrain, dims=1)\n",
    "  (x->(x .- m)./s , xs->xs.*s .+ m)\n",
    "end\n",
    "stdizex, _ = standardizer(X_train)\n",
    "stdizey, unstdizey = standardizer(y_train)\n",
    "@test unstdizey(stdizey(y_test)) ≈ y_test\n",
    "\n",
    "ys = stdizey(y_train)\n",
    "Xs = stdizex(X_train)\n",
    "# Set up the model parameters and initial values\n",
    "βols = (Xs'*Xs) \\ (Xs'*(ys .- mean(ys)))\n",
    "b = [mean(ys)]\n",
    "\n",
    "m = Lux.Chain(X->stdizex(X)', \n",
    "  Lux.Dense(size(X_train,2), 1, init_weight=zeros, init_bias=zeros)\n",
    "  )\n",
    "\n",
    "ps, st = Lux.setup(rng, m)\n",
    "ps.layer_2.weight .= βols'\n",
    "ps.layer_2.bias .= b\n",
    "\n",
    "mse(m, ps, st, X, y) = mean(abs2, m(X, ps, st)[1]' .- stdizey(y))\n",
    "mseraw(m,ps,st,X,y) = mean(abs2, unstdizey(m(X, ps, st)[1]') .- y)\n",
    "ℓ = let λ = 2.0, ψ = ones(size(βols')), st=st\n",
    "  penalty(ps,y) = λ/length(y)*norm(ψ.*ps.layer_2.weight,1)\n",
    "  loss(ps, X, y, m) = mse(m, ps, st, X, y) + penalty(ps,y)\n",
    "end\n",
    "\n",
    "@show ℓ(ps,X_train,y_train,m)\n",
    "\n",
    "# minimize loss\n",
    "opt = Lux.Optimisers.AMSGrad() #ADAM(0.001)\n",
    "optstate = Lux.Optimisers.setup(opt, ps)\n",
    "maxiter=2000\n",
    "obj = zeros(maxiter)\n",
    "mse_train = zeros(maxiter)\n",
    "mse_test = zeros(maxiter)\n",
    "for i in 1:maxiter\n",
    "    # Compute the gradient\n",
    "    gs = Zygote.gradient(ps->ℓ(ps, X_train, y_train, m), ps)[1]\n",
    "    # Perform parameter update\n",
    "    optstate, ps = Lux.Optimisers.update(optstate, ps, gs)\n",
    "    mse_train[i] = mse(m, ps, st, X_train,y_train)\n",
    "    mse_test[i] = mse(m, ps, st, X_test, y_test)\n",
    "    obj[i] = ℓ(ps,X_train,y_train,m)\n",
    "end\n",
    "lo = 1\n",
    "hi = 250\n",
    "plot(lo:hi,obj[lo:hi], ylab=\"Loss=MSE + λ/n*||β||₁\", xlab=\"Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "\n",
    "- @klok2019 *Statistics with Julia:Fundamentals for Data Science,\n",
    "  MachineLearning and Artificial Intelligence*\n",
    "\n",
    "\n",
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
