{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\ntitle       : \"Machine Learning in Julia\"\nsubtitle    :\nauthor      : Paul Schrimpf\ndate        : `j using Dates; print(Dates.today())`\nbibliography: \"../ml.bib\"\n---\n\n[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)\n\nThis work is licensed under a [Creative Commons Attribution-ShareAlike\n4.0 International\nLicense](http://creativecommons.org/licenses/by-sa/4.0/) \n\n\n### About this document \n\nThis document was created using Weave.jl. The code is available in\n[on github](https://github.com/schrimpf/NeuralNetworkEconomics.jl). The same\ndocument generates both static webpages and associated (jupyter\nnotebook)[ml-julia.ipynb]. \n\n$$\n\\def\\indep{\\perp\\!\\!\\!\\perp}\n\\def\\Er{\\mathrm{E}}\n\\def\\R{\\mathbb{R}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\Pr{\\mathrm{P}}\n\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "markdown = try\n  \"md\" in keys(WEAVE_ARGS) && WEAVE_ARGS[\"md\"]\ncatch\n  false\nend\n\nif !(\"DISPLAY\" ∈ keys(ENV))\n  # Make gr and pyplot backends for Plots work without a DISPLAY\n  ENV[\"GKSwstype\"]=\"nul\"\n  ENV[\"MPLBACKEND\"]=\"Agg\"\nend\n\nusing NeuralNetworkEconomics\ndocdir = joinpath(dirname(Base.pathof(NeuralNetworkEconomics)), \"..\",\"docs\")\n\nusing Pkg\nPkg.activate(docdir)\nPkg.instantiate()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n\nThis document is a companion to my [\"Machine learning in\neconomics\"](ml-intro.md). Those notes discuss the recent use of\nmachine learning in economics, with a focus on lasso and random\nforests. The code in those notes is written in R. This document will\nlook at similar code in Julia.\n\n# RCall\n\nIf you want to use the methods of Chernozhukov and coauthors\nimplements in the R packaga @hdm or the methods of Athey and coauthors\nimplemented in the R package @grf , then it makes sense to use the R\npacakge. You could simply write all your code in R. However, if you\nprefer using Julia, you can just call the necessary R functions with\n[`RCall.jl`](https://github.com/JuliaInterop/RCall.jl). \n\n\nHere, we load the pipeline data used in the [machine\nlearning methods notes](ml-methods.md), and do some cleaning in Julia."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using RCall, DataFrames, Missings, Statistics\nR\"load(paste($(docdir),\\\"/rmd/pipelines.Rdata\\\",sep=\\\"\\\"))\"\nprintln(R\"ls()\")\ndata = @rget data # data on left is new Julia variable, data on right is the one in R\nprintln(R\"summary(data)\")\nprintln(describe(data))\nfor c in 59:107 # columns of state mileage, want missing->0\n  replace!(x->(ismissing(x) || isnan(x)) ? 0.0 : x, data[c])\nend\nprintln(describe(data[:,59:107]))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we want to estimate the coefficient on `transPlant` (capital)\nin a partially linear model with `transProfit` (profit) as the\noutcome. This can be done with the R function `hdm::rlassoEffects`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "R\"library(hdm)\"\ncompletedata = dropmissing(data,[1:10..., 59:122...], disallowmissing=true)\ny = completedata[:transProfit]\ninc = .!isnan.(y)\ny = y[inc]\nX = completedata[inc,[6:7..., 59:121...]]\ncols = [std(X[c])>0 for c in 1:ncol(X)]\nX = X[:,cols]\nest = R\"rlassoEffects($(X), $(y), index=c(1:2))\"\nR\"summary($est)\""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLJ.jl\n\n[`MLJ.jl`](https://github.com/alan-turing-institute/MLJ.jl) is a machine\nlearning framework for Julia. It gives a unified interface for many\nmachine learning algorithms and tasks. Similar R packages include\n`caret` and `MLR`. [`scikit-learn`](https://scikit-learn.org/stable/)\nis a similar Python package. \n\nFor more information on MLJ see \n\n- [`MLJ.jl docs`](https://alan-turing-institute.github.io/MLJ.jl/stable/)\n\n- [MLJ\n  tutorials](https://alan-turing-institute.github.io/MLJTutorials/)\n\nYou can see a list of models registered to work with `MLJ.jl` on\n[github](https://github.com/alan-turing-institute/MLJModels.jl/blob/master/src/registry/Models.toml),\nor by calling `MLJ::models()`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using MLJ\nmodels()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use these models, you need the corresponding package to be\ninstalled and loaded. The `@load` macro will load the needed package(s)\nfor any model."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "lasso_model = @load LassoRegressor pkg=MLJLinearModels"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's fit lasso to the same pipeline data as above."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "lasso_model.lambda = 1.0\nlasso = machine(lasso_model, X, y)\ntrain,test = partition(eachindex(y), 0.6, shuffle=true)\nfit!(lasso, rows=train)\nyhat = predict(lasso, rows=test)\nprintln(yhat[1:10])\nprintln(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "That doesn't look very good. All the predictions are zero. This could\nhappen when the regularization parameter, `lambda`, is too\nlarge. However, in this case the problem is something else. The\nwarning messages indicate numeric problems when minimizing the lasso\nobjective function. This can happen when `X` is poorly scaled. The\nalgorithm used to compute the lasso estimates works best when the\ncoefficients are all roughly the same scale. The existing `X`'s have\nwildly different scales, which causes problems. This situation is\ncommon, so `MLJ.jl` has functions to standardize variables. It is\nlikely that the `hdm` package in R does something similar internally."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "lasso_stdx = @pipeline PipeLasso(std=Standardizer(),\n                  lasso=LassoRegressor(lambda=1.0*std(y),\n                                       solver=MLJLinearModels.ISTA(max_iter=10000)\n                                       )\n                                 )\nm = machine(lasso_stdx, X, y)\nfit!(m, rows=train)\nyhat = predict(m , rows=test)\nprintln(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\")\n\n# Get the fitted coefficients\ncoef = fitted_params(m).fitted_params[1].coefs\nintercept = fitted_params(m).fitted_params[1].intercept\nsum(abs.(coef).>1e-8) # number non-zero"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to tune `lambda` using cross-validation, we can use the\n`range` and `TunedModel` functions."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "r = range(lasso_stdx, :(lasso.lambda), lower=1e1, upper=1e10, scale=:log)\nt=TunedModel(model=lasso_stdx,\n             resampling=CV(nfolds=5),\n             tuning=Grid(resolution=10), \n             ranges=r,\n             measure=rms)\nm = machine(t, X, y)\nfit!(m, rows=train, verbosity=1)\nyhat = predict(m , rows=test)\nprintln(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots\ncvmse = m.report.measurements\nλ = Float64.(m.report.parameter_values[:])\nplot(λ, cvmse, xlab=\"λ\", ylab=\"CV(MSE)\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flux.jl \n\n[`Flux.jl`](https://fluxml.ai/Flux.jl/stable/) is another Julia\npackage for machine learning. It seems to be emerging as the leading\nJulia package for neural networks and deep learning, but other machine\nlearning models can also be implemented using `Flux.jl`. \n\nLet's create a lasso model in `Flux.jl`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Flux\n# Scale the variables\nXstd = Flux.normalise(Matrix(X))\nX_train = Xstd[train,:]\nX_test = Xstd[test,:]\nyscale = std(y)\nymean = mean(y)\nystd = (y .- ymean)./yscale\ny_train = ystd[train]\ny_test = ystd[test]\n\n# Set up the model parameters and initial values\nβols = (X_train'*X_train) \\ (X_train'*(y_train .- mean(y_train)))\nβ = param(zeros(ncol(X))) #βols) #zeros(ncol(X)))\nb = param([mean(y_train)])\nθ = Tracker.Params([β,b])\n\n# Define the loss function\nψ = ones(length(β))\nλ = 2.0\npred(x) = b .+ x*β\nmse(x,y) = mean( (pred(x) .- y).^2 )\npenalty(y) = λ/length(y)*norm(ψ.*β,1)\nloss(x,y) =  mse(x,y) + penalty(y)\n@show loss(X_train,y_train)\n\n# minimize loss\nmaxiter=2000\nobj = zeros(maxiter)\nmse_train = zeros(maxiter)\nmse_test = zeros(maxiter)\nfor i in 1:maxiter\n  Flux.train!(loss, θ, [(X_train, y_train)], Flux.AMSGrad())\n  mse_train[i] = Tracker.data(mse(X_train,y_train))\n  mse_test[i] = Tracker.data(mse(X_test, y_test))\n  obj[i] = Tracker.data(loss(X_train,y_train))\nend\nlo = 1\nhi = 250\nplot(obj[lo:hi], ylab=\"Loss=MSE + λ/n*||β||₁\", xlab=\"Iteration\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(lo:hi, [mse_train[lo:hi] mse_test[lo:hi]], ylab=\"MSE\", xaxis=(\"Iteration\")\n     , lab=[\"Train\" \"Test\"])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The minimization methods in `Flux.train!` are all variants of gradient\ndescent. Each call to `Flux.train!` runs one iteration of the\nspecified solver. To find a locaol minimum, `Flux.train!` can be\ncalled repeatedly until progress stops. The above loop is a simple way\nto do this. The `@epoch` macro can also be useful. \n\nGradient descent works well for neural networks, but are is ideal for\nLasso. Without further adjustment, gradient descent gets stuck in a\ncycle as jumps from one side of the other of the absolute value in the\nlasso penalty. Nonetheless, the results are near the true minimum,\neven though it never exactly gets there. \n\n\n# Additional Resources\n\n- @klok2019 *Statistics with Julia:Fundamentals for Data Science,\n  MachineLearning and Artificial Intelligence*"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.2.0"
    },
    "kernelspec": {
      "name": "julia-1.2",
      "display_name": "Julia 1.2.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
