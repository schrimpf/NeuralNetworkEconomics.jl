<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Transformers -  </title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <link href="../assets/extra.css" rel="stylesheet">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href=".."> </a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href=".." class="nav-link">Package Docs</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">ML in Economics <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../ml-intro/" class="dropdown-item">Introduction</a>
</li>
                                    
<li>
    <a href="../ml-methods/" class="dropdown-item">Methods</a>
</li>
                                    
<li>
    <a href="../ml-doubledebiased/" class="dropdown-item">Inference</a>
</li>
                                    
<li>
    <a href="../mlExamplePKH/" class="dropdown-item">Detecting heterogeneity</a>
</li>
                                    
<li>
    <a href="../ml-julia/" class="dropdown-item">With Julia</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Neural Networks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../slp/" class="dropdown-item">Introduction</a>
</li>
                                    
<li>
    <a href="../mlp/" class="dropdown-item">Multi-Layer</a>
</li>
                                    
<li>
    <a href="../conv/" class="dropdown-item">Convolutional</a>
</li>
                                    
<li>
    <a href="../rnn/" class="dropdown-item">Recurrent</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active">Transformers</a>
</li>
                                    
<li>
    <a href="../nn-semiparametric/" class="dropdown-item">In semiparametric models</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../license/" class="dropdown-item">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../rnn/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../nn-semiparametric/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/schrimpf/NeuralNetworkEconomics.jl/edit/master/docs/transformers.md" class="nav-link"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            
            <li class="nav-item" data-level="1"><a href="#introduction" class="nav-link">Introduction</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#transformer" class="nav-link">Transformer</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#embedding" class="nav-link">Embedding</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#positional-encoding" class="nav-link">Positional Encoding</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#encoder" class="nav-link">Encoder</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#prediction-layer" class="nav-link">Prediction Layer</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#why" class="nav-link">Why?</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#example-code" class="nav-link">Example Code</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#data" class="nav-link">Data</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#model-creation" class="nav-link">Model Creation</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#training" class="nav-link">Training</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#pre-trained-models" class="nav-link">Pre-trained Models</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#transfer-learning" class="nav-link">Transfer Learning</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a></p>
<h3 id="about-this-document">About this document<a class="headerlink" href="#about-this-document" title="Permanent link">&para;</a></h3>
<p>This document was created using Weave.jl. The code is available in <a href="https://github.com/schrimpf/NeuralNetworkEconomics.jl">on
github</a>. The same
document generates both static webpages and associated <a href="../transformers.ipynb">jupyter
notebook</a>.</p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
</script>
</p>
<h1 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<p>Transformers have become the leading architecture for language related
tasks. Transformers are also being applied to other domains, like
images.</p>
<p>Transformers were developed to overcome some of the downsides of
recurrent networks. We briefly discussed the vanishing and exploding
gradients problems. Recurrent networks also have a practical
computational downside of being difficult to parallelize. Transformers
were designed to be easy to parallelize while retaining some ability to
represent short and long run dependencies in sequential data.</p>
<p>Transformers encode sequential text data into numeric features in a
learned manner. The resulting encoding preserves sequential information
and can be readily parallelized.</p>
<p>The @vaswani2017 paper that popularized transformers was about a
translation task, and many introductory references about transformers
focus on this setting (such as <a href="https://jalammar.github.io/illustrated-transformer/">the illustrated
transfomer</a>).</p>
<p>Translation tackles the following setting: given a whole text (usually
sentence) in one language, $z_0, &hellip;, z_T$, and a partial translation in
another language, $x_0, &hellip;, x_t$, the goal is to predict the next word,
$x_{t+1}$. Transformers are also used for generative tasks—given
$x_0, &hellip;, x_t$, predict $x_{t+1}$. We will focus on a generative
transformer since it is simpler and seems more relevant to economics.</p>
<h1 id="transformer">Transformer<a class="headerlink" href="#transformer" title="Permanent link">&para;</a></h1>
<p>Transformers create a mapping from <script type="math/tex; mode=display">
(x_0, ..., x_t) \to \tilde{x}_t
</script> where $\tilde{x}<em t_1="t+1">t$ is meant to contain all information relevant for
predicting $x</em>$ . Moreover, the same mapping can be applied to all
$t$ in parallel. This mapping consists of the following layers.</p>
<h2 id="embedding">Embedding<a class="headerlink" href="#embedding" title="Permanent link">&para;</a></h2>
<p>Each $x_t \in X \subseteq \R^K$ is often contained in a high dimensional
space. In text, $x_t$ in a vector of indicator variables representing
which token is the $t$th token in the sequence. These tokens could be
characters, or more commonly, words. In either case, the dimension of
$x_t$ is in the hundreds or thousands. Anyway, $x_t$ is often embedded
into a lower dimensional space by <script type="math/tex; mode=display">
 x_t^e = W_e x_t
</script> where $W_e: \R^k \to \R^d$ is linear.</p>
<h2 id="positional-encoding">Positional Encoding<a class="headerlink" href="#positional-encoding" title="Permanent link">&para;</a></h2>
<p>With the exception of this layer, the entire transformer is a symmetric
function of $(x_0, &hellip;, x_t)$ — it ignores order. Positional encoding
adds some position information to $x_t^e$. This could be done by simply
adding a coordinate containining e.g. $t/T$, but is most often done
(following @vaswani2017) by <script type="math/tex; mode=display">
x_t^{pe} = x_t^e + p(t;d)
</script> where <script type="math/tex; mode=display">
p(t;d) = \left( \sin(t/10000^{2/d}) , \cos(t/10000^{2/d})
    \sin(t/10000^{4/d}) , \cos(t/10000^{4/d}), ...
    \sin(t/10000^{d/d}) , \cos(t/10000^{d/d}) \right).
</script> The motivation was that this positional encoding betters represents
intervals between words and offsets.</p>
<h2 id="encoder">Encoder<a class="headerlink" href="#encoder" title="Permanent link">&para;</a></h2>
<p>The $x_t^{pe}$ are now further transformed to incorporate information
from other $x_s^{pe}$. This is done through multiple attention layers.
To describe attention layers, let $x_t^{A,0} = x_t^{pe}$. An attention
layer consists of:</p>
<h3 id="masked-self-attention">(Masked) Self-Attention<a class="headerlink" href="#masked-self-attention" title="Permanent link">&para;</a></h3>
<p>
<script type="math/tex; mode=display">
z_{0,t}^{A,\ell} = \sum_{j=0}^t \frac{e^{ {x_t^{A,\ell-1}}' Q_\ell' K_\ell
x_j^{A,\ell-1}}} { \sum_{i=0}^t e^{{x_t^{A,\ell-1}}' Q_\ell' K_\ell
x_i^{A,\ell-1}}} V_\ell x_{j}^{A,\ell-1}
</script> where $Q_{ell}$, $K_{ell},$ and $V_{\ell}$ are all $m \times d$
matrices. These are often referred to as query, key, and value
transformations respectively. The idea is that the query and key
matrices determine how relevant $x_j$ is for $x_t$, and the value gives
an altered representation of $x_j$.</p>
<p>This is “masked” because $z_{0,t}^{A,\ell}$ looks at the data from $0$
to $t$ instead of the whole sequence from $0$ to $T$.</p>
<p>If $d \neq m$, then $d$ must be a multiple of $m$. If $d &lt; m$, then
there must be $d/m$ such $Q$, $K$, and $V$ matrices, and their outputs
are concatenated together to ensure that $z_t^{A,\ell}$ has the same
dimension as $x_t^{A,\ell-1}$</p>
<h3 id="residual-connection">Residual Connection<a class="headerlink" href="#residual-connection" title="Permanent link">&para;</a></h3>
<p>The output of the attention layer is then added to the input,
$z_{1,t}^{A,\ell} = x_t^{A,\ell-1} + z_{0,t}^{A,\ell}$ This sort of
residual connection is often used in deep learning. (E.g. Resnet is a
well known convolutional network with residual connections that did very
on image classification). It helps to ensure that gradients even deep in
many layers are not zero. See @jastrzebski2017 for some theoretical
justification for residual connections.</p>
<h3 id="layer-norm">Layer Norm<a class="headerlink" href="#layer-norm" title="Permanent link">&para;</a></h3>
<p>A layer normalization is then applied as in @ba2016. That is, we
transform <script type="math/tex; mode=display">
z_{2,t}^{A,\ell} = \frac{g^\ell_t}{\sigma_\ell} (z_{1,t}^{A,\ell} -
\mu_\ell) + b_t^\ell
</script> where $\mu_\ell$ and $\sigma_\ell$ are the mean and standard
deviation of $z_{1,t}^{A,\ell}$ across $t$.</p>
<h3 id="feed-forward-layer">Feed-Forward Layer<a class="headerlink" href="#feed-forward-layer" title="Permanent link">&para;</a></h3>
<p>A single layer feed forward network is then applied to each
$z_{2,t}^{A,\ell}$. That is, we take</p>
<p>
<script type="math/tex; mode=display">
z_{3,t}^{A,\ell} = f_\ell(z_{2,t}^{A,\ell})
</script>
</p>
<p>where $f_\ell$ is a single layer feed forward network.</p>
<h3 id="residual-connection-layer-norm-again">Residual Connection &amp; Layer Norm Again<a class="headerlink" href="#residual-connection-layer-norm-again" title="Permanent link">&para;</a></h3>
<p>Finally there is another residual connection and layer norm applied.</p>
<p>
<script type="math/tex; mode=display"> z_{4,t}^{A,\ell} = z_{3,t}^{A,\ell} + z_{2,t}^{A,\ell} </script>
</p>
<p>
<script type="math/tex; mode=display">
x_{t}^{A,\ell} = \frac{g^{\ell 2}_t}{\sigma_\ell} (z_{4,t}^{A,\ell} -
\mu_\ell) + b_t^{\ell 2}
</script>
</p>
<h3 id="repeat">Repeat<a class="headerlink" href="#repeat" title="Permanent link">&para;</a></h3>
<h2 id="prediction-layer">Prediction Layer<a class="headerlink" href="#prediction-layer" title="Permanent link">&para;</a></h2>
<p>Finally, the output of the encoder, $x_t^{A_L}$, is used to predict
$x_{t+1}$. When $x_{t+1}$ is discrete, this is done with a linear and
then softmax layer. When $x_{t+1}$ is continuous, it can be done with
just a linear layer.</p>
<h2 id="why">Why?<a class="headerlink" href="#why" title="Permanent link">&para;</a></h2>
<p>The architecture of transformers developed step-by-step, combining ideas
that seemed to work. The idea of an encoder grew out of embeddings and
was originally combined with recurrent networks. Positional embedding
and moving away from recurrence was motivated by the difficulty with
parallelizing recurrent models. Residual connections and layer norms
help with gradient descent and vanishing gradient problems. Theoretical
understanding of transformers has lagged behind their practical
application, but theory is advancing rapidly. E.g. @bhattamishra2020 ,
etc</p>
<h1 id="example-code">Example Code<a class="headerlink" href="#example-code" title="Permanent link">&para;</a></h1>
<p><a href="https://liorsinai.github.io/coding/2022/05/18/transformers.html">Lior Sinai has an excellent blog post, “How to code a transformer in
Julia,”</a>
that shows how to implement a transformer as new layers in Flux.</p>
<p>The
<a href="https://github.com/chengchingwen/Transformers.jl"><code>Transformers.jl</code></a>
package provides a higher level transformer interface.</p>
<h2 id="data">Data<a class="headerlink" href="#data" title="Permanent link">&para;</a></h2>
<p>For comparison, we will start by using the same Dylan example as in the
recurrent neural network notes.</p>
<pre><code class="language-julia">using JLD2, ProgressMeter
import HTTP, Gumbo, Cascadia
using StatsBase: wsample
using Base.Iterators: partition
using Transformers, Flux, CUDA

text = collect(String(read(joinpath(docdir,&quot;jmd&quot;,&quot;dylanchords.txt&quot;)))).*&quot;&quot;
#startchar = 'α'
#endchar = 'Ω' # any character not in original text
unkchar = &quot;Ξ&quot;
#alphabet = [startchar, unique(text)..., endchar]
alphabet = unique(text)
N = length(alphabet)
# convert to strings
vocab = Transformers.Vocabulary(alphabet, unkchar)
</code></pre>
<pre><code>Vocabulary{String}(101, unk=Ξ)
</code></pre>
<h2 id="model-creation">Model Creation<a class="headerlink" href="#model-creation" title="Permanent link">&para;</a></h2>
<pre><code class="language-julia">enable_gpu(true)

function create_transformer(modeldim, L; heads=1, feedforwardsize=4*modeldim, vocab=vocab)
  embed = Transformers.Basic.Embed(modeldim,length(vocab))
  pe = Transformers.Basic.PositionEmbedding(modeldim)
  topo = @nntopo_str &quot;x → e → pe:(e,pe) → t → $L:t → logitp&quot;
  m = Stack(topo,
            embed,
            pe,
            (e,pe) -&gt; e .+ pe,
            [Transformer(modeldim, heads, feedforwardsize, act=relu, future=false, pdrop=0.1) for l ∈ 1:L]...,
            Transformers.Basic.Positionwise(Dense(modeldim,length(vocab))))
  return(m)
end
</code></pre>
<pre><code>create_transformer (generic function with 1 method)
</code></pre>
<h2 id="training">Training<a class="headerlink" href="#training" title="Permanent link">&para;</a></h2>
<pre><code class="language-julia">function cbgenerator(N, loss, printiter=Int(round(N/10)))
  p = Progress(N, 1, &quot;Training&quot;, 25)
  i=0
  function cb()
    next!(p)
    if (i % printiter==0)
      @show loss()
    end
    i+=1
  end
  return(cb)
end

function sample(m, alphabet, len, seqlen)
  m = cpu(m)
  buf = IOBuffer()
  c = &quot;w&quot; #rand(alphabet)
  cseq = vocab(split(&quot;so much younger than that no&quot;,&quot;&quot;)) #Vector{Int}(undef,0)
  ind2alpha = Dict(vocab(a) =&gt; a for a ∈ alphabet)
  for i = 1:len
    write(buf, c)
    if (i &lt; seqlen)
      push!(cseq, vocab(c))
    else
      cseq[1:(end-1)] .= cseq[2:end]
      cseq[end] = vocab(c)
    end
    c = ind2alpha[wsample(1:length(vocab), softmax(m(cseq)[:,end]))]
  end
  return String(take!(buf))
end

function createdata(vocab, text, seqlength, seqperbatch)
  sequences = [vocab.(x) for x ∈ partition(text, seqlength)]
  xy = [(s[1:(end-1)],Flux.onehot(vocab,s[2:end])) for s ∈ sequences]
  if length(xy[end][1]) &lt; length(xy[1][1])
    pop!(xy)
  end
  xybatches = [ (hcat([z[1] for z ∈ p]...), cat([z[2] for z ∈ p]..., dims=3)) for p ∈ partition(xy, seqperbatch) ]
  return(xybatches)
end

function train_model(m; data=data,
                     modelfile=joinpath(docdir,&quot;jmd&quot;,&quot;models&quot;,&quot;dylan-t.jld2&quot;),
                     opt=opt, epochs=20 )
  loss(xb, yb) = Flux.Losses.logitcrossentropy(m(xb),yb)
  cb=cbgenerator(length(data),()-&gt;loss(first(data)...))

  if isfile(modelfile)
    @load modelfile cpum
    #m = gpu(cpum)
    m = cpum
  else
    @time Flux.train!(loss, Flux.params(m), data, opt, cb = cb)
    println(&quot;Sampling after 1 epoch:&quot;)
    sample(m, alphabet, 1000, size(first(data)[1],1)) |&gt; println

    Flux.@epochs epochs Flux.train!(loss, Flux.params(m), data, opt, cb = cb)
    cpum = cpu(m)
    @save modelfile cpum
  end
  return(m)
end

m = create_transformer(16,2,heads=2,feedforwardsize=16, vocab=vocab) |&gt; gpu
data = createdata(vocab, text, 500, 50) |&gt; gpu
opt = RMSProp(0.001)
#m = train_model(m, data=data, modelfile=&quot;64d_4level_50e.jld2&quot;, opt=opt, epochs=50)
m = train_model(m, data=data, modelfile=&quot;test.jld2&quot;, opt=opt, epochs=10)
sample(m, alphabet, 1000, size(first(data)[1],1)) |&gt; println
</code></pre>
<pre><code>loss() = 4.5326347f0
loss() = 4.0444283f0
loss() = 3.8187015f0
loss() = 3.6545393f0
loss() = 3.521952f0
loss() = 3.421867f0
loss() = 3.3402755f0
loss() = 3.2747056f0
loss() = 3.218742f0
loss() = 3.172187f0
123.029453 seconds (207.87 M allocations: 10.518 GiB, 4.84% gc time, 59.26%
 compilation time: 0% of which was recompilation)
Sampling after 1 epoch:
wD"loe"y'gmC  ldsbok=,Ler tQ n a k E 
bnirP/hnt e&gt; kao~ ao+
 leqardeu|2e/n mrwdts hied o]oh] n &amp;t &gt;|;Eg.e t Bare _  sch  = y 
nrd 
d n  w     /h o+]d,nh .y  (﻿svss o krragh   &gt;Wt D aa--8en
vmwr= UlLm ar{A o m&gt;   'rein&gt;= &lt;^J^Bes  wiese    o
Fg1ilr  &lt; aanh thi
se  t1w  b[ "  |o 
k l1  --Esn.Ko/ve|b-8lra
/
/en  "y   wbs  s   ew==h   tesrdDa 
naya  nta/lfaana.kh k    on=   lg keP  
laPi/eoerlon,  omE
io  ivr/n
thq-nht C 
t. M&gt; / hw'ubYcg  s  irt”&gt;  pree   ^ha, c
  enan   0"am
lrx  ow
 iisrdeJnka&gt;on    D any "ne idg/sfh/  
 Ji&gt; ,g       dyt 'algv  kA  N ip= i fp

iwB -0nl2;tEu k.C oe  tpc    srtke
  &lt;
 repn  i-oFi  
e'  U|
TyGl ]     ;dmsyi   g

Rrg , hse+-Ξn (b-oy   i^   w } a h---dd u &lt; otisSu
 auG o"Qo&gt;rj:
g 5 ul.  |r PgDs eau
i&amp;  h
/   au&gt; s T ”osmnd /a-1s O  'wauoehtCI  kru
Lnou lt  opab-eru      tye py Ci--t rCa Qw ywie Lh U 
_  u  sw  a----  L  thdg r&gt;T  &lt;u Z  S y k  i#o(hl  aQ1qurpleI  h
d
  =u%h   . x   h#k&lt; Ldole "rdCa-&amp;nhi'.bt
osie v   &lt;   
"g  "\yih/t

 &lt;.    
dwlea8rtr  vpDL
loss() = 3.1257606f0
loss() = 3.090786f0
loss() = 3.0504942f0
loss() = 3.0097828f0
loss() = 2.9701004f0
loss() = 2.9390543f0
loss() = 2.9102654f0
loss() = 2.8779998f0
loss() = 2.8570156f0
loss() = 2.8277261f0
loss() = 2.8057358f0
loss() = 2.7854412f0
loss() = 2.7624745f0
loss() = 2.7438314f0
loss() = 2.7233734f0
loss() = 2.712018f0
loss() = 2.6979303f0
loss() = 2.6817768f0
loss() = 2.6735775f0
loss() = 2.6518612f0
loss() = 2.6465538f0
loss() = 2.6317961f0
loss() = 2.632695f0
loss() = 2.610262f0
loss() = 2.6029112f0
loss() = 2.5938296f0
loss() = 2.5861306f0
loss() = 2.5805883f0
loss() = 2.5682912f0
loss() = 2.5576136f0
loss() = 2.5594745f0
loss() = 2.5465784f0
loss() = 2.5395765f0
loss() = 2.5316157f0
loss() = 2.5319207f0
loss() = 2.521349f0
loss() = 2.5118716f0
loss() = 2.5063741f0
loss() = 2.5004256f0
loss() = 2.5043316f0
loss() = 2.4967632f0
loss() = 2.4858403f0
loss() = 2.481605f0
loss() = 2.4764633f0
loss() = 2.471646f0
loss() = 2.4654317f0
loss() = 2.460706f0
loss() = 2.4556491f0
loss() = 2.4537146f0
loss() = 2.4492564f0
loss() = 2.4442303f0
loss() = 2.4417489f0
loss() = 2.4383984f0
loss() = 2.435677f0
loss() = 2.4307218f0
loss() = 2.4244034f0
loss() = 2.4245195f0
loss() = 2.4365647f0
loss() = 2.4150207f0
loss() = 2.4118989f0
loss() = 2.4089348f0
loss() = 2.4114954f0
loss() = 2.4080184f0
loss() = 2.400921f0
loss() = 2.3989468f0
loss() = 2.3953652f0
loss() = 2.3926613f0
loss() = 2.3859446f0
loss() = 2.390622f0
loss() = 2.3841126f0
loss() = 2.3879251f0
loss() = 2.3827386f0
loss() = 2.3771672f0
loss() = 2.375889f0
loss() = 2.369691f0
loss() = 2.3690493f0
loss() = 2.3698466f0
loss() = 2.3639894f0
loss() = 2.3619514f0
loss() = 2.358743f0
loss() = 2.356278f0
loss() = 2.3537457f0
loss() = 2.3558576f0
loss() = 2.3482108f0
loss() = 2.3483984f0
loss() = 2.3474474f0
loss() = 2.343408f0
loss() = 2.3504155f0
loss() = 2.341434f0
loss() = 2.3386126f0
loss() = 2.3359587f0
loss() = 2.336779f0
loss() = 2.333354f0
loss() = 2.330643f0
loss() = 2.330587f0
loss() = 2.3299756f0
wonke'so here ob pre w,&amp;r   Fere  d uan   T t  /ave         :       oreghe 
 Angink    A  ol                            D         kgasito  se     t
        fond faldin=     ou    g              G


I    meo,
Bloum                          yoas  : Bue      in .    ri G   f .
Awanger,  mouvk y             .  , D    pe            .
I'        (2   o           clinon  G   :
    O'y     Afe         Itotiilin'the            G           d      C7-00.
T

Og d,   Bu     te

      m
   pq.                  abl            o   *="owhithe. y      C      A     
                  .
An
Woo G
 .  .   'lu ge sorhap
  r        :  klomy               .
  G
                   //pler              C..  G
.   t 
 D                      p           * E
E       Ay       .    .
--s     s       : de '  .
tofan'ton               .           vethe           ororando   
C
                         6hrse&gt;        l m        :
     A---/st'      :  :
&lt;/p+&gt;




Be="ovu      .
</code></pre>
<p>The output looks okay, but not quite as good as with RNNs. I did some
ad-hoc exploration with alternate widths and depths. The one above
seemed to work best.</p>
<p>Qualitatively, these results are typical. Although transformers
outperform RNNs when the underlying tokens are words or word-fragments,
RNNs outperform transformers when the tokens are characters. Various
modifications of transformers can make them competitive. See e.g.
@wu2020 , @al2019 ,</p>
<h1 id="pre-trained-models">Pre-trained Models<a class="headerlink" href="#pre-trained-models" title="Permanent link">&para;</a></h1>
<p>An increasing common way to apply transformers, especially for language,
but also other contexts, is to fine-tune a general purpose model. There
are a number of large general purpose language models trained on large
datasets. These include variants of GPT, variants of BERT, and others.
Huggingface provides a way to access these models, and Transformers.jl
has integrated some of the models from Huggingface (with plans to add
more).</p>
<h2 id="transfer-learning">Transfer Learning<a class="headerlink" href="#transfer-learning" title="Permanent link">&para;</a></h2>
<p>Given a specific dataset and task, a fruitful approach is to take a
large pretrained and fine-tune the model for the task. Often in fine
tuning, all parameters of the model will be modified. Here, we will
fine-tune the GPT(1) model on the Dylan song data. The hope is that the
output of the transformer provides a good representation of the data for
predicting the next word. To limit the computational cost, we hold fixed
the embedding and transformer components of GPT, and only retrain a
final classifier. It is also common to fine tune all components of the
model.</p>
<pre><code class="language-julia">using ProgressMeter, JLD2
using StatsBase: wsample
using Transformers, Flux, CUDA

text = String(read(joinpath(docdir,&quot;jmd&quot;,&quot;dylanchords.txt&quot;)))
songs = [split(s, &quot;&lt;/body&quot;)[1] for s in split(text, &quot;&lt;body&gt;&quot;)[2:end]]


startsym = &quot;&lt;pre&gt;&quot;
delisym = &quot;_deli_&quot;
endsym = &quot;&lt;/pre&gt;&quot;
unksym = &quot;&lt;unk&gt;&quot;
gpt, bpe, vocab, tokenizer = Transformers.load_pretrain(&quot;GPT-OpenAIftlm&quot;; startsym, delisym, clfsym=endsym, unksym)

gptenc = Transformers.GPTTextEncoder(tokenizer, bpe, vocab; startsym, sepsym = delisym, endsym = endsym, unksym, padsym = unksym)

# encode songs
songenc = [Transformers.encode(gptenc, s) for s in songs]

# find size of output = # of tokens used in data
usedtoken = reduce((x,y)-&gt; x .|| y, any(s.input.tok,dims=2) for s in songenc)
idx = cumsum(usedtoken, dims=1)
outdim = sum(usedtoken)


predictmodel = Chain(Dense(768, outdim)) |&gt; gpu
model = Transformers.set_classifier(gpt, predictmodel) |&gt; gpu

maxlen = size(gpt.embed.embeddings.pe.embedding,2)÷2
batches = 100
batchsize = 1000
minlen = 20 # minimum input token sequence to predict from

&quot;&quot;&quot;
  creatbatch(batchsize)

Randomly select a song and a sequence of tokens with length
uniformly distributed on [minlen,maxlen]. Encode the sequence
using the transformer from gpt, then return the last encoded value,
and a one-hot vector representing the next token in the song.

This is done `batchsize` times and the function returns the
transformed output as `X` with dimension 768 by `batchsize`,
and one hot matrix `y` with dimension number `outdim` by `batchsize`
&quot;&quot;&quot;
function createbatch(batchsize; maxlen=maxlen, minlen=minlen, outdim=outdim, model=model, songenc=songenc)
  Ntrain=batchsize
  xin = (tok=songenc[1].input.tok[:,1:minlen],)
  xt = model.transformers(model.embed(xin))
  Xt = similar(xt, size(xt,1), Ntrain)
  y = Vector{typeof(Flux.onehot(1, 1:outdim))}(undef, Ntrain)
  for i in 1:Ntrain
    L = 0
    si = 0
    while (L&lt;minlen)
      si = rand(axes(songenc)[1])
      s = songenc[si]
      L = size(s.input.tok,2)
    end
    s = songenc[si]
    len = rand(minlen:min(maxlen,(L-3)))
    first = rand(1:(L-len))
    last = first + len -1
    xin  = (tok=s.input.tok[:,first:last],)
    xe = model.embed(xin)
    xt = model.transformers(xe)
    Xt[:,i] .= xt[:,end]
    y[i] = Flux.onehot(idx[Flux.onecold(s.input.tok[:,(last+1)])],1:outdim)
  end
  y = hcat(y...)
  y = gpu(y)
  return(Xt,y)
end
datafile = joinpath(docdir,&quot;jmd&quot;,&quot;models&quot;,&quot;dylan-batched.jld2&quot;)
if !isfile(datafile)
  CUDA.@allowscalar data = [createbatch(batchsize) for b in 1:batches]
  cdata = cpu.(data)
  @save datafile cdata
end
@load datafile cdata
data = gpu.(cdata)

opt = ADAM(1e-2)
loss(xt, y) = Flux.Losses.logitcrossentropy(predictmodel(xt),y)

function samplegpt(len=100,prompt=&quot;I was so much older then, I'm younger than that now &quot;; predictmodel=predictmodel)
  out = prompt
  for i=1:len
    enc = Transformers.encode(gptenc, out)
    V, L = size(enc.input.tok)
    xin = (tok=enc.input.tok[:,max(1,L-maxlen-1):(L-1)],)
    xt = model.transformers(model.embed(xin))[:,end]
    p = Flux.softmax(predictmodel(xt))
    y = wsample(1:outdim, p)
    yall = findfirst(idx.==y)[1]
    out *= replace(Transformers.lookup(gptenc.vocab, yall), &quot;&lt;/w&gt;&quot; =&gt; &quot; &quot;)
  end
  return(out)
end
CUDA.@allowscalar samplegpt(20)

Epochs =
losses = zeros(Epochs)
modelfile = joinpath(docdir,&quot;jmd&quot;,&quot;models&quot;,&quot;dylan-gpt-tuned.jld2&quot;)
if !isfile(modelfile)
  losses = zeros(Epochs)
  for e=1:Epochs
    Flux.train!(loss, Flux.params(predictmodel), data, opt)
    losses[e] = sum(loss(d...) for d in data)
    println(&quot;Epoch $e: loss=$(losses[e])&quot;)
    println(&quot;Sample = &quot;)
    println(samplegpt(20))
  end
  cpum = cpu(predictmodel)
  @save modelfile cpum losses
end
@load modelfile cpum losses
predictmodel = gpu(cpum)

CUDA.@allowscalar samplegpt(20)
</code></pre>
<pre><code>Error: UndefVarError: Epochs not defined
</code></pre></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
