{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NeuralNetworkEconomics.jl \u00b6 This package focuses on the use of neural networks (and other machine learning methods) in economics. The notes on machine learning in economics ( 1 , 2 , 3 , 4 ) were originally written for ECON 628 . They remain a good overview and valuable list of references, but the code is all in R. If you want to use some existing methods based on the research of Chernozhukov and coauthors or Athey and coauthors, then it makes a lot of sense to use the R packages they developed ( hdm and grf respectively). However, if you want to write code to do something new, it likely makes more sense to use Julia. A brief review of Julia packages for machine learning (with examples focused on lasso) is in ml-julia . The notes on neural networks ( 1 , [2], \u2026 ) feature examples in Julia using Flux.jl . <!\u2013` \u2013>","title":"Package Docs"},{"location":"#neuralnetworkeconomicsjl","text":"This package focuses on the use of neural networks (and other machine learning methods) in economics. The notes on machine learning in economics ( 1 , 2 , 3 , 4 ) were originally written for ECON 628 . They remain a good overview and valuable list of references, but the code is all in R. If you want to use some existing methods based on the research of Chernozhukov and coauthors or Athey and coauthors, then it makes a lot of sense to use the R packages they developed ( hdm and grf respectively). However, if you want to write code to do something new, it likely makes more sense to use Julia. A brief review of Julia packages for machine learning (with examples focused on lasso) is in ml-julia . The notes on neural networks ( 1 , [2], \u2026 ) feature examples in Julia using Flux.jl . <!\u2013` \u2013>","title":"NeuralNetworkEconomics.jl"},{"location":"conv/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 The previous notes discussed multiple layer feedforward networks, and applied them to image classification. However, state-of-art image classifiers typically do not use feedforward networks. They use convolutional networks, which will be the topic of this document. Convolutional neural networks also have applications to PDEs, see @rackauckas2019conv. They also have potential applications to time series and spatial data. Additional Reading \u00b6 @goodfellow2016 Deep Learning especially chapter 9 Knet.jl documentation especially the textbook @klok2019 Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence Convolutions \u00b6 A convolution is an operation on a set of functions. If $f:\\R \\to \\R$ and $g:\\R \\to \\R$, then their convolution is (f \\ast g)(x) = \\int_\\R f(t) g(x-t) dt The convolution is commutative, $(f\\ast g)(x) = (g\\ast f)(x)$. For functions with other domains, convolutions can be defined analogously. For example, for $f, g: \\mathbb{Z} \\to \\R$, (f \\ast g)(j) = \\sum_{i \\in \\mathbb{Z}} f(i)g(j-i) Kernel density estimation and regression are convolutions. For example, the kernel density estimator can be written as \\begin{align*} \\hat{f}(x) = & \\frac{1}{nh} \\sum_{i=1}^n k((x-x_i)/h) \\\\ = & \\int \\left(\\frac{1}{n} \\sum_{i=1}^n \\delta_{x_i}(t) \\right)k((x-t)/h)/h dt \\end{align*} where $\\delta_{x_i}(t)$ is the Dirac measure at $x_i$. Convolutions appear in image processing as blurring and smoothing filters. Taking a local average of pixels is a convolution. Image source The similarity to kernel regression should be obvious. The 3x3 matrix 1 0 1 0 1 0 1 0 1 in the animation above is called a stencil or a kernel. In addition to blurring and smoothing, convolutions can detect patterns in images. For example 1 \u00d7 2 filters of [-1 1] and [1, -1] will pick out vertical transitions from dark to light and vice versa. Here\u2019s an illustration of how this looks using an image from the MNIST data. using MLDatasets using ImageFiltering using Plots Plots.pyplot() train_x, labels = MNIST(split=:train)[:] imgs = [Gray.(train_x[:,:,i]') for i \u2208 1:size(train_x,3)] i = 1 plot(plot(RGB.(1 .- imgs[i]), title=\"Original\", aspect_ratio=:equal, axis=false, ticks=false), plot(RGB.(1 .- imgs[i] .+ imfilter(imgs[i], centered([-1 1])), 1 .- imgs[i], 1 .- imgs[i] .+ imfilter(imgs[i], centered([1 -1]))), title=\"Vertical edge filters\", aspect_ratio=:equal, axis=false, ticks=false), layout=(1, 2) ) In this image, the output of one edge filter is yellow, the other is blue, and the original image is black. From this example, we see that small hand-crafted convolution kernels can pick out patterns, like edges. One idea would be to use the output of these kernels as features in a machine learning model. Another idea is to treat the weigths in a convolution matrix as part of the model to be estimated. This is exactly what convolutional neural networks do. Example : MNIST \u00b6 Let\u2019s see how convolutional neural networks can be applied to the MNIST data. The code in this section was adapted from the Flux model zoo. using Flux, MLDatasets, Statistics using Flux: onehotbatch, onecold, throttle, @epochs using Base.Iterators: repeated, partition using JLD2 using ColorSchemes, ProgressMeter cscheme = colorschemes[:BrBG_4]; When we were fitting feed-forward networks with this data, we simply treated images as vetors of length $28^2$. That was appropriate there because the model we were using did not explicitly utilize spacial information. With convolutions, we need to preserve spatial information, so we need to treat the images as $28 \\times 28$ arrays. # set up training and testing data createx(imgdata) = reduce((x,y)->cat(x,y, dims=4), reshape.(float.(imgdata),28,28, 1) ) X = Float32.(reshape(train_x, 28, 28, 1, size(train_x,3))) |> gpu Y = onehotbatch(labels, 0:9) |> gpu test_x, test_y = MNIST(split=:test)[:] tX = Float32.(reshape(test_x, 28, 28, 1, size(test_x,3))) |> gpu tY = onehotbatch(test_y, 0:9) |> gpu; When working with convolutions, Flux.jl wants the data to be stored as a $K_1 \\times K_2 \\times C \\times N$ array where $K_1 \\times K_2$ is the dimension of the images (or whatever else the data represents), $C$ is the number of channels. Since we have grayscale images, we have one channel. Color images would have three channels. Radar and satellite imagery can have more channels (satellites often collect non-visible frequencies of light). Channels are also a useful abstraction throughout the neural network. We usually want to apply multiple convolution filters to extract different features of the images. The output of each convolution is stored in a channel. Each convolution is intended to pick up some local pattern in the image. It might be useful to modify certain pattens by applying an activation function to the output of the convolution. Finally, convolutions detect patterns, but for many image classification tasks, we do not necessarily care exactly where a pattern occurs. A cat remains a a cat wherever it is located in a picture. Motivated by this, a \u201cpooling\u201d operation is often applied to the output of convolutions. These are similar to convolutions with fixed weights. Common pooling operations include taking the average within a rectangle of fixed size and taking the maximum. Pooling (and convolution) can reduce dimension by only looking at non-overlapping (or partially non-overlapping) regions. In Flux.jl the default for convolutions is to look at every pixel, and the default for pooling is to look at non-overlapping regions. This behavior can be changed by changing the stride option. Mathematically, we can express a convolutional layer as follows. Let $x$ be an $N \\times N$ input channel, $\\psi$ be an activation function, $b \\in \\R$ is a bias, and $w$ be $M \\times M$ convolution weights, indexed from -M/2 to M/2. The result of applying the convolution is: \\psi((x \\ast w)[k,\\ell]) = \\psi\\left(\\sum_{i=1-P}^{N+P} \\sum_{j=1-P}^{N+P} x[i,j]*w[k-i, j - \\ell] + b\\right) where if an index is out-of-bounds simply set that term in the sum to $0$. $P$ is the \u201cpadding\u201d. If $P=M/2$, then the size of $x\\ast w$ is the same as $x$. If $P<M/2$, the convolution decreases the size of the image. Then if we apply a maximum pooling function with dimension $D \\times D$, we get a $N/D \\times N/D$ array with elements maxpool(\\psi((x \\ast w) )[n,m] = \\max_{1 \\leq i \\leq d, 1 \\leq j \\leq d} \\{\\psi((x \\ast w)[dk+i,d\\ell+j])\\} The following code defines a convolutional network with three convolutional layers followed by a dense feed-forward layer for the output. model = Chain( # First convolution layer, operating upon a 28x28 image Conv((3, 3), 1=>16, pad=(1,1), relu), MaxPool((2,2)), # Second convolution, operating upon a 14x14 image Conv((3, 3), 16=>32, pad=(1,1), relu), MaxPool((2,2)), # Third convolution, operating upon a 7x7 image Conv((3, 3), 32=>32, pad=(1,1), relu), MaxPool((2,2)), # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N) # which is where we get the 288 in the `Dense` layer below: x -> reshape(x, :, size(x, 4)), Dense(288, 10) #, # Finally, softmax to get nice probabilities #softmax ) |> gpu # count parameters nparam(m::Chain)=sum(nparam.(m)) nparam(m::Conv)=length(m.weight) + length(m.bias) nparam(m::Dense)=length(m.weight)+length(m.bias) nparam(m) = 0 println(\"There $(nparam(model)) parameters\") There 16938 parameters Now, let\u2019s train the model. We are going to train the model a few times with slightly different details, so it makes sense to define a function for the training loop. function accuracy(m, x, y) # onecold(m(x)) results in very slow code for large x, so we avoid it coldx = vec(map(x->x[1], argmax(m(x), dims=1))) coldy = onecold(y) return(mean(coldx.==coldy)) end function train_mnist!(model, X, Y, tX, tY, modelname; loss = (x,y)->Flux.logitcrossentropy(model(x), y), accuracy = (x,y)->accuracy(model, x, y), batchsize=length(tY), reps_per_epoch=1, maxepochs=200, rerun=false ) Xsmall = X[:,:,:,1:1000] Ysmall = Y[:,1:1000] evalcb = () -> @show(loss(Xsmall, Ysmall), loss(tX,tY), accuracy(Xsmall,Ysmall), accuracy(tX,tY)) parts=Base.Iterators.partition(1:size(X,ndims(X)), batchsize); batches = [(X[:,:,:,p], Y[:,p]) for p in parts]; data = repeat(batches, reps_per_epoch); # The model and entire training data do not fit in my GPU # memory. For monitoring progress we will occassionally print the # loss and accuracy summed over the entire data. function sumloss(batches) L = zero(Float32) for i in 1:length(batches) L += loss(batches[i]...) end L /= length(batches) end function acc(batches) L = zero(Float32) for i in 1:length(batches) L += accuracy(batches[i]...) end L /= length(batches) end opt = ADAM(0.001) acctest = zeros(maxepochs) acctrain = zeros(maxepochs) losstest = zeros(maxepochs) losstrain = zeros(maxepochs) @info(\"Beginning training loop...\") best_acc = 0.0 last_improvement = 0 e = 0 docdir = joinpath(dirname(Base.pathof(NeuralNetworkEconomics)), \"..\",\"docs\") progress = Progress(maxepochs, 1, \"Training model\", 50) while e<maxepochs e += 1 modelfile = joinpath(docdir,\"jmd\",\"models\",\"$modelname-$e-epochs.jld2\") if rerun || !isfile(modelfile) @time Flux.train!(loss, Flux.params(model), data, opt, cb = throttle(evalcb, 20)) # save model cpum = cpu(model) losstrain[e]= sumloss(batches) acctrain[e] = acc(batches) losstest[e]=loss(tX,tY) acctest[e] =accuracy(tX,tY) @save modelfile cpum losstrain acctrain losstest acctest else @load modelfile cpum losstrain acctrain losstest acctest model = gpu(cpum) end next!(progress) if (acctest[e]>best_acc) best_acc = acctest[e] last_improvement=e end # If we haven't seen improvement in 3 epochs, we stop (could also # try droping learning rat but it would take time) we are cheating # here by using thest test accuracy as a stopping criteria ... if ((e - last_improvement) >= 3) && (opt.eta <= 1e-6) @warn(\" -> At epoch $e, haven't improved in 3 epochs. Stopping training.\") break end end return(model=model, losstrain=losstrain[1:e], acctrain=acctrain[1:e], losstest=losstest[1:e], acctest=acctest[1:e]) end train_mnist! (generic function with 1 method) Now we train the model. We will begin by following a similar training strategy as in the previous notes. That is, we will use large batches and a low number of passes through the data per-epoch. out = train_mnist!(model, X, Y, tX, tY, \"conv\"; batchsize=2500, reps_per_epoch=2, maxepochs=200, rerun=false ) @show maximum(out.acctest) maximum(out.acctest) = 0.99 0.99 Since I save the model to disk to avoid waiting for it to rerun everytime I change this document, the above output does not include the training time. It takes roughly 10 seconds per epoch. In terms of testing accuracy, this model does quite well. The deep feedforward network with nearly 12 million parameters from the previous notes , had an accuracy greater than 98%. The convolutional network used here with 16 thousand parameters has a accuracy of 0.99%. The training time of these two models was roughly the same. function training_plot(out) ll = Int(round(length(out.losstrain)*0.75)) lt = Int(round(length(out.losstrain)*0.5)) plot( plot([out.losstrain, out.losstest], xscale=:log10, xlab=\"Epochs\", title=\"Cross-Entropy Loss\", ylims=(0.0, 0.125), annotations=[(ll, out.losstrain[ll], Plots.text(\"training\", pointsize=12, valign=:bottom, color=get(cscheme,1))), (lt, out.losstest[lt], Plots.text(\"test\", pointsize=12, valign=:bottom, color=get(cscheme,0)))], leg=false, color_palette=get(cscheme,[1,0]) ), plot([out.acctrain, out.acctest], xscale=:log10, xlab=\"Epochs\", title=\"Accuracy\", ylims=(0.95, 1.0), color_palette=get(cscheme,[1,0]), leg=false ), layout=(2,1) ) end training_plot(out) Small Batches \u00b6 If you look at the code in the Flux model zoo for this model, it claims to achieve over 99% testing accuracy. The code above is mostly identical to the model zoo, but it differs in two ways. One is that the model zoo uses much smaller batches of 128 observations. The second is that the model zoo adds some gaussian noise to the images during training. We will look at how each of these changes affect the results. First, let\u2019s just reduce the batch size to 128. model = Chain( # same as before, but resetting initial values Conv((3, 3), 1=>16, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 16=>32, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 32=>32, pad=(1,1), relu), MaxPool((2,2)), x -> reshape(x, :, size(x, 4)), Dense(288, 10) ) |> gpu smallbatch = train_mnist!(model, X, Y, tX, tY, \"conv-smallbatch\"; loss=(x,y)->Flux.logitcrossentropy(model(x),y), batchsize=128, reps_per_epoch=1, maxepochs=200, rerun=false ) @show maximum(smallbatch.acctest) training_plot(smallbatch) maximum(smallbatch.acctest) = 0.9926 Smaller batches have improved the accuracy from 98.83% to just over 99%. Note that although the number of epochs are roughly the same as above, the number of gradient descent iterations is much higher. The total run-time is roughly the same. Data Augmentation \u00b6 The example in the model zoo also augments the training data by adding a small amount of Gaussian noise to the training images. Data augmentation is used by the best models among the MNIST benchmarks on LeCun\u2019s website . For example @ciresan2010 randomly distorts images with small rotations and stretching. Adding Gaussian noise is not as well geometrically motivated, but it has the advantage of being very easy to implement. We will add a $\\epsilon \\sim N(0, 0.1)$ to each pixel in the images. Here is what this looks like for a few images plot([plot(RGB.(1 .- 0.1*randn(size(imgs[i]))) .- round.(imgs[i])) for i in 1:9]..., xlab=\"\", ylab=\"\", aspect_ratio=:equal, axis=false, ticks=false) With this added noise, the digits are still easily recognizable, so we would hope that our model can classify them. addnoise(x, \u03c3) = x .+ \u03c3.*gpu(randn(eltype(x), size(x))) @show accuracy(out.model, addnoise(tX,0.1f0), tY) @show accuracy(smallbatch.model, addnoise(tX,0.1f0), tY) Error: DimensionMismatch: A has dimensions (10,288) but B has dimensions (3 ,960000) The models trained above do a pretty good job of classifying noisy images, but not quite as well as the original. What if we train the models with noise? Here, we will train with the original large batches and added noise. model = Chain( # same as before, but resetting initial values Conv((3, 3), 1=>16, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 16=>32, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 32=>32, pad=(1,1), relu), MaxPool((2,2)), x -> reshape(x, :, size(x, 4)), Dense(288, 10) ) |> gpu outn = train_mnist!(model, X, Y, tX, tY, \"conv-augrandn\"; loss=(x,y)->Flux.logitcrossentropy(model(x .+ 0.1f0*gpu(randn(eltype(x), size(x)))),y), batchsize=2500, reps_per_epoch=2, maxepochs=200, rerun=false ) @show maximum(outn.acctest) maximum(outn.acctest) = 0.9922 0.9922 training_plot(outn) With large batches, adding noise has improved the accuracy very little Small Batches and Data Augmentation \u00b6 Let\u2019s try combining small batches and data augmentation. model = Chain( # same as before, but resetting initial values Conv((3, 3), 1=>16, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 16=>32, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 32=>32, pad=(1,1), relu), MaxPool((2,2)), x -> reshape(x, :, size(x, 4)), Dense(288, 10) ) |> gpu smallnoisy = train_mnist!(model, X, Y, tX, tY, \"conv-augrandn-smallbatch\"; loss=(x,y)->Flux.logitcrossentropy(model(x .+ 0.1f0*gpu(randn(eltype(x), size(x)))),y), batchsize=128, reps_per_epoch=1, maxepochs=200, rerun=false ) @show maximum(smallnoisy.acctest) training_plot(smallnoisy) maximum(smallnoisy.acctest) = 0.9934 This gives the highest test accuracy we have achieved so far. Let\u2019s look at some of the missclassified digits. tlabels = test_y timgs = [Gray.(test_x[:,:,i]') for i \u2208 1:size(test_x,3)] # predicted labels mlabels = onecold(cpu(smallnoisy.model(tX))).-1 @show mean(mlabels.==tlabels) # = accuracy @show sum(mlabels .!= tlabels) miss=findall(mlabels .!= tlabels) plot( [plot(RGB.(1 .- timgs[i]), title=\"$(tlabels[i]) as $(mlabels[i])\", axis=false, ticks=false, aspect_ratio=:equal) for i in miss[1:16]]...) mean(mlabels .== tlabels) = 0.9916 sum(mlabels .!= tlabels) = 84 Looking inside the Black Box \u00b6 Our fitted model is somewhat of a black box, but when we are working with images and convolution, we can somewhat look inside it. We can display the images that are generated by each convolutional and/or max pool layer. Let\u2019s do this for one image. i = 1 m = smallnoisy.model figs = Array{typeof(plot()), 1}(undef, 3) j = 1 for l in 1:length(m) global j, figs if (typeof(m[l]) <: MaxPool) layer = m[1:l](reshape(X[:,:,:,i], 28,28,1,1)) figs[j] = plot( [plot(RGB.(cpu(1 .- layer[:,:,c,1])), aspect_ratio=:equal, axis=false, ticks=false) for c in 1:size(layer,3)]...) j += 1 end end plot(RGB.(cpu(1 .- X[:,:,1,i])), aspect_ratio=:equal, axis=false, ticks=false) The original is above. Below is are the output of the first convolution + max pool layer. figs[1] Now the second figs[2] And the last figs[3] These 32 three by three images then get stacked into a vector and passed into the final dense layer. I\u2019m not sure that there\u2019s much to learn from looking at these images. Maybe it\u2019s best to keep the box closed. References \u00b6","title":"Convolutional"},{"location":"conv/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min}","title":"About this document"},{"location":"conv/#introduction","text":"The previous notes discussed multiple layer feedforward networks, and applied them to image classification. However, state-of-art image classifiers typically do not use feedforward networks. They use convolutional networks, which will be the topic of this document. Convolutional neural networks also have applications to PDEs, see @rackauckas2019conv. They also have potential applications to time series and spatial data.","title":"Introduction"},{"location":"conv/#additional-reading","text":"@goodfellow2016 Deep Learning especially chapter 9 Knet.jl documentation especially the textbook @klok2019 Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence","title":"Additional Reading"},{"location":"conv/#convolutions","text":"A convolution is an operation on a set of functions. If $f:\\R \\to \\R$ and $g:\\R \\to \\R$, then their convolution is (f \\ast g)(x) = \\int_\\R f(t) g(x-t) dt The convolution is commutative, $(f\\ast g)(x) = (g\\ast f)(x)$. For functions with other domains, convolutions can be defined analogously. For example, for $f, g: \\mathbb{Z} \\to \\R$, (f \\ast g)(j) = \\sum_{i \\in \\mathbb{Z}} f(i)g(j-i) Kernel density estimation and regression are convolutions. For example, the kernel density estimator can be written as \\begin{align*} \\hat{f}(x) = & \\frac{1}{nh} \\sum_{i=1}^n k((x-x_i)/h) \\\\ = & \\int \\left(\\frac{1}{n} \\sum_{i=1}^n \\delta_{x_i}(t) \\right)k((x-t)/h)/h dt \\end{align*} where $\\delta_{x_i}(t)$ is the Dirac measure at $x_i$. Convolutions appear in image processing as blurring and smoothing filters. Taking a local average of pixels is a convolution. Image source The similarity to kernel regression should be obvious. The 3x3 matrix 1 0 1 0 1 0 1 0 1 in the animation above is called a stencil or a kernel. In addition to blurring and smoothing, convolutions can detect patterns in images. For example 1 \u00d7 2 filters of [-1 1] and [1, -1] will pick out vertical transitions from dark to light and vice versa. Here\u2019s an illustration of how this looks using an image from the MNIST data. using MLDatasets using ImageFiltering using Plots Plots.pyplot() train_x, labels = MNIST(split=:train)[:] imgs = [Gray.(train_x[:,:,i]') for i \u2208 1:size(train_x,3)] i = 1 plot(plot(RGB.(1 .- imgs[i]), title=\"Original\", aspect_ratio=:equal, axis=false, ticks=false), plot(RGB.(1 .- imgs[i] .+ imfilter(imgs[i], centered([-1 1])), 1 .- imgs[i], 1 .- imgs[i] .+ imfilter(imgs[i], centered([1 -1]))), title=\"Vertical edge filters\", aspect_ratio=:equal, axis=false, ticks=false), layout=(1, 2) ) In this image, the output of one edge filter is yellow, the other is blue, and the original image is black. From this example, we see that small hand-crafted convolution kernels can pick out patterns, like edges. One idea would be to use the output of these kernels as features in a machine learning model. Another idea is to treat the weigths in a convolution matrix as part of the model to be estimated. This is exactly what convolutional neural networks do.","title":"Convolutions"},{"location":"conv/#example-mnist","text":"Let\u2019s see how convolutional neural networks can be applied to the MNIST data. The code in this section was adapted from the Flux model zoo. using Flux, MLDatasets, Statistics using Flux: onehotbatch, onecold, throttle, @epochs using Base.Iterators: repeated, partition using JLD2 using ColorSchemes, ProgressMeter cscheme = colorschemes[:BrBG_4]; When we were fitting feed-forward networks with this data, we simply treated images as vetors of length $28^2$. That was appropriate there because the model we were using did not explicitly utilize spacial information. With convolutions, we need to preserve spatial information, so we need to treat the images as $28 \\times 28$ arrays. # set up training and testing data createx(imgdata) = reduce((x,y)->cat(x,y, dims=4), reshape.(float.(imgdata),28,28, 1) ) X = Float32.(reshape(train_x, 28, 28, 1, size(train_x,3))) |> gpu Y = onehotbatch(labels, 0:9) |> gpu test_x, test_y = MNIST(split=:test)[:] tX = Float32.(reshape(test_x, 28, 28, 1, size(test_x,3))) |> gpu tY = onehotbatch(test_y, 0:9) |> gpu; When working with convolutions, Flux.jl wants the data to be stored as a $K_1 \\times K_2 \\times C \\times N$ array where $K_1 \\times K_2$ is the dimension of the images (or whatever else the data represents), $C$ is the number of channels. Since we have grayscale images, we have one channel. Color images would have three channels. Radar and satellite imagery can have more channels (satellites often collect non-visible frequencies of light). Channels are also a useful abstraction throughout the neural network. We usually want to apply multiple convolution filters to extract different features of the images. The output of each convolution is stored in a channel. Each convolution is intended to pick up some local pattern in the image. It might be useful to modify certain pattens by applying an activation function to the output of the convolution. Finally, convolutions detect patterns, but for many image classification tasks, we do not necessarily care exactly where a pattern occurs. A cat remains a a cat wherever it is located in a picture. Motivated by this, a \u201cpooling\u201d operation is often applied to the output of convolutions. These are similar to convolutions with fixed weights. Common pooling operations include taking the average within a rectangle of fixed size and taking the maximum. Pooling (and convolution) can reduce dimension by only looking at non-overlapping (or partially non-overlapping) regions. In Flux.jl the default for convolutions is to look at every pixel, and the default for pooling is to look at non-overlapping regions. This behavior can be changed by changing the stride option. Mathematically, we can express a convolutional layer as follows. Let $x$ be an $N \\times N$ input channel, $\\psi$ be an activation function, $b \\in \\R$ is a bias, and $w$ be $M \\times M$ convolution weights, indexed from -M/2 to M/2. The result of applying the convolution is: \\psi((x \\ast w)[k,\\ell]) = \\psi\\left(\\sum_{i=1-P}^{N+P} \\sum_{j=1-P}^{N+P} x[i,j]*w[k-i, j - \\ell] + b\\right) where if an index is out-of-bounds simply set that term in the sum to $0$. $P$ is the \u201cpadding\u201d. If $P=M/2$, then the size of $x\\ast w$ is the same as $x$. If $P<M/2$, the convolution decreases the size of the image. Then if we apply a maximum pooling function with dimension $D \\times D$, we get a $N/D \\times N/D$ array with elements maxpool(\\psi((x \\ast w) )[n,m] = \\max_{1 \\leq i \\leq d, 1 \\leq j \\leq d} \\{\\psi((x \\ast w)[dk+i,d\\ell+j])\\} The following code defines a convolutional network with three convolutional layers followed by a dense feed-forward layer for the output. model = Chain( # First convolution layer, operating upon a 28x28 image Conv((3, 3), 1=>16, pad=(1,1), relu), MaxPool((2,2)), # Second convolution, operating upon a 14x14 image Conv((3, 3), 16=>32, pad=(1,1), relu), MaxPool((2,2)), # Third convolution, operating upon a 7x7 image Conv((3, 3), 32=>32, pad=(1,1), relu), MaxPool((2,2)), # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N) # which is where we get the 288 in the `Dense` layer below: x -> reshape(x, :, size(x, 4)), Dense(288, 10) #, # Finally, softmax to get nice probabilities #softmax ) |> gpu # count parameters nparam(m::Chain)=sum(nparam.(m)) nparam(m::Conv)=length(m.weight) + length(m.bias) nparam(m::Dense)=length(m.weight)+length(m.bias) nparam(m) = 0 println(\"There $(nparam(model)) parameters\") There 16938 parameters Now, let\u2019s train the model. We are going to train the model a few times with slightly different details, so it makes sense to define a function for the training loop. function accuracy(m, x, y) # onecold(m(x)) results in very slow code for large x, so we avoid it coldx = vec(map(x->x[1], argmax(m(x), dims=1))) coldy = onecold(y) return(mean(coldx.==coldy)) end function train_mnist!(model, X, Y, tX, tY, modelname; loss = (x,y)->Flux.logitcrossentropy(model(x), y), accuracy = (x,y)->accuracy(model, x, y), batchsize=length(tY), reps_per_epoch=1, maxepochs=200, rerun=false ) Xsmall = X[:,:,:,1:1000] Ysmall = Y[:,1:1000] evalcb = () -> @show(loss(Xsmall, Ysmall), loss(tX,tY), accuracy(Xsmall,Ysmall), accuracy(tX,tY)) parts=Base.Iterators.partition(1:size(X,ndims(X)), batchsize); batches = [(X[:,:,:,p], Y[:,p]) for p in parts]; data = repeat(batches, reps_per_epoch); # The model and entire training data do not fit in my GPU # memory. For monitoring progress we will occassionally print the # loss and accuracy summed over the entire data. function sumloss(batches) L = zero(Float32) for i in 1:length(batches) L += loss(batches[i]...) end L /= length(batches) end function acc(batches) L = zero(Float32) for i in 1:length(batches) L += accuracy(batches[i]...) end L /= length(batches) end opt = ADAM(0.001) acctest = zeros(maxepochs) acctrain = zeros(maxepochs) losstest = zeros(maxepochs) losstrain = zeros(maxepochs) @info(\"Beginning training loop...\") best_acc = 0.0 last_improvement = 0 e = 0 docdir = joinpath(dirname(Base.pathof(NeuralNetworkEconomics)), \"..\",\"docs\") progress = Progress(maxepochs, 1, \"Training model\", 50) while e<maxepochs e += 1 modelfile = joinpath(docdir,\"jmd\",\"models\",\"$modelname-$e-epochs.jld2\") if rerun || !isfile(modelfile) @time Flux.train!(loss, Flux.params(model), data, opt, cb = throttle(evalcb, 20)) # save model cpum = cpu(model) losstrain[e]= sumloss(batches) acctrain[e] = acc(batches) losstest[e]=loss(tX,tY) acctest[e] =accuracy(tX,tY) @save modelfile cpum losstrain acctrain losstest acctest else @load modelfile cpum losstrain acctrain losstest acctest model = gpu(cpum) end next!(progress) if (acctest[e]>best_acc) best_acc = acctest[e] last_improvement=e end # If we haven't seen improvement in 3 epochs, we stop (could also # try droping learning rat but it would take time) we are cheating # here by using thest test accuracy as a stopping criteria ... if ((e - last_improvement) >= 3) && (opt.eta <= 1e-6) @warn(\" -> At epoch $e, haven't improved in 3 epochs. Stopping training.\") break end end return(model=model, losstrain=losstrain[1:e], acctrain=acctrain[1:e], losstest=losstest[1:e], acctest=acctest[1:e]) end train_mnist! (generic function with 1 method) Now we train the model. We will begin by following a similar training strategy as in the previous notes. That is, we will use large batches and a low number of passes through the data per-epoch. out = train_mnist!(model, X, Y, tX, tY, \"conv\"; batchsize=2500, reps_per_epoch=2, maxepochs=200, rerun=false ) @show maximum(out.acctest) maximum(out.acctest) = 0.99 0.99 Since I save the model to disk to avoid waiting for it to rerun everytime I change this document, the above output does not include the training time. It takes roughly 10 seconds per epoch. In terms of testing accuracy, this model does quite well. The deep feedforward network with nearly 12 million parameters from the previous notes , had an accuracy greater than 98%. The convolutional network used here with 16 thousand parameters has a accuracy of 0.99%. The training time of these two models was roughly the same. function training_plot(out) ll = Int(round(length(out.losstrain)*0.75)) lt = Int(round(length(out.losstrain)*0.5)) plot( plot([out.losstrain, out.losstest], xscale=:log10, xlab=\"Epochs\", title=\"Cross-Entropy Loss\", ylims=(0.0, 0.125), annotations=[(ll, out.losstrain[ll], Plots.text(\"training\", pointsize=12, valign=:bottom, color=get(cscheme,1))), (lt, out.losstest[lt], Plots.text(\"test\", pointsize=12, valign=:bottom, color=get(cscheme,0)))], leg=false, color_palette=get(cscheme,[1,0]) ), plot([out.acctrain, out.acctest], xscale=:log10, xlab=\"Epochs\", title=\"Accuracy\", ylims=(0.95, 1.0), color_palette=get(cscheme,[1,0]), leg=false ), layout=(2,1) ) end training_plot(out)","title":"Example : MNIST"},{"location":"conv/#small-batches","text":"If you look at the code in the Flux model zoo for this model, it claims to achieve over 99% testing accuracy. The code above is mostly identical to the model zoo, but it differs in two ways. One is that the model zoo uses much smaller batches of 128 observations. The second is that the model zoo adds some gaussian noise to the images during training. We will look at how each of these changes affect the results. First, let\u2019s just reduce the batch size to 128. model = Chain( # same as before, but resetting initial values Conv((3, 3), 1=>16, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 16=>32, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 32=>32, pad=(1,1), relu), MaxPool((2,2)), x -> reshape(x, :, size(x, 4)), Dense(288, 10) ) |> gpu smallbatch = train_mnist!(model, X, Y, tX, tY, \"conv-smallbatch\"; loss=(x,y)->Flux.logitcrossentropy(model(x),y), batchsize=128, reps_per_epoch=1, maxepochs=200, rerun=false ) @show maximum(smallbatch.acctest) training_plot(smallbatch) maximum(smallbatch.acctest) = 0.9926 Smaller batches have improved the accuracy from 98.83% to just over 99%. Note that although the number of epochs are roughly the same as above, the number of gradient descent iterations is much higher. The total run-time is roughly the same.","title":"Small Batches"},{"location":"conv/#data-augmentation","text":"The example in the model zoo also augments the training data by adding a small amount of Gaussian noise to the training images. Data augmentation is used by the best models among the MNIST benchmarks on LeCun\u2019s website . For example @ciresan2010 randomly distorts images with small rotations and stretching. Adding Gaussian noise is not as well geometrically motivated, but it has the advantage of being very easy to implement. We will add a $\\epsilon \\sim N(0, 0.1)$ to each pixel in the images. Here is what this looks like for a few images plot([plot(RGB.(1 .- 0.1*randn(size(imgs[i]))) .- round.(imgs[i])) for i in 1:9]..., xlab=\"\", ylab=\"\", aspect_ratio=:equal, axis=false, ticks=false) With this added noise, the digits are still easily recognizable, so we would hope that our model can classify them. addnoise(x, \u03c3) = x .+ \u03c3.*gpu(randn(eltype(x), size(x))) @show accuracy(out.model, addnoise(tX,0.1f0), tY) @show accuracy(smallbatch.model, addnoise(tX,0.1f0), tY) Error: DimensionMismatch: A has dimensions (10,288) but B has dimensions (3 ,960000) The models trained above do a pretty good job of classifying noisy images, but not quite as well as the original. What if we train the models with noise? Here, we will train with the original large batches and added noise. model = Chain( # same as before, but resetting initial values Conv((3, 3), 1=>16, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 16=>32, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 32=>32, pad=(1,1), relu), MaxPool((2,2)), x -> reshape(x, :, size(x, 4)), Dense(288, 10) ) |> gpu outn = train_mnist!(model, X, Y, tX, tY, \"conv-augrandn\"; loss=(x,y)->Flux.logitcrossentropy(model(x .+ 0.1f0*gpu(randn(eltype(x), size(x)))),y), batchsize=2500, reps_per_epoch=2, maxepochs=200, rerun=false ) @show maximum(outn.acctest) maximum(outn.acctest) = 0.9922 0.9922 training_plot(outn) With large batches, adding noise has improved the accuracy very little","title":"Data Augmentation"},{"location":"conv/#small-batches-and-data-augmentation","text":"Let\u2019s try combining small batches and data augmentation. model = Chain( # same as before, but resetting initial values Conv((3, 3), 1=>16, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 16=>32, pad=(1,1), relu), MaxPool((2,2)), Conv((3, 3), 32=>32, pad=(1,1), relu), MaxPool((2,2)), x -> reshape(x, :, size(x, 4)), Dense(288, 10) ) |> gpu smallnoisy = train_mnist!(model, X, Y, tX, tY, \"conv-augrandn-smallbatch\"; loss=(x,y)->Flux.logitcrossentropy(model(x .+ 0.1f0*gpu(randn(eltype(x), size(x)))),y), batchsize=128, reps_per_epoch=1, maxepochs=200, rerun=false ) @show maximum(smallnoisy.acctest) training_plot(smallnoisy) maximum(smallnoisy.acctest) = 0.9934 This gives the highest test accuracy we have achieved so far. Let\u2019s look at some of the missclassified digits. tlabels = test_y timgs = [Gray.(test_x[:,:,i]') for i \u2208 1:size(test_x,3)] # predicted labels mlabels = onecold(cpu(smallnoisy.model(tX))).-1 @show mean(mlabels.==tlabels) # = accuracy @show sum(mlabels .!= tlabels) miss=findall(mlabels .!= tlabels) plot( [plot(RGB.(1 .- timgs[i]), title=\"$(tlabels[i]) as $(mlabels[i])\", axis=false, ticks=false, aspect_ratio=:equal) for i in miss[1:16]]...) mean(mlabels .== tlabels) = 0.9916 sum(mlabels .!= tlabels) = 84","title":"Small Batches and Data Augmentation"},{"location":"conv/#looking-inside-the-black-box","text":"Our fitted model is somewhat of a black box, but when we are working with images and convolution, we can somewhat look inside it. We can display the images that are generated by each convolutional and/or max pool layer. Let\u2019s do this for one image. i = 1 m = smallnoisy.model figs = Array{typeof(plot()), 1}(undef, 3) j = 1 for l in 1:length(m) global j, figs if (typeof(m[l]) <: MaxPool) layer = m[1:l](reshape(X[:,:,:,i], 28,28,1,1)) figs[j] = plot( [plot(RGB.(cpu(1 .- layer[:,:,c,1])), aspect_ratio=:equal, axis=false, ticks=false) for c in 1:size(layer,3)]...) j += 1 end end plot(RGB.(cpu(1 .- X[:,:,1,i])), aspect_ratio=:equal, axis=false, ticks=false) The original is above. Below is are the output of the first convolution + max pool layer. figs[1] Now the second figs[2] And the last figs[3] These 32 three by three images then get stacked into a vector and passed into the final dense layer. I\u2019m not sure that there\u2019s much to learn from looking at these images. Maybe it\u2019s best to keep the box closed.","title":"Looking inside the Black Box"},{"location":"conv/#references","text":"","title":"References"},{"location":"fun_with_text/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} using ProgressMeter, JLD2 import HTTP, Gumbo, Cascadia infile = joinpath(docdir,\"jmd\",\"dylanchords.txt\") if !isfile(infile) @error \"$infile not found. See rnn.jmd for code to create it.\" end text = String(read(infile)); songs = split(text, \"</html>\"); songs = songs[1:(end-1)]; # last one is empty lyrics=Array{String,1}(undef,length(songs)); titles=Array{String,1}(undef,length(songs)); # function to extract text from HTMLNodes gettext(h::Gumbo.HTMLText) = Gumbo.text(h) gettext(h::AbstractArray) = length(h)==0 ? \"\" : prod(gettext, h) gettext(h::Gumbo.HTMLNode) = gettext(Gumbo.children(h)) # remove chords from verses function removechords(txt) chordregexp=r\"(^)( {0,1000}|\\()(A|B|C|D|E|G|F)(\\S{0,6})(\\)| {2,1000}| \\.|$).*\"m txt2=replace(txt, chordregexp => \"\\n\") replace(txt2, r\"( {2,1000})\"=>\" \") end for (i,song) = enumerate(songs) html=Gumbo.parsehtml(song); t = eachmatch(Cascadia.Selector(\".songtitle\"), html.root) (length(t)==1) || @warn \"multiple songtitles for songs[$i]\" titles[i] = gettext(t) t = eachmatch(Cascadia.Selector(\".verse,.refrain\"), html.root) lyrics[i] = removechords(gettext(t)) end lyrics = lyrics[length.(lyrics).>0]; using Flux using Flux: onehot, chunk, batchseq, throttle, crossentropy using StatsBase: wsample using Base.Iterators: partition using ProgressMeter endchar = '\u03a9' # any character not in original text alphabet = [unique(collect(prod(lyrics)))..., endchar] stop = onehot(endchar, alphabet); # maxlen = 100 # we will split songs after this many characters # #batchsize=100 # we use this many \"songs\" (after splitting) per gradient descent batch # nextchars = ((x)->[x[2:end]..., endchar]).(lyrics); # function makex(lyrics, maxlen) # hotlyrics=map.(c->onehot(c, alphabet), collect.(lyrics)); # cs = Int.(ceil.(length.(hotlyrics)./maxlen)) # foo=chunk.(hotlyrics, cs); # X=[gpu.(batchseq(f, stop)) for f in foo]; # end # Xb = (makex(lyrics, maxlen)); # Yb = (makex(nextchars, maxlen)); # #addaf(daf) # # [gpu.(batchseq(map.(c->onehot(c, alphabet), nextchars[p]), stop)) # # for p in partition(1:length(splitlyrics), batchsize)]; # #Xb = [[x[:,p] for x in X] for p in partition(1:size(X[1],2), batchsize)]; # #Yb = [[y[:,p] for y in Y] for p in partition(1:size(Y[1],2), batchsize)]; # data = collect(zip(Xb,Yb)); # println(\"There are $(length(data)) batches per epoch\") if true text = collect(prod(lyrics)); hottext = map(ch -> onehot(ch, alphabet), text); maxlen = 100 batchsize = 1000 Xseq = collect(partition(gpu.(batchseq(chunk(hottext, batchsize),stop)), maxlen)); Yseq = collect(partition(gpu.(batchseq(chunk(hottext[2:end], batchsize),stop)), maxlen)); data = collect(zip(Xseq, Yseq)); end N = length(alphabet) function sample(m, alphabet, len) m = cpu(m) Flux.reset!(m) buf = IOBuffer() c = '\\n' for i = 1:len write(buf, c) c = wsample(alphabet, m(onehot(c, alphabet)).data) end return String(take!(buf)) end opt = RMSProp(0.005) # this will take awhile, so a fancier call back with a progress meter is nice to have function cbgenerator(N, loss, printiter=Int(round(N/10))) p=Progress(N, 1, \"Training\", 25) i=0 function cb() next!(p) if (i % printiter==0) @show loss() end i+=1 end return(cb) end N = length(alphabet) testsong = 1 function train_model!(m; N=N, data=data, modelfile=\"tmp.jld2\", opt=RMSProp(0.01), testset=testsong, epochs=20) function loss(xb::V, yb::V) where V <:AbstractVector #Flux.reset!(m) l = sum(crossentropy.(m.(xb),yb))/length(xb) #l = crossentropy(m(xb),yb) Flux.truncate!(m) return(l) end #function loss(xb, yb) # Flux.reset!(m) # after each song, forget gradients # l = crossentropy(m(xb),yb) # return(l) #end cb=cbgenerator(length(data)*(epochs+1), ()->loss(data[testset]...)) if isfile(modelfile) @load modelfile cpum m = gpu(cpum) end @time Flux.train!(loss, Flux.params(m), data, opt, cb = cb) println(\"Sampling after 1 epoch:\") sample(m, alphabet, 1000) |> println Flux.@epochs epochs Flux.train!(loss, Flux.params(m), data, opt, cb = cb) cpum = cpu(m) @save modelfile cpum return(m) end for L in [75] #, 32, 64, 128] #, 256, 512] #L = 100 file = joinpath(docdir,\"jmd\",\"dylanlyrics-$L.jld2\") m = Chain(LSTM(N, L), LSTM(L, L), Dense(L, N), softmax) |> gpu #m = Chain(GRU(N, L), GRU(L, L), Dense(L, N), softmax) |> gpu opt=RMSProp(0.01) m = train_model!(m, opt=opt, epochs=100, modelfile=file) println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"Model $L has $(sum([prod(size(p)) for p in Flux.params(m)])) parameters\") println(\"Sample from model $L\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(sample(m, alphabet, 5000)) println() end Error: UndefVarError: truncate! not defined References \u00b6","title":"Fun with text"},{"location":"fun_with_text/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} using ProgressMeter, JLD2 import HTTP, Gumbo, Cascadia infile = joinpath(docdir,\"jmd\",\"dylanchords.txt\") if !isfile(infile) @error \"$infile not found. See rnn.jmd for code to create it.\" end text = String(read(infile)); songs = split(text, \"</html>\"); songs = songs[1:(end-1)]; # last one is empty lyrics=Array{String,1}(undef,length(songs)); titles=Array{String,1}(undef,length(songs)); # function to extract text from HTMLNodes gettext(h::Gumbo.HTMLText) = Gumbo.text(h) gettext(h::AbstractArray) = length(h)==0 ? \"\" : prod(gettext, h) gettext(h::Gumbo.HTMLNode) = gettext(Gumbo.children(h)) # remove chords from verses function removechords(txt) chordregexp=r\"(^)( {0,1000}|\\()(A|B|C|D|E|G|F)(\\S{0,6})(\\)| {2,1000}| \\.|$).*\"m txt2=replace(txt, chordregexp => \"\\n\") replace(txt2, r\"( {2,1000})\"=>\" \") end for (i,song) = enumerate(songs) html=Gumbo.parsehtml(song); t = eachmatch(Cascadia.Selector(\".songtitle\"), html.root) (length(t)==1) || @warn \"multiple songtitles for songs[$i]\" titles[i] = gettext(t) t = eachmatch(Cascadia.Selector(\".verse,.refrain\"), html.root) lyrics[i] = removechords(gettext(t)) end lyrics = lyrics[length.(lyrics).>0]; using Flux using Flux: onehot, chunk, batchseq, throttle, crossentropy using StatsBase: wsample using Base.Iterators: partition using ProgressMeter endchar = '\u03a9' # any character not in original text alphabet = [unique(collect(prod(lyrics)))..., endchar] stop = onehot(endchar, alphabet); # maxlen = 100 # we will split songs after this many characters # #batchsize=100 # we use this many \"songs\" (after splitting) per gradient descent batch # nextchars = ((x)->[x[2:end]..., endchar]).(lyrics); # function makex(lyrics, maxlen) # hotlyrics=map.(c->onehot(c, alphabet), collect.(lyrics)); # cs = Int.(ceil.(length.(hotlyrics)./maxlen)) # foo=chunk.(hotlyrics, cs); # X=[gpu.(batchseq(f, stop)) for f in foo]; # end # Xb = (makex(lyrics, maxlen)); # Yb = (makex(nextchars, maxlen)); # #addaf(daf) # # [gpu.(batchseq(map.(c->onehot(c, alphabet), nextchars[p]), stop)) # # for p in partition(1:length(splitlyrics), batchsize)]; # #Xb = [[x[:,p] for x in X] for p in partition(1:size(X[1],2), batchsize)]; # #Yb = [[y[:,p] for y in Y] for p in partition(1:size(Y[1],2), batchsize)]; # data = collect(zip(Xb,Yb)); # println(\"There are $(length(data)) batches per epoch\") if true text = collect(prod(lyrics)); hottext = map(ch -> onehot(ch, alphabet), text); maxlen = 100 batchsize = 1000 Xseq = collect(partition(gpu.(batchseq(chunk(hottext, batchsize),stop)), maxlen)); Yseq = collect(partition(gpu.(batchseq(chunk(hottext[2:end], batchsize),stop)), maxlen)); data = collect(zip(Xseq, Yseq)); end N = length(alphabet) function sample(m, alphabet, len) m = cpu(m) Flux.reset!(m) buf = IOBuffer() c = '\\n' for i = 1:len write(buf, c) c = wsample(alphabet, m(onehot(c, alphabet)).data) end return String(take!(buf)) end opt = RMSProp(0.005) # this will take awhile, so a fancier call back with a progress meter is nice to have function cbgenerator(N, loss, printiter=Int(round(N/10))) p=Progress(N, 1, \"Training\", 25) i=0 function cb() next!(p) if (i % printiter==0) @show loss() end i+=1 end return(cb) end N = length(alphabet) testsong = 1 function train_model!(m; N=N, data=data, modelfile=\"tmp.jld2\", opt=RMSProp(0.01), testset=testsong, epochs=20) function loss(xb::V, yb::V) where V <:AbstractVector #Flux.reset!(m) l = sum(crossentropy.(m.(xb),yb))/length(xb) #l = crossentropy(m(xb),yb) Flux.truncate!(m) return(l) end #function loss(xb, yb) # Flux.reset!(m) # after each song, forget gradients # l = crossentropy(m(xb),yb) # return(l) #end cb=cbgenerator(length(data)*(epochs+1), ()->loss(data[testset]...)) if isfile(modelfile) @load modelfile cpum m = gpu(cpum) end @time Flux.train!(loss, Flux.params(m), data, opt, cb = cb) println(\"Sampling after 1 epoch:\") sample(m, alphabet, 1000) |> println Flux.@epochs epochs Flux.train!(loss, Flux.params(m), data, opt, cb = cb) cpum = cpu(m) @save modelfile cpum return(m) end for L in [75] #, 32, 64, 128] #, 256, 512] #L = 100 file = joinpath(docdir,\"jmd\",\"dylanlyrics-$L.jld2\") m = Chain(LSTM(N, L), LSTM(L, L), Dense(L, N), softmax) |> gpu #m = Chain(GRU(N, L), GRU(L, L), Dense(L, N), softmax) |> gpu opt=RMSProp(0.01) m = train_model!(m, opt=opt, epochs=100, modelfile=file) println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"Model $L has $(sum([prod(size(p)) for p in Flux.params(m)])) parameters\") println(\"Sample from model $L\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(sample(m, alphabet, 5000)) println() end Error: UndefVarError: truncate! not defined","title":"About this document"},{"location":"fun_with_text/#references","text":"","title":"References"},{"location":"license/","text":"The notes and examples are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf. BibTeX citation. The license for the package source code is here.","title":"License"},{"location":"ml-doubledebiased/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Using machine learning to estimate causal effects \u00b6 Double debiased machine learning \u00b6 @chernozhukov2018, @chernozhukov2017 Parameter of interest $\\theta \\in \\R^{d_\\theta}$ Nuisance parameter $\\eta \\in T$ Moment conditions \\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta} with $\\psi$ known Estimate $\\hat{\\eta}$ using some machine learning method Estimate $\\hat{\\theta}$ using cross-fitting Cross-fitting \u00b6 Randomly partition into $K$ subsets $(I_k)_{k=1}^K$ $I^c_k = {1, \u2026, n} \\setminus I_k$ $\\hat{\\eta}_k =$ estimate of $\\eta$ using $I^c_k$ Estimator: \\begin{align*} 0 = & \\frac{1}{K} \\sum_{k=1}^K \\frac{K}{n} \\sum_{i \\in I_k} \\psi(w_i;\\hat{\\theta},\\hat{\\eta}_k) \\\\ 0 = & \\frac{1}{K} \\sum_{k=1}^K \\En_k[ \\psi(w_i;\\hat{\\theta},\\hat{\\eta}_k)] \\end{align*} Assumptions \u00b6 Linear score \\psi(w;\\theta,\\eta) = \\psi^a(w;\\eta) \\theta + \\psi^b(w;\\eta) Near Neyman orthogonality: \\lambda_n := \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{\\partial \\eta \\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right] } \\leq \\delta_n n^{-1/2} Assumptions [assumptions-1] \u00b6 Rate conditions: for $\\delta_n \\to 0$ and $\\Delta_n \\to 0$, we have $\\Pr(\\hat{\\eta}_k \\in \\mathcal{T}_n) \\geq 1-\\Delta_n$ and \\begin{align*} r_n := & \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{ \\Er[\\psi^a(W;\\eta)] - \\Er[\\psi^a(W;\\eta_0)]} \\leq \\delta_n \\\\ r_n' := & \\sup_{\\eta \\in \\mathcal{T}_n} \\Er\\left[ \\norm{ \\psi(W;\\theta_0,\\eta) - \\psi(W;\\theta_0,\\eta_0)}^2 \\right]^{1/2} \\leq \\delta_n \\\\ \\lambda_n' := & \\sup_{r \\in (0,1), \\eta \\in \\mathcal{T}_n} \\norm{ \\partial_r^2 \\Er\\left[\\psi(W;\\theta_0, \\eta_0 + r(\\eta - \\eta_0)) \\right]} \\leq \\delta_n/\\sqrt{n} \\end{align*} Moments exist and other regularity conditions We focus on the case of linear scores to simplify proofs and all of our examples have scores linear in $\\theta$. @chernozhukov2018 cover nonlinear scores as well. These rate conditions might look a little strange. The rate conditions are stated this way because they\u2019re exactly what is needed for the result to work. $\\Delta_n$ and $\\delta_n$ are sequences converging to $0$. $\\mathcal{T}_n$ is a shrinking neighborhood of $\\eta_0$. A good exercise would be show that if $\\psi$ is a smooth function of $\\eta$ and $\\theta$, and $\\Er[(\\hat{\\eta}(x) - \\eta_0(x))^2]^{1/2} = O(\\epsilon_n) = o(n^{-1/4})$, then we can meet the above conditions with $r_n = r_n\u2019 = \\epsilon_n$ and $\\lambda_n\u2019 = \\epsilon_n^2$. Proof outline: \u00b6 Let \\begin{align*} \\hat{J} = & \\frac{1}{K} \\sum_{k=1}^K \\En_k [\\psi^a(w_i;\\hat{\\eta}_k)], \\\\ J_0 = & \\Er[\\psi^a(w_i;\\eta_0)], \\\\ R_{n,1} = & \\hat{J}-J_0 \\end{align*} Show: \\small \\begin{align*} \\sqrt{n}(\\hat{\\theta} - \\theta_0) = & -\\sqrt{n} J_0^{-1} \\En[\\psi(w_i;\\theta_0,\\eta_0)] + \\\\ & + (J_0^{-1} - \\hat{J}^{-1}) \\left(\\sqrt{n} \\En[\\psi(w_i;\\theta_0,\\eta_0)] + \\sqrt{n}R_{n,2}\\right) + \\\\ & + \\sqrt{n}J_0^{-1}\\underbrace{\\left(\\frac{1}{K} \\sum_{k=1}^K \\En_k[ \\psi(w_i;\\theta_0,\\hat{\\eta}_k)] - \\En[\\psi(w_i;\\theta_0,\\eta_0)]\\right)}_{R_{n,2}} \\end{align*} Show $\\norm{R_{n,1}} = O_p(n^{-1/2} + r_n)$ Show $\\norm{R_{n,2}}= O_p(n^{-1/2} r_n\u2019 + \\lambda_n + \\lambda_n\u2019)$ For details see the appendix of @chernozhukov2018. Proof outline: Lemma 6.1 \u00b6 Lemma 6.1 If $\\Pr(\\norm{X_m} > \\epsilon_m | Y_m) \\to_p 0$, then $\\Pr(\\norm{X_m}>\\epsilon_m) \\to 0$. If $\\Er[\\norm{X_m}^q/\\epsilon_m^q | Y_m] \\to_p 0$ for $q\\geq 1$, then $\\Pr(\\norm{X_m}>\\epsilon_m) \\to 0$. If $\\norm{X_m} = O_p(A_m)$ conditional on $Y_m$ (i.e. for any $\\ell_m \\to \\infty$, $\\Pr(\\norm{X_m} > \\ell_m A_m | Y_m) \\to_p 0$), then $\\norm{X_m} = O_p(A_m)$ unconditionally by dominated convergence from Markov\u2019s inequality follows from (a) Proof outline: $R_{n,1}$ \u00b6 R_{n,1} = \\hat{J}-J_0 = \\frac{1}{K} \\sum_k \\left( \\En_k[\\psi^a(w_i;\\hat{\\eta}_k)] - \\Er[\\psi^a(W;\\eta_0)] \\right) \\norm{\\En_k[\\psi^a(w_i;\\hat{\\eta}_k)] - \\Er[\\psi^a(W;\\eta_0)]} \\leq U_{1,k} + U_{2,k} where \\begin{align*} U_{1,k} = & \\norm{\\En_k[\\psi^a(w_i;\\hat{\\eta}_k)] - \\Er[\\psi^a(W;\\hat{\\eta}_k)| I^c_k]} \\\\ U_{2,k} = & \\norm{ \\Er[\\psi^a(W;\\hat{\\eta}_k)| I^c_k] - \\Er[\\psi^a(W;\\eta_0)]} \\end{align*} Proof outline: $R_{n,2}$ \u00b6 $R_{n,2} = \\frac{1}{K} \\sum_{k=1}^K \\En_k\\left[ \\psi(w_i;\\theta_0,\\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0) \\right]$ \\sqrt{n} \\norm{\\En_k\\left[ \\psi(w_i;\\theta_0,\\hat{\\eta}_k) - psi(w_i;\\theta_0,\\eta_0) \\right]} \\leq U_{3,k} + U_{4,k} where \\small \\begin{align*} U_{3,k} = & \\norm{ \\frac{1}{\\sqrt{n}} \\sum_{i \\in I_k} \\left( \\psi(w_i;\\theta_0, \\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0) - \\Er[ \\psi(w_i;\\theta_0, \\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0)] \\right) } \\\\ U_{4,k} = & \\sqrt{n} \\norm{ \\Er[ \\psi(w_i;\\theta_0, \\hat{\\eta}_k) | I_k^c] - \\Er[\\psi(w_i;\\theta_0,\\eta_0)]} \\end{align*} $U_{4,k} = \\sqrt{n} \\norm{f_k(1)}$ where f_k(r) = \\Er[\\psi(W;\\theta_0,\\eta_0 + r(\\hat{\\eta}_k - \\eta_0)) | I^c_k] - \\Er[\\psi(W;\\theta_0,\\eta_0)] Asymptotic normality \u00b6 \\sqrt{n} \\sigma^{-1} (\\hat{\\theta} - \\theta_0) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\bar{\\psi}(w_i) + O_p(\\rho_n) \\leadsto N(0,I) $\\rho_n := n^{-1/2} + r_n + r_n\u2019 + n^{1/2} (\\lambda_n +\\lambda_n\u2019) \\lesssim \\delta_n$ Influence function \\bar{\\psi}(w) = -\\sigma^{-1} J_0^{-1} \\psi(w;\\theta_0,\\eta_0) with \\sigma^2 = J_0^{-1} \\Er\\left[ \\psi(w;\\theta_0,\\eta_0) \\psi(w;\\theta_0,\\eta_0)'\\right] (J_0^{-1})' This is the DML2 case of theorem 3.1 of @chernozhukov2018. Creating orthogonal moments \u00b6 Need \\partial \\eta\\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right] \\approx 0 Given an some model, how do we find a suitable $\\psi$? Orthogonal scores via concentrating-out \u00b6 Original model: (\\theta_0, \\beta_0) = \\argmax_{\\theta, \\beta} \\Er[\\ell(W;\\theta,\\beta)] Define \\eta(\\theta) = \\beta(\\theta) = \\argmax_\\beta \\Er[\\ell(W;\\theta,\\beta)] First order condition from $\\max_\\theta \\Er[\\ell(W;\\theta,\\beta(\\theta))]$ is 0 = \\Er\\left[ \\underbrace{\\frac{\\partial \\ell}{\\partial \\theta} + \\frac{\\partial \\ell}{\\partial \\beta} \\frac{d \\beta}{d \\theta}}_{\\psi(W;\\theta,\\beta(\\theta))} \\right] Orthogonal scores via projection \u00b6 Original model: $m: \\mathcal{W} \\times \\R^{d_\\theta} \\times \\R^{d_h} \\to \\R^{d_m}$ \\Er[m(W;\\theta_0,h_0(Z))|R] = 0 Let $A(R)$ be $d_\\theta \\times d_m$ moment selection matrix, $\\Omega(R)$ $d_m \\times d_m$ weighting matrix, and \\begin{align*} \\Gamma(R) = & \\partial_{v'} \\Er[m(W;\\theta_0,v)|R]|_{v=h_0(Z)} \\\\ G(Z) = & \\Er[A(R)'\\Omega(R)^{-1} \\Gamma(R)|Z] \\Er[\\Gamma(R)'\\Omega(R)^{-1} \\Gamma(R) |Z]^{-1} \\\\ \\mu_0(R) = & A(R)'\\Omega(R)^{-1} - G(Z) \\Gamma(R)'\\Omega(R)^{-1} \\end{align*} $\\eta = (\\mu, h)$ and \\psi(W;\\theta, \\eta) = \\mu(R) m(W;\\theta, h(Z)) @chernozhukov2018 show how to construct orthogonal scores in a few examples via concentrating out and projection. @chernozhukov2015 also discusses creating orthogonal scores. Example: average derivative \u00b6 $x,y \\in \\R^1$, $\\Er[y|x] = f_0(x)$, $p(x) =$ density of $x$ $\\theta_0 = \\Er[f_0\u2019(x)]$ Joint objective \\min_{\\theta,f} \\Er\\left[ (y - f(x))^2 + (\\theta - f'(x)^2) \\right] Solve for minimizing $f$ given $\\theta$ f_\\theta(x) = \\Er[y|x] - \\theta \\partial_x \\log p(x) + f''(x) + f'(x) \\partial_x \\log p(x) Concentrated objective: \\min_\\theta \\Er\\left[ (y - f_\\theta(x))^2 + (\\theta - f_\\theta'(x)^2) \\right] First order condition at $f_\\theta = f_0$ gives 0 = \\Er\\left[ (y - f_0(x))\\partial_x \\log p(x) + (\\theta - f_0'(x)) \\right] We\u2019ll go over this derivation in lecture, but I don\u2019t think I\u2019ll have time to type it here. See @cnr2018 for an approach to estimating average derivatives (and other linear in $\\theta$ models) that doesn\u2019t require explicitly calculating an orthogonal moment condition. Example : average derivative with endogeneity \u00b6 $x,y \\in \\R^1$, $p(x) =$ density of $x$ Model : $\\Er[y - f(x) | z] = 0$ $\\theta_0 = \\Er[f_0\u2019(x)]$ Joint objective: \\min_{\\theta,f} \\Er\\left[ \\Er[y - f(x)|z]^2 + (\\theta - f'(x))^2 \\right] then f_\\theta(x) = (T^\\ast T)^{-1}\\left((T^\\ast \\Er[y|z])(x) - \\theta \\partial_x \\log p(x)\\right) where $T:\\mathcal{L}^2_{p} \\to \\mathcal{L}^2_{\\mu_z}$ with $(T f)(z) = \\Er[f(x) |z]$ and $T^\\ast :\\mathcal{L}^2_{\\mu_z} \\to \\mathcal{L}^2_{p}$ with $(T^\\ast g)(z) = \\Er[g(z) |x]$ Orthogonal moment condition : 0 = \\Er\\left[ \\Er[y - f(x) | z] (T (T^\\ast T)^{-1} \\partial_x \\log p)(z) + (\\theta - f'(x)) \\right] The first order condition for $f$ in the joint objective function is \\begin{align*} 0 = \\Er \\left[ \\Er[y-f(x) |z]\\Er[v(x)|z] + (\\theta - f'(x))(-v'(x)) \\right] \\end{align*} Writing these expectations as integrals, integrating by parts to get rid of $v\u2019(x)$, and switching the order of integration, gives \\begin{align*} 0 = \\int_\\mathcal{X} v(x)\\left( \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - (\\theta-f'(x))\\partial_x \\log p(x) - f''(x) \\right) p(x) dx \\end{align*} Notice that integrating by parts $\\int f\u2019\u2018(x) p(x) dx = \\int f\u2019 p\u2019(x) dx$ eliminates the terms with $f\u2019$ and $f\u2019\u2018$, leaving \\begin{align*} 0 = \\int_\\mathcal{X} v(x)\\left( \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - \\theta \\partial_x \\log p(x) \\right) p(x) dx \\end{align*} For this to be $0$ for all $v$, we need 0 = \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - \\theta \\partial_x \\log p(x) or equivalently using $T$ and $T^\\ast$, 0 = \\left(T^\\ast(E[y|z] - T f)\\right)(x) - \\theta \\partial_x \\log p(x) Note that $T$ and $T^\\ast$ are linear, and $T^\\ast$ is the adjoint of $T$. Also, identification of $f$ requires $T$ is one to one. Hence, if $f$ is identified, $T^\\ast T$ is invertible. Therefore, we can solve for $f$ as: f_\\theta(x) = (T^\\ast T)^{-1} \\left( (T^\\ast \\Er[y |z])(x) - \\theta \\partial \\log p(x) \\right) Plugging $f_\\theta(x)$ back into the objective function and then differentiating with respect to $\\theta$ gives the orthogonal moment condition on the slide. Verifying that this moment condition is indeed orthogonal is slightly tedious. Writing out some of the expectations as integrals, changing order of integrations, and judiciously factoring out terms, will eventually lead to the desired conclusion. @cfr2007 is an excellent review about estimating $(T^\\ast T)^{-1}$ and the inverses of other linear transformations. Example: average elasticity \u00b6 Demand $D(p)$, quantities $q$, instruments $z$ \\Er[q-D(p) |z] = 0 Average elasticity $\\theta \\Er[D\u2019(p)/D(p) | z ]$ Joint objective : \\min_{\\theta,D} \\Er\\left[ \\Er[q - D(p)|z]^2 + (\\theta - D'(p)/D(p))^2 \\right] Example: control function \u00b6 \\begin{align*} 0 = & \\Er[d - p(x,z) | x,z] \\\\ 0 = & \\Er[y - x\\beta - g(p(x,z)) | x,z] \\end{align*} Treatment heterogeneity \u00b6 Potential outcomes model Treatment $d \\in {0,1}$ Potential outcomes $y(1), y(0)$ Covariates $x$ Unconfoundedness or instruments Objects of interest: Conditional average treatment effect $s_0(x) = \\Er[y(1) - y(0) | x]$ Range and other measures of spread of conditional average treatment effect Most and least affected groups Fixed, finite groups \u00b6 $G_1, \u2026, G_K$ finite partition of support $(x)$ Estimate $\\Er[y(1) - y(0) | x \\in G_k]$ as above pros: easy inference, reveals some heterogeneity cons: poorly chosen partition hides some heterogeneity, searching partitions violates inference Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments \u00b6 @cddf2018 Use machine learning to find partition with sample splitting to allow easy inference Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]} Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments [generic-machine-learning-inference-on-heterogenous-treatment-effects-in-randomized-experiments-1] \u00b6 Define $G_k = 1{\\ell_{k-1} \\leq S(x) \\leq \\ell_k}$ Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\sum_k \\gamma_k (d-P(X)) 1(G_k) + \\epsilon $\\hat{\\gamma}_k \\to_p \\Er[y(1) - y(0) | G_k]$ Best linear projection of CATE \u00b6 Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]} Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\beta_0 (d-P(x)) + \\beta_1 (d-P(x))(S(x) - \\Er[S(x)]) + \\epsilon $\\hat{\\beta} 0, \\hat{\\beta}_1 \\to_p \\argmin \\Er[(s_0(x) - b_0 - b_1 (S(x)-E[S(x)]))^2]$ Inference on CATE \u00b6 Inference on $\\Er[y(1) - y(0) | x] = s_0(x)$ challenging when $x$ high dimensional and/or few restrictions on $s_0$ Pointwise results for random forests : @wager2018, @athey2016 Recent review of high dimensional inference : @bcchk2018 Random forest asymptotic normality \u00b6 @wager2018 $\\mu(x) = \\Er[y|x]$ $\\hat{\\mu}(x)$ estimate from honest random forest honest $=$ trees independent of outcomes being averaged sample-splitting or trees formed using another outcome Then \\frac{\\hat{\\mu}(x) - \\mu(x)}{\\hat{\\sigma}_n(x)} \\leadsto N(0,1) $\\hat{\\sigma}_n(x) \\to 0$ slower than $n^{-1/2}$ Random forest asymptotic normality \u00b6 Results are pointwise, but what about? $H_0: \\mu(x_1) = \\mu(x_2)$ ${x: \\mu(x) \\geq 0 }$ $\\Pr(\\mu(x) \\leq 0)$ Uniform inference \u00b6 @bcchk2018 @bccw2018 <!-- --- --> Bibliography \u00b6 <!-- --- -->","title":"Inference"},{"location":"ml-doubledebiased/#using-machine-learning-to-estimate-causal-effects","text":"","title":"Using machine learning to estimate causal effects"},{"location":"ml-doubledebiased/#double-debiased-machine-learning","text":"@chernozhukov2018, @chernozhukov2017 Parameter of interest $\\theta \\in \\R^{d_\\theta}$ Nuisance parameter $\\eta \\in T$ Moment conditions \\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta} with $\\psi$ known Estimate $\\hat{\\eta}$ using some machine learning method Estimate $\\hat{\\theta}$ using cross-fitting","title":"Double debiased machine learning"},{"location":"ml-doubledebiased/#cross-fitting","text":"Randomly partition into $K$ subsets $(I_k)_{k=1}^K$ $I^c_k = {1, \u2026, n} \\setminus I_k$ $\\hat{\\eta}_k =$ estimate of $\\eta$ using $I^c_k$ Estimator: \\begin{align*} 0 = & \\frac{1}{K} \\sum_{k=1}^K \\frac{K}{n} \\sum_{i \\in I_k} \\psi(w_i;\\hat{\\theta},\\hat{\\eta}_k) \\\\ 0 = & \\frac{1}{K} \\sum_{k=1}^K \\En_k[ \\psi(w_i;\\hat{\\theta},\\hat{\\eta}_k)] \\end{align*}","title":"Cross-fitting"},{"location":"ml-doubledebiased/#assumptions","text":"Linear score \\psi(w;\\theta,\\eta) = \\psi^a(w;\\eta) \\theta + \\psi^b(w;\\eta) Near Neyman orthogonality: \\lambda_n := \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{\\partial \\eta \\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right] } \\leq \\delta_n n^{-1/2}","title":"Assumptions"},{"location":"ml-doubledebiased/#assumptions-assumptions-1","text":"Rate conditions: for $\\delta_n \\to 0$ and $\\Delta_n \\to 0$, we have $\\Pr(\\hat{\\eta}_k \\in \\mathcal{T}_n) \\geq 1-\\Delta_n$ and \\begin{align*} r_n := & \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{ \\Er[\\psi^a(W;\\eta)] - \\Er[\\psi^a(W;\\eta_0)]} \\leq \\delta_n \\\\ r_n' := & \\sup_{\\eta \\in \\mathcal{T}_n} \\Er\\left[ \\norm{ \\psi(W;\\theta_0,\\eta) - \\psi(W;\\theta_0,\\eta_0)}^2 \\right]^{1/2} \\leq \\delta_n \\\\ \\lambda_n' := & \\sup_{r \\in (0,1), \\eta \\in \\mathcal{T}_n} \\norm{ \\partial_r^2 \\Er\\left[\\psi(W;\\theta_0, \\eta_0 + r(\\eta - \\eta_0)) \\right]} \\leq \\delta_n/\\sqrt{n} \\end{align*} Moments exist and other regularity conditions We focus on the case of linear scores to simplify proofs and all of our examples have scores linear in $\\theta$. @chernozhukov2018 cover nonlinear scores as well. These rate conditions might look a little strange. The rate conditions are stated this way because they\u2019re exactly what is needed for the result to work. $\\Delta_n$ and $\\delta_n$ are sequences converging to $0$. $\\mathcal{T}_n$ is a shrinking neighborhood of $\\eta_0$. A good exercise would be show that if $\\psi$ is a smooth function of $\\eta$ and $\\theta$, and $\\Er[(\\hat{\\eta}(x) - \\eta_0(x))^2]^{1/2} = O(\\epsilon_n) = o(n^{-1/4})$, then we can meet the above conditions with $r_n = r_n\u2019 = \\epsilon_n$ and $\\lambda_n\u2019 = \\epsilon_n^2$.","title":"Assumptions [assumptions-1]"},{"location":"ml-doubledebiased/#proof-outline","text":"Let \\begin{align*} \\hat{J} = & \\frac{1}{K} \\sum_{k=1}^K \\En_k [\\psi^a(w_i;\\hat{\\eta}_k)], \\\\ J_0 = & \\Er[\\psi^a(w_i;\\eta_0)], \\\\ R_{n,1} = & \\hat{J}-J_0 \\end{align*} Show: \\small \\begin{align*} \\sqrt{n}(\\hat{\\theta} - \\theta_0) = & -\\sqrt{n} J_0^{-1} \\En[\\psi(w_i;\\theta_0,\\eta_0)] + \\\\ & + (J_0^{-1} - \\hat{J}^{-1}) \\left(\\sqrt{n} \\En[\\psi(w_i;\\theta_0,\\eta_0)] + \\sqrt{n}R_{n,2}\\right) + \\\\ & + \\sqrt{n}J_0^{-1}\\underbrace{\\left(\\frac{1}{K} \\sum_{k=1}^K \\En_k[ \\psi(w_i;\\theta_0,\\hat{\\eta}_k)] - \\En[\\psi(w_i;\\theta_0,\\eta_0)]\\right)}_{R_{n,2}} \\end{align*} Show $\\norm{R_{n,1}} = O_p(n^{-1/2} + r_n)$ Show $\\norm{R_{n,2}}= O_p(n^{-1/2} r_n\u2019 + \\lambda_n + \\lambda_n\u2019)$ For details see the appendix of @chernozhukov2018.","title":"Proof outline:"},{"location":"ml-doubledebiased/#proof-outline-lemma-61","text":"Lemma 6.1 If $\\Pr(\\norm{X_m} > \\epsilon_m | Y_m) \\to_p 0$, then $\\Pr(\\norm{X_m}>\\epsilon_m) \\to 0$. If $\\Er[\\norm{X_m}^q/\\epsilon_m^q | Y_m] \\to_p 0$ for $q\\geq 1$, then $\\Pr(\\norm{X_m}>\\epsilon_m) \\to 0$. If $\\norm{X_m} = O_p(A_m)$ conditional on $Y_m$ (i.e. for any $\\ell_m \\to \\infty$, $\\Pr(\\norm{X_m} > \\ell_m A_m | Y_m) \\to_p 0$), then $\\norm{X_m} = O_p(A_m)$ unconditionally by dominated convergence from Markov\u2019s inequality follows from (a)","title":"Proof outline: Lemma 6.1"},{"location":"ml-doubledebiased/#proof-outline-r_n1","text":"R_{n,1} = \\hat{J}-J_0 = \\frac{1}{K} \\sum_k \\left( \\En_k[\\psi^a(w_i;\\hat{\\eta}_k)] - \\Er[\\psi^a(W;\\eta_0)] \\right) \\norm{\\En_k[\\psi^a(w_i;\\hat{\\eta}_k)] - \\Er[\\psi^a(W;\\eta_0)]} \\leq U_{1,k} + U_{2,k} where \\begin{align*} U_{1,k} = & \\norm{\\En_k[\\psi^a(w_i;\\hat{\\eta}_k)] - \\Er[\\psi^a(W;\\hat{\\eta}_k)| I^c_k]} \\\\ U_{2,k} = & \\norm{ \\Er[\\psi^a(W;\\hat{\\eta}_k)| I^c_k] - \\Er[\\psi^a(W;\\eta_0)]} \\end{align*}","title":"Proof outline: $R_{n,1}$"},{"location":"ml-doubledebiased/#proof-outline-r_n2","text":"$R_{n,2} = \\frac{1}{K} \\sum_{k=1}^K \\En_k\\left[ \\psi(w_i;\\theta_0,\\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0) \\right]$ \\sqrt{n} \\norm{\\En_k\\left[ \\psi(w_i;\\theta_0,\\hat{\\eta}_k) - psi(w_i;\\theta_0,\\eta_0) \\right]} \\leq U_{3,k} + U_{4,k} where \\small \\begin{align*} U_{3,k} = & \\norm{ \\frac{1}{\\sqrt{n}} \\sum_{i \\in I_k} \\left( \\psi(w_i;\\theta_0, \\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0) - \\Er[ \\psi(w_i;\\theta_0, \\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0)] \\right) } \\\\ U_{4,k} = & \\sqrt{n} \\norm{ \\Er[ \\psi(w_i;\\theta_0, \\hat{\\eta}_k) | I_k^c] - \\Er[\\psi(w_i;\\theta_0,\\eta_0)]} \\end{align*} $U_{4,k} = \\sqrt{n} \\norm{f_k(1)}$ where f_k(r) = \\Er[\\psi(W;\\theta_0,\\eta_0 + r(\\hat{\\eta}_k - \\eta_0)) | I^c_k] - \\Er[\\psi(W;\\theta_0,\\eta_0)]","title":"Proof outline: $R_{n,2}$"},{"location":"ml-doubledebiased/#asymptotic-normality","text":"\\sqrt{n} \\sigma^{-1} (\\hat{\\theta} - \\theta_0) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\bar{\\psi}(w_i) + O_p(\\rho_n) \\leadsto N(0,I) $\\rho_n := n^{-1/2} + r_n + r_n\u2019 + n^{1/2} (\\lambda_n +\\lambda_n\u2019) \\lesssim \\delta_n$ Influence function \\bar{\\psi}(w) = -\\sigma^{-1} J_0^{-1} \\psi(w;\\theta_0,\\eta_0) with \\sigma^2 = J_0^{-1} \\Er\\left[ \\psi(w;\\theta_0,\\eta_0) \\psi(w;\\theta_0,\\eta_0)'\\right] (J_0^{-1})' This is the DML2 case of theorem 3.1 of @chernozhukov2018.","title":"Asymptotic normality"},{"location":"ml-doubledebiased/#creating-orthogonal-moments","text":"Need \\partial \\eta\\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right] \\approx 0 Given an some model, how do we find a suitable $\\psi$?","title":"Creating orthogonal moments"},{"location":"ml-doubledebiased/#orthogonal-scores-via-concentrating-out","text":"Original model: (\\theta_0, \\beta_0) = \\argmax_{\\theta, \\beta} \\Er[\\ell(W;\\theta,\\beta)] Define \\eta(\\theta) = \\beta(\\theta) = \\argmax_\\beta \\Er[\\ell(W;\\theta,\\beta)] First order condition from $\\max_\\theta \\Er[\\ell(W;\\theta,\\beta(\\theta))]$ is 0 = \\Er\\left[ \\underbrace{\\frac{\\partial \\ell}{\\partial \\theta} + \\frac{\\partial \\ell}{\\partial \\beta} \\frac{d \\beta}{d \\theta}}_{\\psi(W;\\theta,\\beta(\\theta))} \\right]","title":"Orthogonal scores via concentrating-out"},{"location":"ml-doubledebiased/#orthogonal-scores-via-projection","text":"Original model: $m: \\mathcal{W} \\times \\R^{d_\\theta} \\times \\R^{d_h} \\to \\R^{d_m}$ \\Er[m(W;\\theta_0,h_0(Z))|R] = 0 Let $A(R)$ be $d_\\theta \\times d_m$ moment selection matrix, $\\Omega(R)$ $d_m \\times d_m$ weighting matrix, and \\begin{align*} \\Gamma(R) = & \\partial_{v'} \\Er[m(W;\\theta_0,v)|R]|_{v=h_0(Z)} \\\\ G(Z) = & \\Er[A(R)'\\Omega(R)^{-1} \\Gamma(R)|Z] \\Er[\\Gamma(R)'\\Omega(R)^{-1} \\Gamma(R) |Z]^{-1} \\\\ \\mu_0(R) = & A(R)'\\Omega(R)^{-1} - G(Z) \\Gamma(R)'\\Omega(R)^{-1} \\end{align*} $\\eta = (\\mu, h)$ and \\psi(W;\\theta, \\eta) = \\mu(R) m(W;\\theta, h(Z)) @chernozhukov2018 show how to construct orthogonal scores in a few examples via concentrating out and projection. @chernozhukov2015 also discusses creating orthogonal scores.","title":"Orthogonal scores via projection"},{"location":"ml-doubledebiased/#example-average-derivative","text":"$x,y \\in \\R^1$, $\\Er[y|x] = f_0(x)$, $p(x) =$ density of $x$ $\\theta_0 = \\Er[f_0\u2019(x)]$ Joint objective \\min_{\\theta,f} \\Er\\left[ (y - f(x))^2 + (\\theta - f'(x)^2) \\right] Solve for minimizing $f$ given $\\theta$ f_\\theta(x) = \\Er[y|x] - \\theta \\partial_x \\log p(x) + f''(x) + f'(x) \\partial_x \\log p(x) Concentrated objective: \\min_\\theta \\Er\\left[ (y - f_\\theta(x))^2 + (\\theta - f_\\theta'(x)^2) \\right] First order condition at $f_\\theta = f_0$ gives 0 = \\Er\\left[ (y - f_0(x))\\partial_x \\log p(x) + (\\theta - f_0'(x)) \\right] We\u2019ll go over this derivation in lecture, but I don\u2019t think I\u2019ll have time to type it here. See @cnr2018 for an approach to estimating average derivatives (and other linear in $\\theta$ models) that doesn\u2019t require explicitly calculating an orthogonal moment condition.","title":"Example: average derivative"},{"location":"ml-doubledebiased/#example-average-derivative-with-endogeneity","text":"$x,y \\in \\R^1$, $p(x) =$ density of $x$ Model : $\\Er[y - f(x) | z] = 0$ $\\theta_0 = \\Er[f_0\u2019(x)]$ Joint objective: \\min_{\\theta,f} \\Er\\left[ \\Er[y - f(x)|z]^2 + (\\theta - f'(x))^2 \\right] then f_\\theta(x) = (T^\\ast T)^{-1}\\left((T^\\ast \\Er[y|z])(x) - \\theta \\partial_x \\log p(x)\\right) where $T:\\mathcal{L}^2_{p} \\to \\mathcal{L}^2_{\\mu_z}$ with $(T f)(z) = \\Er[f(x) |z]$ and $T^\\ast :\\mathcal{L}^2_{\\mu_z} \\to \\mathcal{L}^2_{p}$ with $(T^\\ast g)(z) = \\Er[g(z) |x]$ Orthogonal moment condition : 0 = \\Er\\left[ \\Er[y - f(x) | z] (T (T^\\ast T)^{-1} \\partial_x \\log p)(z) + (\\theta - f'(x)) \\right] The first order condition for $f$ in the joint objective function is \\begin{align*} 0 = \\Er \\left[ \\Er[y-f(x) |z]\\Er[v(x)|z] + (\\theta - f'(x))(-v'(x)) \\right] \\end{align*} Writing these expectations as integrals, integrating by parts to get rid of $v\u2019(x)$, and switching the order of integration, gives \\begin{align*} 0 = \\int_\\mathcal{X} v(x)\\left( \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - (\\theta-f'(x))\\partial_x \\log p(x) - f''(x) \\right) p(x) dx \\end{align*} Notice that integrating by parts $\\int f\u2019\u2018(x) p(x) dx = \\int f\u2019 p\u2019(x) dx$ eliminates the terms with $f\u2019$ and $f\u2019\u2018$, leaving \\begin{align*} 0 = \\int_\\mathcal{X} v(x)\\left( \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - \\theta \\partial_x \\log p(x) \\right) p(x) dx \\end{align*} For this to be $0$ for all $v$, we need 0 = \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - \\theta \\partial_x \\log p(x) or equivalently using $T$ and $T^\\ast$, 0 = \\left(T^\\ast(E[y|z] - T f)\\right)(x) - \\theta \\partial_x \\log p(x) Note that $T$ and $T^\\ast$ are linear, and $T^\\ast$ is the adjoint of $T$. Also, identification of $f$ requires $T$ is one to one. Hence, if $f$ is identified, $T^\\ast T$ is invertible. Therefore, we can solve for $f$ as: f_\\theta(x) = (T^\\ast T)^{-1} \\left( (T^\\ast \\Er[y |z])(x) - \\theta \\partial \\log p(x) \\right) Plugging $f_\\theta(x)$ back into the objective function and then differentiating with respect to $\\theta$ gives the orthogonal moment condition on the slide. Verifying that this moment condition is indeed orthogonal is slightly tedious. Writing out some of the expectations as integrals, changing order of integrations, and judiciously factoring out terms, will eventually lead to the desired conclusion. @cfr2007 is an excellent review about estimating $(T^\\ast T)^{-1}$ and the inverses of other linear transformations.","title":"Example : average derivative with endogeneity"},{"location":"ml-doubledebiased/#example-average-elasticity","text":"Demand $D(p)$, quantities $q$, instruments $z$ \\Er[q-D(p) |z] = 0 Average elasticity $\\theta \\Er[D\u2019(p)/D(p) | z ]$ Joint objective : \\min_{\\theta,D} \\Er\\left[ \\Er[q - D(p)|z]^2 + (\\theta - D'(p)/D(p))^2 \\right]","title":"Example: average elasticity"},{"location":"ml-doubledebiased/#example-control-function","text":"\\begin{align*} 0 = & \\Er[d - p(x,z) | x,z] \\\\ 0 = & \\Er[y - x\\beta - g(p(x,z)) | x,z] \\end{align*}","title":"Example: control function"},{"location":"ml-doubledebiased/#treatment-heterogeneity","text":"Potential outcomes model Treatment $d \\in {0,1}$ Potential outcomes $y(1), y(0)$ Covariates $x$ Unconfoundedness or instruments Objects of interest: Conditional average treatment effect $s_0(x) = \\Er[y(1) - y(0) | x]$ Range and other measures of spread of conditional average treatment effect Most and least affected groups","title":"Treatment heterogeneity"},{"location":"ml-doubledebiased/#fixed-finite-groups","text":"$G_1, \u2026, G_K$ finite partition of support $(x)$ Estimate $\\Er[y(1) - y(0) | x \\in G_k]$ as above pros: easy inference, reveals some heterogeneity cons: poorly chosen partition hides some heterogeneity, searching partitions violates inference","title":"Fixed, finite groups"},{"location":"ml-doubledebiased/#generic-machine-learning-inference-on-heterogenous-treatment-effects-in-randomized-experiments","text":"@cddf2018 Use machine learning to find partition with sample splitting to allow easy inference Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]}","title":"Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments"},{"location":"ml-doubledebiased/#generic-machine-learning-inference-on-heterogenous-treatment-effects-in-randomized-experiments-generic-machine-learning-inference-on-heterogenous-treatment-effects-in-randomized-experiments-1","text":"Define $G_k = 1{\\ell_{k-1} \\leq S(x) \\leq \\ell_k}$ Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\sum_k \\gamma_k (d-P(X)) 1(G_k) + \\epsilon $\\hat{\\gamma}_k \\to_p \\Er[y(1) - y(0) | G_k]$","title":"Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments [generic-machine-learning-inference-on-heterogenous-treatment-effects-in-randomized-experiments-1]"},{"location":"ml-doubledebiased/#best-linear-projection-of-cate","text":"Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]} Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\beta_0 (d-P(x)) + \\beta_1 (d-P(x))(S(x) - \\Er[S(x)]) + \\epsilon $\\hat{\\beta} 0, \\hat{\\beta}_1 \\to_p \\argmin \\Er[(s_0(x) - b_0 - b_1 (S(x)-E[S(x)]))^2]$","title":"Best linear projection of CATE"},{"location":"ml-doubledebiased/#inference-on-cate","text":"Inference on $\\Er[y(1) - y(0) | x] = s_0(x)$ challenging when $x$ high dimensional and/or few restrictions on $s_0$ Pointwise results for random forests : @wager2018, @athey2016 Recent review of high dimensional inference : @bcchk2018","title":"Inference on CATE"},{"location":"ml-doubledebiased/#random-forest-asymptotic-normality","text":"@wager2018 $\\mu(x) = \\Er[y|x]$ $\\hat{\\mu}(x)$ estimate from honest random forest honest $=$ trees independent of outcomes being averaged sample-splitting or trees formed using another outcome Then \\frac{\\hat{\\mu}(x) - \\mu(x)}{\\hat{\\sigma}_n(x)} \\leadsto N(0,1) $\\hat{\\sigma}_n(x) \\to 0$ slower than $n^{-1/2}$","title":"Random forest asymptotic normality"},{"location":"ml-doubledebiased/#random-forest-asymptotic-normality_1","text":"Results are pointwise, but what about? $H_0: \\mu(x_1) = \\mu(x_2)$ ${x: \\mu(x) \\geq 0 }$ $\\Pr(\\mu(x) \\leq 0)$","title":"Random forest asymptotic normality"},{"location":"ml-doubledebiased/#uniform-inference","text":"@bcchk2018 @bccw2018 <!-- --- -->","title":"Uniform inference"},{"location":"ml-doubledebiased/#bibliography","text":"<!-- --- -->","title":"Bibliography"},{"location":"ml-intro/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 These notes will examine the incorportion of machine learning methods in classic econometric techniques for estimating causal effects. More specifally, we will focus on estimating treatment effects using matching and instrumental variables. In these estimators (and many others) there is a low-dimensional parameter of interest, such as the average treatment effect, but estimating it requires also estimating a potentially high dimensional nuisance parameter, such as the propensity score. Machine learning methods were developed for prediction with high dimensional data. It is then natural to try to use machine learning for estimating high dimensional nuisance parameters. Care must be taken when doing so though because the flexibility and complexity that make machine learning so good at prediction also pose challenges for inference. Example: partially linear model \u00b6 y_i = \\theta d_i + f(x_i) + \\epsilon_i Interested in $\\theta$ Assume $\\Er[\\epsilon|d,x] = 0$ Nuisance parameter $f()$ E.g. Donohue and Levitt ( 2001 ) The simplest example of the setting we will analyze is a partially linear model. We have some regressor of interest, $d$, and we want to estimate the effect of $d$ on $y$. We have a rich enough set of controls that we are willing to believe that $\\Er[\\epsilon|d,x] = 0$. $d_i$ and $y_i$ are scalars, while $x_i$ is a vector. We are not interested in $x$ per se, but we need to include it to avoid omitted variable bias. Typical applied econometric practice would be to choose some transfrom of $x$, say $X = T(x)$, where $X$ could be some subset of $x$, along with interactions, powers, and so on. Then estimate a linear regression y = \\theta d + X'\\beta + \\epsilon and then perhaps also report results for a handful of different choices of $T(x)$. Some downsides to the typical applied econometric practice include: The choice of T is arbitrary, which opens the door to specification searching and p-hacking. If $x$ is high dimensional, and $X$ is low dimensional, a poor choice will lead to omitted variable bias. Even if $x$ is low dimensional, if $f(x)$ is poorly approximated by $X\u2019\\beta$, there will be omitted variable bias. In some sense, machine learning can be thought of as a way to choose $T$ is an automated and data-driven way. There will be still be a choice of machine learning method and often tuning parameters for that method, so some arbitrary decisions remain. Hopefully though these decisions have less impact. You may already be familiar with traditional nonparametric econometric methods like series / sieves and kernels. These share much in common with machine learning. What makes machine learning different that traditional nonparametric methods? Machine learning methods appear to have better predictive performance, and arguably more practical data-driven methods to choose tuning parameters. Machine learning methods can deal with high dimensional $x$, while traditional nonparametric methods focus on situations with low dimensional $x$. Example : Effect of abortion on crime Donohue and Levitt ( 2001 ) estimate a regression of state crime rates on crime type relevant abortion rates and controls, y_{it} = \\theta a_{it} + x_{it}'\\beta + \\delta_i + \\gamma_t + \\epsilon_{it}. $a_{it}$ is a weighted average of lagged abortion rates in state $i$, with the weight on the $\\ell$th lag equal to the fraction of age $\\ell$ people who commit a given crime type. The covariates $x$ are the log of lagged prisoners per capita, the log of lagged police per capita, the unemployment rate, per-capita income, the poverty rate, AFDC generosity at time t\u2009\u2212\u200915, a dummy for concealed weapons law, and beer consumption per cap. Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ) reanalyze this setup using lasso to allow a more flexible specification of controls. They allow for many interactions and quadratic terms, leading to 284 controls. Example: Matching \u00b6 Binary treatment $d_i \\in {0,1}$ Potential outcomes $y_i(0), y_i(1)$, observe $y_i = y_i(d_i)$ Interested in average treatment effect : $\\theta = \\Er[y_i(1) - y_i(0)]$ Covariates $x_i$ Assume unconfoundedness : $d_i \\indep y_i(1), y_i(0) | x_i$ E.g. Connors et al. ( 1996 ) The partially linear and matching models are closely related. If the conditional mean independence assumption of the partially linear model is strengthing to conditional indepence then the partially linear model is a special case of the matching model with constant treatment effects, $y_i(1) - y_i(0) = \\theta$. Thus the matching model can be viewed as a generalization of the partially linear model that allows for treatment effect heterogeneity. Example: Matching \u00b6 Estimatable formulae for ATE : \\begin{align*} \\theta = & \\Er\\left[\\frac{y_i d_i}{\\Pr(d = 1 | x_i)} - \\frac{y_i (1-d_i)}{1-\\Pr(d=1|x_i)} \\right] \\\\ \\theta = & \\Er\\left[\\Er[y_i | d_i = 1, x_i] - \\Er[y_i | d_i = 0 , x_i]\\right] \\\\ \\theta = & \\Er\\left[ \\begin{array}{l} d_i \\frac{y_i - \\Er[y_i | d_i = 1, x_i]}{\\Pr(d=1|x_i)} - (1-d_i)\\frac{y_i - \\Er[y_i | d_i = 0, x_i]}{1-\\Pr(d=1|x_i)} + \\\\ + \\Er[y_i | d_i = 1, x_i] - \\Er[y_i | d_i = 0 , x_i]\\end{array}\\right] \\end{align*} All the expectations in these three formulae involve observable data. Thus, we can form an estimate of $\\theta$ be replacing the expectations and conditional expectations with appropriate estimators. For example, to use the first formula, we could estimate a logit model for the probability of treatment, \\hat{\\Pr}(d=1|x_i) = \\frac{e^{X_i' \\hat{\\beta}}}{1+e^{X_i'\\hat{\\beta}}} where, as above, $X$ is a some chosen transformation of $x_i$. Then we simply take an average to estimate $\\theta$. \\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i d_i}{\\hat{\\Pr}(d=1|x_i)} - \\frac{y_i(1-d_i)} {1-\\hat{\\Pr}(d=1|x_i)} As in the partially linear model, estimating the parameter of interest, $\\theta$, requires estimating a potentially high dimensional nuisance parameter, in this case $\\hat{\\Pr}(d=1|x)$. Similarly, the second expression would require estimating conditional expectations of $y$ as nuisance parameters. The third expression requires estimating both conditional expecations of $y$ and $d$. The third expression might appear needlessly complicated, but we will see later that it has some desirable properties that will make using it essential when very flexible machine learning estimators for the conditional expectations are used. The origin of the name \u201cmatching\u201d can be seen in the second expression. One way to estimate that expression would be to take each person in the treatment group, find someone with the same (or nearly the same) $x$ in the control group, difference the outcome of this matched pair, and then average over the whole sample. (Actually this gives the average treatment effect on the treated. For the ATE, you would also have to do the same with roles of the groups switched and average all the differences.) When $x$ is multi-dimensional, there is some ambiguity about what it means for two $x$ values to be nearly the same. An important insight of Rosenbaum and Rubin ( 1983 ) is that it is sufficient to match on the propensity score, $P(d=1|x)$, instead. Example: effectiveness of heart catheterization Connors et al. ( 1996 ) use matching to estimate the effectiveness of heart catheterization in critically ill patients. Their dataset contains 5735 patients and 72 covariates. Athey et al. ( 2017 ) reanalyze this data using a variety of machine learning methods. References: Imbens ( 2004 ) reviews the traditional econometric literature on matching. Imbens ( 2015 ) focuses on practical advice for matching and includes a brief mention of incorporating machine learning. Both the partially linear model and treatment effects model can be extended to situations with endogeneity and instrumental variables. Example: IV \u00b6 \\begin{align*} y_i = & \\theta d_i + f(x_i) + \\epsilon_i \\\\ d_i = & g(x_i, z_i) + u_i \\end{align*} Interested in $\\theta$ Assume $\\Er[\\epsilon|x,z] = 0$, $\\Er[u|x,z]=0$ Nuisance parameters $f()$, $g()$ E.g. Angrist and Krueger ( 1991 ) Most of the remarks about the partially linear model also apply here. Hartford et al. ( 2017 ) estimate a generalization of this model with $y_i = f(d_i, x_i) +\\epsilon$ using deep neural networks. Example : compulsory schooling and earnings Angrist and Krueger ( 1991 ) use quarter of birth as an instrument for years of schooling to estimate the effect of schooling on earnings. Since compulsory schooling laws typically specify a minimum age at which a person can leave school instead of a minimum years of schooling, people born at different times of the year can be required to complete one more or one less year of schooling. Compulsory schooling laws and their effect on attained schooling can vary with state and year. Hence, Angrist and Krueger ( 1991 ) considered specifying $g(x,z)$ as all interactions of quarter of birth, state, and year dummies. Having so many instruments leads to statistical problems with 2SLS. Example: LATE \u00b6 Binary instrumet $z_i \\in {0,1}$ Potential treatments $d_i(0), d_i(1) \\in {0,1}$, $d_i = d_i(Z_i)$ Potential outcomes $y_i(0), y_i(1)$, observe $y_i = y_i(d_i)$ Covariates $x_i$ $(y_i(1), y_i(0), d_i(1), d_i(0)) \\indep z_i | x_i$ Local average treatment effect: \\begin{align*} \\theta = & \\Er\\left[\\Er[y_i(1) - y_i(0) | x, d_i(1) > d_i(0)]\\right] \\\\ = & \\Er\\left[\\frac{\\Er[y|z=1,x] - \\Er[y|z=0,x]} {\\Er[d|z=1,x]-\\Er[d|z=0,x]} \\right] \\end{align*} See Abadie ( 2003 ). Belloni et al. ( 2017 ) analyze estimation of this model using Lasso and other machine learning methods. General setup \u00b6 Parameter of interest $\\theta \\in \\R^{d_\\theta}$ Nuisance parameter $\\eta \\in T$ Moment conditions \\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta} with $\\psi$ known Estimate $\\hat{\\eta}$ using some machine learning method Estimate $\\hat{\\theta}$ from \\En[\\psi(w_i;\\hat{\\theta},\\hat{\\eta}) ] = 0 We are following the setup and notation of Chernozhukov et al. ( 2018 ). As in the examples, the dimension of $\\theta$ is fixed and small. The dimension of $\\eta$ is large and might be increasing with sample size. $T$ is some normed vector space. Example: partially linear model \u00b6 y_i = \\theta_0 d_i + f_0(x_i) + \\epsilon_i Compare the estimates from $\\En[d_i(y_i - \\tilde{\\theta} d_i - \\hat{f}(x_i)) ] = 0$ and $\\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0$ where $m(x) = \\Er[d|x]$ and $\\mu(y) = \\Er[y|x]$ Example: partially linear model In the partially linear model, y_i = \\theta_0 d_i + f_0(x_i) + \\epsilon_i we can let $w_i = (y_i, x_i)$ and $\\eta = f$. There are a variety of candidates for $\\psi$. An obvious (but flawed) one is $\\psi(w_i; \\theta, \\eta) = (y_i - \\theta_0 d_i - f_0(x_i))d_i$. With this choice of $\\psi$, we have \\begin{align*} 0 = & \\En[d_i(y_i - \\hat{\\theta} d_i - \\hat{f}(x_i)) ] \\\\ \\hat{\\theta} = & \\En[d_i^2]^{-1} \\En[d_i (y_i - \\hat{f}(x_i))] \\\\ (\\hat{\\theta} - \\theta_0) = & \\En[d_i^2]^{-1} \\En[d_i \\epsilon_i] + \\En[d_i^2]^{-1} \\En[d_i (f_0(x_i) - \\hat{f}(x_i))] \\end{align*} The first term of this expression is quite promising. $d_i$ and $\\epsilon_i$ are both finite dimensional random variables, so a law of large numbers will apply to $\\En[d_i^2]$, and a central limit theorem would apply to $\\sqrt{n} \\En[d_i \\epsilon_i]$. Unfortunately, the second expression is problematic. To accomodate high dimensional $x$ and allow for flexible $f()$, machine learning estimators must introduce some sort of regularization to control variance. This regularization also introduces some bias. The bias generally vanishes, but at a slower than $\\sqrt{n}$ rate. Hence \\sqrt{n} \\En[d_i (f_0(x_i) - \\hat{f}(x_i))] \\to \\infty. To get around this problem, we must modify our estimate of $\\theta$. Let $m(x) = \\Er[d|x]$ and $\\mu(y) = \\Er[y|x]$. Let $\\hat{m}()$ and $\\hat{\\mu}()$ be some estimates. Then we can estimate $\\theta$ by partialling out: \\begin{align*} 0 = & \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] \\\\ \\hat{\\theta} = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i))] \\\\ (\\hat{\\theta} - \\theta_0) = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\left(\\En[(d_i - \\hat{m}(x_i))\\epsilon_i] + \\En[(d_i - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] \\right) \\\\ = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\left( a + b +c + d \\right) \\end{align*} where \\begin{align*} a = & \\En[(d_i -m(x_i))\\epsilon_i] \\\\ b = & \\En[(m(x_i)-\\hat{m}(x_i))\\epsilon_i] \\\\ c = & \\En[v_i(\\mu(x_i) - \\hat{\\mu}(x_i))] \\\\ d = & \\En[(m(x_i) - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] \\end{align*} with $v_i = d_i - \\Er[d_i | x_i]$. The term $a$ is well behaved and $\\sqrt{n}a \\leadsto N(0,\\Sigma)$ under standard conditions. Although terms $b$ and $c$ appear similar to the problematic term in the initial estimator, they are better behaved because $\\Er[v|x] = 0$ and $\\Er[\\epsilon|x] = 0$. This makes it possible, but difficult to show that $\\sqrt{n}b \\to_p = 0$ and $\\sqrt{n} c \\to_p = 0$, see e.g. Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ). However, the conditions on $\\hat{m}$ and $\\hat{\\mu}$ needed to show this are slightly restrictive, and appropriate conditions might not be known for all estimators. Chernozhukov et al. ( 2018 ) describe a sample splitting modification to $\\hat{\\theta}$ that allows $\\sqrt{n} b$ and $\\sqrt{n} c$ to vanish under weaker conditions (essentially the same rate condition as needed for $\\sqrt{n} d$ to vanish.) The last term, $d$, is a considerable improvement upon the first estimator. Instead of involving the error in one estimate, it now involes the product of the error in two estimates. By the Cauchy-Schwarz inequality, d \\leq \\sqrt{\\En[(m(x_i) - \\hat{m}(x_i))^2]} \\sqrt{\\En[(\\mu(x_i) - \\hat{\\mu}(x_i))^2]}. So if the estimates of $m$ and $\\mu$ converge at rates faster than $n^{-1/4}$, then $\\sqrt{n} d \\to_p 0$. This $n^{-1/4}$ rate is reached by many machine learning estimators. Lessons from the example \u00b6 Need an extra condition on moments \u2013 Neyman orthogonality \\partial \\eta \\Er[\\psi(W;\\theta_0,\\eta_0)](\\eta-\\eta_0) = 0 Want estimators faster than $n^{-1/4}$ in the prediction norm, \\sqrt{\\En[(\\hat{\\eta}(x_i) - \\eta(x_i))^2]} \\lesssim_P n^{-1/4} Also want estimators that satisfy something like \\sqrt{n} \\En[(\\eta(x_i)-\\hat{\\eta}(x_i))\\epsilon_i] = o_p(1) Sample splitting will make this easier References by topic \u00b6 Matching Imbens ( 2015 ) Imbens ( 2004 ) Surveys on machine learning in econometrics Athey and Imbens ( 2017 ) Mullainathan and Spiess ( 2017 ) Athey and Imbens ( 2018 ) Athey et al. ( 2017 ) Athey and Imbens ( 2015 ), Athey and Imbens ( 2018 ) Machine learning Breiman and others ( 2001 ) Friedman, Hastie, and Tibshirani ( 2009 ) James et al. ( 2013 ) Efron and Hastie ( 2016 ) Introduction to lasso Belloni and Chernozhukov ( 2011 ) Friedman, Hastie, and Tibshirani ( 2009 ) section 3.4 Chernozhukov, Hansen, and Spindler ( 2016 ) Introduction to random forests Friedman, Hastie, and Tibshirani ( 2009 ) section 9.2 Bold references are recommended reading. They are generally shorter and less technical than some of the others. Aspiring econometricians should read much more than just the bold references. Neyman orthogonalization Chernozhukov, Chetverikov, et al. ( 2017 ) Chernozhukov, Hansen, and Spindler ( 2015 ) Chernozhukov et al. ( 2018 ) Belloni et al. ( 2017 ) Lasso for causal inference Alexandre Belloni, Chernozhukov, and Hansen ( 2014 b ) Belloni et al. ( 2012 ) Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ) Chernozhukov, Goldman, et al. ( 2017 ) Chernozhukov, Hansen, and Spindler ( 2016 ) hdm R package Random forests for causal inference Athey, Tibshirani, and Wager ( 2016 ) Wager and Athey ( 2018 ) Tibshirani et al. ( 2018 ) grf R package Athey and Imbens ( 2016 ) There is considerable overlap among these categories. The papers listed under Neyman orthogonalization all include use of lasso and some include random forests. The papers on lasso all involve some use of orthogonalization. References [references] \u00b6 Abadie, Alberto. 2003. \u201cSemiparametric Instrumental Variable Estimation of Treatment Response Models.\u201d Journal of Econometrics 113 (2): 231\u201363. https://doi.org/https://doi.org/10.1016/S0304-4076(02)00201-4 . Angrist, Joshua D., and Alan B. Krueger. 1991. \u201cDoes Compulsory School Attendance Affect Schooling and Earnings?\u201d The Quarterly Journal of Economics 106 (4): pp. 979\u20131014. http://www.jstor.org/stable/2937954 . Athey, Susan, and Guido Imbens. 2015. \u201cLectures on Machine Learning.\u201d NBER Summer Institute. . \u2014\u2014\u2014. 2016. \u201cRecursive Partitioning for Heterogeneous Causal Effects.\u201d Proceedings of the National Academy of Sciences 113 (27): 7353\u201360. https://doi.org/10.1073/pnas.1510489113 . \u2014\u2014\u2014. 2018. \u201cMachine Learning and Econometrics.\u201d AEA Continuing Education. https://www.aeaweb.org/conference/cont-ed/2018-webcasts . Athey, Susan, Guido Imbens, Thai Pham, and Stefan Wager. 2017. \u201cEstimating Average Treatment Effects: Supplementary Analyses and Remaining Challenges.\u201d American Economic Review 107 (5): 278\u201381. https://doi.org/10.1257/aer.p20171042 . Athey, Susan, and Guido W. Imbens. 2017. \u201cThe State of Applied Econometrics: Causality and Policy Evaluation.\u201d Journal of Economic Perspectives 31 (2): 3\u201332. https://doi.org/10.1257/jep.31.2.3 . Athey, Susan, Julie Tibshirani, and Stefan Wager. 2016. \u201cGeneralized Random Forests.\u201d https://arxiv.org/abs/1610.01271 . Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen. 2012. \u201cSparse Models and Methods for Optimal Instruments with an Application to Eminent Domain.\u201d Econometrica 80 (6): 2369\u20132429. https://doi.org/10.3982/ECTA9626 . Belloni, A., V. Chernozhukov, I. Fern\u00e1ndez-Val, and C. Hansen. 2017. \u201cProgram Evaluation and Causal Inference with High-Dimensional Data.\u201d Econometrica 85 (1): 233\u201398. https://doi.org/10.3982/ECTA12723 . Belloni, Alexandre, and Victor Chernozhukov. 2011. \u201cHigh Dimensional Sparse Econometric Models: An Introduction.\u201d In Inverse Problems and High-Dimensional Estimation: Stats in the Ch\u00e2teau Summer School, August 31 - September 4, 2009 , edited by Pierre Alquier, Eric Gautier, and Gilles Stoltz, 121\u201356. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-19989-9_3 . Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014a. \u201cInference on Treatment Effects After Selection Among High-Dimensional Controls\u2020.\u201d The Review of Economic Studies 81 (2): 608\u201350. https://doi.org/10.1093/restud/rdt044 . \u2014\u2014\u2014. 2014b. \u201cHigh-Dimensional Methods and Inference on Structural and Treatment Effects.\u201d Journal of Economic Perspectives 28 (2): 29\u201350. https://doi.org/10.1257/jep.28.2.29 . Breiman, Leo, and others. 2001. \u201cStatistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).\u201d Statistical Science 16 (3): 199\u2013231. https://projecteuclid.org/euclid.ss/1009213726 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, and Whitney Newey. 2017. \u201cDouble/Debiased/Neyman Machine Learning of Treatment Effects.\u201d American Economic Review 107 (5): 261\u201365. https://doi.org/10.1257/aer.p20171038 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Chernozhukov, Victor, Matt Goldman, Vira Semenova, and Matt Taddy. 2017. \u201cOrthogonal Machine Learning for Demand Estimation: High Dimensional Causal Inference in Dynamic Panels.\u201d https://arxiv.org/abs/1712.09988v2 . Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. \u201chdm: High-Dimensional Metrics.\u201d R Journal 8 (2): 185\u201399. https://journal.r-project.org/archive/2016/RJ-2016-040/index.html . Chernozhukov, Victor, Christian Hansen, and Martin Spindler. 2015. \u201cValid Post-Selection and Post-Regularization Inference: An Elementary, General Approach.\u201d Annual Review of Economics 7 (1): 649\u201388. https://doi.org/10.1146/annurev-economics-012315-015826 . Connors, Alfred F., Theodore Speroff, Neal V. Dawson, Charles Thomas, Frank E. Harrell Jr, Douglas Wagner, Norman Desbiens, et al. 1996. \u201cThe Effectiveness of Right Heart Catheterization in the Initial Care of Critically Ill Patients.\u201d JAMA 276 (11): 889\u201397. https://doi.org/10.1001/jama.1996.03540110043030 . Donohue, John J., III, and Steven D. Levitt. 2001. \u201cThe Impact of Legalized Abortion on Crime*.\u201d The Quarterly Journal of Economics 116 (2): 379\u2013420. https://doi.org/10.1162/00335530151144050 . Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference . Vol. 5. Cambridge University Press. . Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning . Springer series in statistics. . Hartford, Jason, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. 2017. \u201cDeep IV: A Flexible Approach for Counterfactual Prediction.\u201d In Proceedings of the 34th International Conference on Machine Learning , edited by Doina Precup and Yee Whye Teh, 70:1414\u201323. Proceedings of Machine Learning Research. International Convention Centre, Sydney, Australia: PMLR. http://proceedings.mlr.press/v70/hartford17a.html . Imbens, Guido W. 2004. \u201cNonparametric Estimation of Average Treatment Effects Under Exogeneity: A Review.\u201d The Review of Economics and Statistics 86 (1): 4\u201329. https://doi.org/10.1162/003465304323023651 . \u2014\u2014\u2014. 2015. \u201cMatching Methods in Practice: Three Examples.\u201d Journal of Human Resources 50 (2): 373\u2013419. https://doi.org/10.3368/jhr.50.2.373 . James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning . Vol. 112. Springer. . Mullainathan, Sendhil, and Jann Spiess. 2017. \u201cMachine Learning: An Applied Econometric Approach.\u201d Journal of Economic Perspectives 31 (2): 87\u2013106. https://doi.org/10.1257/jep.31.2.87 . Rosenbaum, Paul R., and Donald B. Rubin. 1983. \u201cThe Central Role of the Propensity Score in Observational Studies for Causal Effects.\u201d Biometrika 70 (1): 41\u201355. https://doi.org/10.1093/biomet/70.1.41 . Tibshirani, Julie, Susan Athey, Stefan Wager, Rina Friedberg, Luke Miner, and Marvin Wright. 2018. Grf: Generalized Random Forests (Beta) . https://CRAN.R-project.org/package=grf . Wager, Stefan, and Susan Athey. 2018. \u201cEstimation and Inference of Heterogeneous Treatment Effects Using Random Forests.\u201d Journal of the American Statistical Association 0 (0): 1\u201315. https://doi.org/10.1080/01621459.2017.1319839 .","title":"Introduction"},{"location":"ml-intro/#introduction","text":"These notes will examine the incorportion of machine learning methods in classic econometric techniques for estimating causal effects. More specifally, we will focus on estimating treatment effects using matching and instrumental variables. In these estimators (and many others) there is a low-dimensional parameter of interest, such as the average treatment effect, but estimating it requires also estimating a potentially high dimensional nuisance parameter, such as the propensity score. Machine learning methods were developed for prediction with high dimensional data. It is then natural to try to use machine learning for estimating high dimensional nuisance parameters. Care must be taken when doing so though because the flexibility and complexity that make machine learning so good at prediction also pose challenges for inference.","title":"Introduction"},{"location":"ml-intro/#example-partially-linear-model","text":"y_i = \\theta d_i + f(x_i) + \\epsilon_i Interested in $\\theta$ Assume $\\Er[\\epsilon|d,x] = 0$ Nuisance parameter $f()$ E.g. Donohue and Levitt ( 2001 ) The simplest example of the setting we will analyze is a partially linear model. We have some regressor of interest, $d$, and we want to estimate the effect of $d$ on $y$. We have a rich enough set of controls that we are willing to believe that $\\Er[\\epsilon|d,x] = 0$. $d_i$ and $y_i$ are scalars, while $x_i$ is a vector. We are not interested in $x$ per se, but we need to include it to avoid omitted variable bias. Typical applied econometric practice would be to choose some transfrom of $x$, say $X = T(x)$, where $X$ could be some subset of $x$, along with interactions, powers, and so on. Then estimate a linear regression y = \\theta d + X'\\beta + \\epsilon and then perhaps also report results for a handful of different choices of $T(x)$. Some downsides to the typical applied econometric practice include: The choice of T is arbitrary, which opens the door to specification searching and p-hacking. If $x$ is high dimensional, and $X$ is low dimensional, a poor choice will lead to omitted variable bias. Even if $x$ is low dimensional, if $f(x)$ is poorly approximated by $X\u2019\\beta$, there will be omitted variable bias. In some sense, machine learning can be thought of as a way to choose $T$ is an automated and data-driven way. There will be still be a choice of machine learning method and often tuning parameters for that method, so some arbitrary decisions remain. Hopefully though these decisions have less impact. You may already be familiar with traditional nonparametric econometric methods like series / sieves and kernels. These share much in common with machine learning. What makes machine learning different that traditional nonparametric methods? Machine learning methods appear to have better predictive performance, and arguably more practical data-driven methods to choose tuning parameters. Machine learning methods can deal with high dimensional $x$, while traditional nonparametric methods focus on situations with low dimensional $x$. Example : Effect of abortion on crime Donohue and Levitt ( 2001 ) estimate a regression of state crime rates on crime type relevant abortion rates and controls, y_{it} = \\theta a_{it} + x_{it}'\\beta + \\delta_i + \\gamma_t + \\epsilon_{it}. $a_{it}$ is a weighted average of lagged abortion rates in state $i$, with the weight on the $\\ell$th lag equal to the fraction of age $\\ell$ people who commit a given crime type. The covariates $x$ are the log of lagged prisoners per capita, the log of lagged police per capita, the unemployment rate, per-capita income, the poverty rate, AFDC generosity at time t\u2009\u2212\u200915, a dummy for concealed weapons law, and beer consumption per cap. Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ) reanalyze this setup using lasso to allow a more flexible specification of controls. They allow for many interactions and quadratic terms, leading to 284 controls.","title":"Example: partially linear model"},{"location":"ml-intro/#example-matching","text":"Binary treatment $d_i \\in {0,1}$ Potential outcomes $y_i(0), y_i(1)$, observe $y_i = y_i(d_i)$ Interested in average treatment effect : $\\theta = \\Er[y_i(1) - y_i(0)]$ Covariates $x_i$ Assume unconfoundedness : $d_i \\indep y_i(1), y_i(0) | x_i$ E.g. Connors et al. ( 1996 ) The partially linear and matching models are closely related. If the conditional mean independence assumption of the partially linear model is strengthing to conditional indepence then the partially linear model is a special case of the matching model with constant treatment effects, $y_i(1) - y_i(0) = \\theta$. Thus the matching model can be viewed as a generalization of the partially linear model that allows for treatment effect heterogeneity.","title":"Example: Matching"},{"location":"ml-intro/#example-matching_1","text":"Estimatable formulae for ATE : \\begin{align*} \\theta = & \\Er\\left[\\frac{y_i d_i}{\\Pr(d = 1 | x_i)} - \\frac{y_i (1-d_i)}{1-\\Pr(d=1|x_i)} \\right] \\\\ \\theta = & \\Er\\left[\\Er[y_i | d_i = 1, x_i] - \\Er[y_i | d_i = 0 , x_i]\\right] \\\\ \\theta = & \\Er\\left[ \\begin{array}{l} d_i \\frac{y_i - \\Er[y_i | d_i = 1, x_i]}{\\Pr(d=1|x_i)} - (1-d_i)\\frac{y_i - \\Er[y_i | d_i = 0, x_i]}{1-\\Pr(d=1|x_i)} + \\\\ + \\Er[y_i | d_i = 1, x_i] - \\Er[y_i | d_i = 0 , x_i]\\end{array}\\right] \\end{align*} All the expectations in these three formulae involve observable data. Thus, we can form an estimate of $\\theta$ be replacing the expectations and conditional expectations with appropriate estimators. For example, to use the first formula, we could estimate a logit model for the probability of treatment, \\hat{\\Pr}(d=1|x_i) = \\frac{e^{X_i' \\hat{\\beta}}}{1+e^{X_i'\\hat{\\beta}}} where, as above, $X$ is a some chosen transformation of $x_i$. Then we simply take an average to estimate $\\theta$. \\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i d_i}{\\hat{\\Pr}(d=1|x_i)} - \\frac{y_i(1-d_i)} {1-\\hat{\\Pr}(d=1|x_i)} As in the partially linear model, estimating the parameter of interest, $\\theta$, requires estimating a potentially high dimensional nuisance parameter, in this case $\\hat{\\Pr}(d=1|x)$. Similarly, the second expression would require estimating conditional expectations of $y$ as nuisance parameters. The third expression requires estimating both conditional expecations of $y$ and $d$. The third expression might appear needlessly complicated, but we will see later that it has some desirable properties that will make using it essential when very flexible machine learning estimators for the conditional expectations are used. The origin of the name \u201cmatching\u201d can be seen in the second expression. One way to estimate that expression would be to take each person in the treatment group, find someone with the same (or nearly the same) $x$ in the control group, difference the outcome of this matched pair, and then average over the whole sample. (Actually this gives the average treatment effect on the treated. For the ATE, you would also have to do the same with roles of the groups switched and average all the differences.) When $x$ is multi-dimensional, there is some ambiguity about what it means for two $x$ values to be nearly the same. An important insight of Rosenbaum and Rubin ( 1983 ) is that it is sufficient to match on the propensity score, $P(d=1|x)$, instead. Example: effectiveness of heart catheterization Connors et al. ( 1996 ) use matching to estimate the effectiveness of heart catheterization in critically ill patients. Their dataset contains 5735 patients and 72 covariates. Athey et al. ( 2017 ) reanalyze this data using a variety of machine learning methods. References: Imbens ( 2004 ) reviews the traditional econometric literature on matching. Imbens ( 2015 ) focuses on practical advice for matching and includes a brief mention of incorporating machine learning. Both the partially linear model and treatment effects model can be extended to situations with endogeneity and instrumental variables.","title":"Example: Matching"},{"location":"ml-intro/#example-iv","text":"\\begin{align*} y_i = & \\theta d_i + f(x_i) + \\epsilon_i \\\\ d_i = & g(x_i, z_i) + u_i \\end{align*} Interested in $\\theta$ Assume $\\Er[\\epsilon|x,z] = 0$, $\\Er[u|x,z]=0$ Nuisance parameters $f()$, $g()$ E.g. Angrist and Krueger ( 1991 ) Most of the remarks about the partially linear model also apply here. Hartford et al. ( 2017 ) estimate a generalization of this model with $y_i = f(d_i, x_i) +\\epsilon$ using deep neural networks. Example : compulsory schooling and earnings Angrist and Krueger ( 1991 ) use quarter of birth as an instrument for years of schooling to estimate the effect of schooling on earnings. Since compulsory schooling laws typically specify a minimum age at which a person can leave school instead of a minimum years of schooling, people born at different times of the year can be required to complete one more or one less year of schooling. Compulsory schooling laws and their effect on attained schooling can vary with state and year. Hence, Angrist and Krueger ( 1991 ) considered specifying $g(x,z)$ as all interactions of quarter of birth, state, and year dummies. Having so many instruments leads to statistical problems with 2SLS.","title":"Example: IV"},{"location":"ml-intro/#example-late","text":"Binary instrumet $z_i \\in {0,1}$ Potential treatments $d_i(0), d_i(1) \\in {0,1}$, $d_i = d_i(Z_i)$ Potential outcomes $y_i(0), y_i(1)$, observe $y_i = y_i(d_i)$ Covariates $x_i$ $(y_i(1), y_i(0), d_i(1), d_i(0)) \\indep z_i | x_i$ Local average treatment effect: \\begin{align*} \\theta = & \\Er\\left[\\Er[y_i(1) - y_i(0) | x, d_i(1) > d_i(0)]\\right] \\\\ = & \\Er\\left[\\frac{\\Er[y|z=1,x] - \\Er[y|z=0,x]} {\\Er[d|z=1,x]-\\Er[d|z=0,x]} \\right] \\end{align*} See Abadie ( 2003 ). Belloni et al. ( 2017 ) analyze estimation of this model using Lasso and other machine learning methods.","title":"Example: LATE"},{"location":"ml-intro/#general-setup","text":"Parameter of interest $\\theta \\in \\R^{d_\\theta}$ Nuisance parameter $\\eta \\in T$ Moment conditions \\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta} with $\\psi$ known Estimate $\\hat{\\eta}$ using some machine learning method Estimate $\\hat{\\theta}$ from \\En[\\psi(w_i;\\hat{\\theta},\\hat{\\eta}) ] = 0 We are following the setup and notation of Chernozhukov et al. ( 2018 ). As in the examples, the dimension of $\\theta$ is fixed and small. The dimension of $\\eta$ is large and might be increasing with sample size. $T$ is some normed vector space.","title":"General setup"},{"location":"ml-intro/#example-partially-linear-model_1","text":"y_i = \\theta_0 d_i + f_0(x_i) + \\epsilon_i Compare the estimates from $\\En[d_i(y_i - \\tilde{\\theta} d_i - \\hat{f}(x_i)) ] = 0$ and $\\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0$ where $m(x) = \\Er[d|x]$ and $\\mu(y) = \\Er[y|x]$ Example: partially linear model In the partially linear model, y_i = \\theta_0 d_i + f_0(x_i) + \\epsilon_i we can let $w_i = (y_i, x_i)$ and $\\eta = f$. There are a variety of candidates for $\\psi$. An obvious (but flawed) one is $\\psi(w_i; \\theta, \\eta) = (y_i - \\theta_0 d_i - f_0(x_i))d_i$. With this choice of $\\psi$, we have \\begin{align*} 0 = & \\En[d_i(y_i - \\hat{\\theta} d_i - \\hat{f}(x_i)) ] \\\\ \\hat{\\theta} = & \\En[d_i^2]^{-1} \\En[d_i (y_i - \\hat{f}(x_i))] \\\\ (\\hat{\\theta} - \\theta_0) = & \\En[d_i^2]^{-1} \\En[d_i \\epsilon_i] + \\En[d_i^2]^{-1} \\En[d_i (f_0(x_i) - \\hat{f}(x_i))] \\end{align*} The first term of this expression is quite promising. $d_i$ and $\\epsilon_i$ are both finite dimensional random variables, so a law of large numbers will apply to $\\En[d_i^2]$, and a central limit theorem would apply to $\\sqrt{n} \\En[d_i \\epsilon_i]$. Unfortunately, the second expression is problematic. To accomodate high dimensional $x$ and allow for flexible $f()$, machine learning estimators must introduce some sort of regularization to control variance. This regularization also introduces some bias. The bias generally vanishes, but at a slower than $\\sqrt{n}$ rate. Hence \\sqrt{n} \\En[d_i (f_0(x_i) - \\hat{f}(x_i))] \\to \\infty. To get around this problem, we must modify our estimate of $\\theta$. Let $m(x) = \\Er[d|x]$ and $\\mu(y) = \\Er[y|x]$. Let $\\hat{m}()$ and $\\hat{\\mu}()$ be some estimates. Then we can estimate $\\theta$ by partialling out: \\begin{align*} 0 = & \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] \\\\ \\hat{\\theta} = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i))] \\\\ (\\hat{\\theta} - \\theta_0) = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\left(\\En[(d_i - \\hat{m}(x_i))\\epsilon_i] + \\En[(d_i - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] \\right) \\\\ = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\left( a + b +c + d \\right) \\end{align*} where \\begin{align*} a = & \\En[(d_i -m(x_i))\\epsilon_i] \\\\ b = & \\En[(m(x_i)-\\hat{m}(x_i))\\epsilon_i] \\\\ c = & \\En[v_i(\\mu(x_i) - \\hat{\\mu}(x_i))] \\\\ d = & \\En[(m(x_i) - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] \\end{align*} with $v_i = d_i - \\Er[d_i | x_i]$. The term $a$ is well behaved and $\\sqrt{n}a \\leadsto N(0,\\Sigma)$ under standard conditions. Although terms $b$ and $c$ appear similar to the problematic term in the initial estimator, they are better behaved because $\\Er[v|x] = 0$ and $\\Er[\\epsilon|x] = 0$. This makes it possible, but difficult to show that $\\sqrt{n}b \\to_p = 0$ and $\\sqrt{n} c \\to_p = 0$, see e.g. Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ). However, the conditions on $\\hat{m}$ and $\\hat{\\mu}$ needed to show this are slightly restrictive, and appropriate conditions might not be known for all estimators. Chernozhukov et al. ( 2018 ) describe a sample splitting modification to $\\hat{\\theta}$ that allows $\\sqrt{n} b$ and $\\sqrt{n} c$ to vanish under weaker conditions (essentially the same rate condition as needed for $\\sqrt{n} d$ to vanish.) The last term, $d$, is a considerable improvement upon the first estimator. Instead of involving the error in one estimate, it now involes the product of the error in two estimates. By the Cauchy-Schwarz inequality, d \\leq \\sqrt{\\En[(m(x_i) - \\hat{m}(x_i))^2]} \\sqrt{\\En[(\\mu(x_i) - \\hat{\\mu}(x_i))^2]}. So if the estimates of $m$ and $\\mu$ converge at rates faster than $n^{-1/4}$, then $\\sqrt{n} d \\to_p 0$. This $n^{-1/4}$ rate is reached by many machine learning estimators.","title":"Example: partially linear model"},{"location":"ml-intro/#lessons-from-the-example","text":"Need an extra condition on moments \u2013 Neyman orthogonality \\partial \\eta \\Er[\\psi(W;\\theta_0,\\eta_0)](\\eta-\\eta_0) = 0 Want estimators faster than $n^{-1/4}$ in the prediction norm, \\sqrt{\\En[(\\hat{\\eta}(x_i) - \\eta(x_i))^2]} \\lesssim_P n^{-1/4} Also want estimators that satisfy something like \\sqrt{n} \\En[(\\eta(x_i)-\\hat{\\eta}(x_i))\\epsilon_i] = o_p(1) Sample splitting will make this easier","title":"Lessons from the example"},{"location":"ml-intro/#references-by-topic","text":"Matching Imbens ( 2015 ) Imbens ( 2004 ) Surveys on machine learning in econometrics Athey and Imbens ( 2017 ) Mullainathan and Spiess ( 2017 ) Athey and Imbens ( 2018 ) Athey et al. ( 2017 ) Athey and Imbens ( 2015 ), Athey and Imbens ( 2018 ) Machine learning Breiman and others ( 2001 ) Friedman, Hastie, and Tibshirani ( 2009 ) James et al. ( 2013 ) Efron and Hastie ( 2016 ) Introduction to lasso Belloni and Chernozhukov ( 2011 ) Friedman, Hastie, and Tibshirani ( 2009 ) section 3.4 Chernozhukov, Hansen, and Spindler ( 2016 ) Introduction to random forests Friedman, Hastie, and Tibshirani ( 2009 ) section 9.2 Bold references are recommended reading. They are generally shorter and less technical than some of the others. Aspiring econometricians should read much more than just the bold references. Neyman orthogonalization Chernozhukov, Chetverikov, et al. ( 2017 ) Chernozhukov, Hansen, and Spindler ( 2015 ) Chernozhukov et al. ( 2018 ) Belloni et al. ( 2017 ) Lasso for causal inference Alexandre Belloni, Chernozhukov, and Hansen ( 2014 b ) Belloni et al. ( 2012 ) Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ) Chernozhukov, Goldman, et al. ( 2017 ) Chernozhukov, Hansen, and Spindler ( 2016 ) hdm R package Random forests for causal inference Athey, Tibshirani, and Wager ( 2016 ) Wager and Athey ( 2018 ) Tibshirani et al. ( 2018 ) grf R package Athey and Imbens ( 2016 ) There is considerable overlap among these categories. The papers listed under Neyman orthogonalization all include use of lasso and some include random forests. The papers on lasso all involve some use of orthogonalization.","title":"References by topic"},{"location":"ml-intro/#references-references","text":"Abadie, Alberto. 2003. \u201cSemiparametric Instrumental Variable Estimation of Treatment Response Models.\u201d Journal of Econometrics 113 (2): 231\u201363. https://doi.org/https://doi.org/10.1016/S0304-4076(02)00201-4 . Angrist, Joshua D., and Alan B. Krueger. 1991. \u201cDoes Compulsory School Attendance Affect Schooling and Earnings?\u201d The Quarterly Journal of Economics 106 (4): pp. 979\u20131014. http://www.jstor.org/stable/2937954 . Athey, Susan, and Guido Imbens. 2015. \u201cLectures on Machine Learning.\u201d NBER Summer Institute. . \u2014\u2014\u2014. 2016. \u201cRecursive Partitioning for Heterogeneous Causal Effects.\u201d Proceedings of the National Academy of Sciences 113 (27): 7353\u201360. https://doi.org/10.1073/pnas.1510489113 . \u2014\u2014\u2014. 2018. \u201cMachine Learning and Econometrics.\u201d AEA Continuing Education. https://www.aeaweb.org/conference/cont-ed/2018-webcasts . Athey, Susan, Guido Imbens, Thai Pham, and Stefan Wager. 2017. \u201cEstimating Average Treatment Effects: Supplementary Analyses and Remaining Challenges.\u201d American Economic Review 107 (5): 278\u201381. https://doi.org/10.1257/aer.p20171042 . Athey, Susan, and Guido W. Imbens. 2017. \u201cThe State of Applied Econometrics: Causality and Policy Evaluation.\u201d Journal of Economic Perspectives 31 (2): 3\u201332. https://doi.org/10.1257/jep.31.2.3 . Athey, Susan, Julie Tibshirani, and Stefan Wager. 2016. \u201cGeneralized Random Forests.\u201d https://arxiv.org/abs/1610.01271 . Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen. 2012. \u201cSparse Models and Methods for Optimal Instruments with an Application to Eminent Domain.\u201d Econometrica 80 (6): 2369\u20132429. https://doi.org/10.3982/ECTA9626 . Belloni, A., V. Chernozhukov, I. Fern\u00e1ndez-Val, and C. Hansen. 2017. \u201cProgram Evaluation and Causal Inference with High-Dimensional Data.\u201d Econometrica 85 (1): 233\u201398. https://doi.org/10.3982/ECTA12723 . Belloni, Alexandre, and Victor Chernozhukov. 2011. \u201cHigh Dimensional Sparse Econometric Models: An Introduction.\u201d In Inverse Problems and High-Dimensional Estimation: Stats in the Ch\u00e2teau Summer School, August 31 - September 4, 2009 , edited by Pierre Alquier, Eric Gautier, and Gilles Stoltz, 121\u201356. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-19989-9_3 . Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014a. \u201cInference on Treatment Effects After Selection Among High-Dimensional Controls\u2020.\u201d The Review of Economic Studies 81 (2): 608\u201350. https://doi.org/10.1093/restud/rdt044 . \u2014\u2014\u2014. 2014b. \u201cHigh-Dimensional Methods and Inference on Structural and Treatment Effects.\u201d Journal of Economic Perspectives 28 (2): 29\u201350. https://doi.org/10.1257/jep.28.2.29 . Breiman, Leo, and others. 2001. \u201cStatistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).\u201d Statistical Science 16 (3): 199\u2013231. https://projecteuclid.org/euclid.ss/1009213726 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, and Whitney Newey. 2017. \u201cDouble/Debiased/Neyman Machine Learning of Treatment Effects.\u201d American Economic Review 107 (5): 261\u201365. https://doi.org/10.1257/aer.p20171038 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Chernozhukov, Victor, Matt Goldman, Vira Semenova, and Matt Taddy. 2017. \u201cOrthogonal Machine Learning for Demand Estimation: High Dimensional Causal Inference in Dynamic Panels.\u201d https://arxiv.org/abs/1712.09988v2 . Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. \u201chdm: High-Dimensional Metrics.\u201d R Journal 8 (2): 185\u201399. https://journal.r-project.org/archive/2016/RJ-2016-040/index.html . Chernozhukov, Victor, Christian Hansen, and Martin Spindler. 2015. \u201cValid Post-Selection and Post-Regularization Inference: An Elementary, General Approach.\u201d Annual Review of Economics 7 (1): 649\u201388. https://doi.org/10.1146/annurev-economics-012315-015826 . Connors, Alfred F., Theodore Speroff, Neal V. Dawson, Charles Thomas, Frank E. Harrell Jr, Douglas Wagner, Norman Desbiens, et al. 1996. \u201cThe Effectiveness of Right Heart Catheterization in the Initial Care of Critically Ill Patients.\u201d JAMA 276 (11): 889\u201397. https://doi.org/10.1001/jama.1996.03540110043030 . Donohue, John J., III, and Steven D. Levitt. 2001. \u201cThe Impact of Legalized Abortion on Crime*.\u201d The Quarterly Journal of Economics 116 (2): 379\u2013420. https://doi.org/10.1162/00335530151144050 . Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference . Vol. 5. Cambridge University Press. . Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning . Springer series in statistics. . Hartford, Jason, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. 2017. \u201cDeep IV: A Flexible Approach for Counterfactual Prediction.\u201d In Proceedings of the 34th International Conference on Machine Learning , edited by Doina Precup and Yee Whye Teh, 70:1414\u201323. Proceedings of Machine Learning Research. International Convention Centre, Sydney, Australia: PMLR. http://proceedings.mlr.press/v70/hartford17a.html . Imbens, Guido W. 2004. \u201cNonparametric Estimation of Average Treatment Effects Under Exogeneity: A Review.\u201d The Review of Economics and Statistics 86 (1): 4\u201329. https://doi.org/10.1162/003465304323023651 . \u2014\u2014\u2014. 2015. \u201cMatching Methods in Practice: Three Examples.\u201d Journal of Human Resources 50 (2): 373\u2013419. https://doi.org/10.3368/jhr.50.2.373 . James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning . Vol. 112. Springer. . Mullainathan, Sendhil, and Jann Spiess. 2017. \u201cMachine Learning: An Applied Econometric Approach.\u201d Journal of Economic Perspectives 31 (2): 87\u2013106. https://doi.org/10.1257/jep.31.2.87 . Rosenbaum, Paul R., and Donald B. Rubin. 1983. \u201cThe Central Role of the Propensity Score in Observational Studies for Causal Effects.\u201d Biometrika 70 (1): 41\u201355. https://doi.org/10.1093/biomet/70.1.41 . Tibshirani, Julie, Susan Athey, Stefan Wager, Rina Friedberg, Luke Miner, and Marvin Wright. 2018. Grf: Generalized Random Forests (Beta) . https://CRAN.R-project.org/package=grf . Wager, Stefan, and Susan Athey. 2018. \u201cEstimation and Inference of Heterogeneous Treatment Effects Using Random Forests.\u201d Journal of the American Statistical Association 0 (0): 1\u201315. https://doi.org/10.1080/01621459.2017.1319839 .","title":"References [references]"},{"location":"ml-julia/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 This document is a companion to my \u201cMachine learning in economics\u201d . Those notes discuss the recent use of machine learning in economics, with a focus on lasso and random forests. The code in those notes is written in R. This document will look at similar code in Julia. RCall \u00b6 If you want to use the methods of Chernozhukov and coauthors implements in the R packaga @hdm or the methods of Athey and coauthors implemented in the R package @grf , then it makes sense to use the R pacakge. You could simply write all your code in R. However, if you prefer using Julia, you can just call the necessary R functions with RCall.jl . Here, we load the pipeline data used in the machine learning methods notes , and do some cleaning in Julia. using RCall, DataFrames, Missings, Statistics R\"load(paste($(docdir),\\\"/rmd/pipelines.Rdata\\\",sep=\\\"\\\"))\" println(R\"ls()\") data = @rget data # data on left is new Julia variable, data on right is the one in R println(R\"summary(data[,1:5])\") println(describe(data[:,1:5])) for c in 59:107 # columns of state mileage, want missing->0 replace!(x->(ismissing(x) || isnan(x)) ? 0.0 : x, data[!,c]) end println(describe(data[:,59:65])) Error: InitError: Try adding /usr/lib64/R/lib to the \"LD_LIBRARY_PATH\" environmental variable and restarting Julia. during initialization of module RCall Suppose we want to estimate the coefficient on transPlant (capital) in a partially linear model with transProfit (profit) as the outcome. This can be done with the R function hdm::rlassoEffects . R\"library(hdm)\" completedata = dropmissing(data,[1:10..., 59:122...], disallowmissing=true) y = completedata[!,:transProfit] inc = .!isnan.(y) y = y[inc] X = completedata[inc,[6:7..., 59:121...]] cols = [std(X[!,c])>0 for c in 1:ncol(X)] X = X[:,cols] est = R\"rlassoEffects($(X), $(y), index=c(1:2))\" R\"summary($est)\" Error: LoadError: UndefVarError: @R_str not defined in expression starting at /home/paul/.julia/dev/NeuralNetworkEconomics/docs/jmd/ml-julia.jmd:2 MLJ.jl \u00b6 MLJ.jl is a machine learning framework for Julia. It gives a unified interface for many machine learning algorithms and tasks. Similar R packages include caret and MLR . scikit-learn is a similar Python package. For more information on MLJ see MLJ.jl docs MLJ tutorials You can see a list of models registered to work with MLJ.jl on github , or by calling MLJ::models() . using MLJ models() 200-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstr act_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hy perparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_ methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteratio n_parameter, :load_path, :package_license, :package_url, :package_uuid, :pr edict_scitype, :prediction_type, :reporting_operations, :reports_feature_im portances, :supports_class_weights, :supports_online, :supports_training_lo sses, :supports_weights, :transform_scitype, :input_scitype, :target_scityp e, :output_scitype)}}: (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... ) (name = ABODDetector, package_name = OutlierDetectionPython, ... ) (name = AEDetector, package_name = OutlierDetectionNetworks, ... ) (name = ARDRegressor, package_name = ScikitLearn, ... ) (name = AdaBoostClassifier, package_name = ScikitLearn, ... ) (name = AdaBoostRegressor, package_name = ScikitLearn, ... ) (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... ) (name = AffinityPropagation, package_name = ScikitLearn, ... ) (name = AgglomerativeClustering, package_name = ScikitLearn, ... ) (name = BM25Transformer, package_name = MLJText, ... ) \u22ee (name = TheilSenRegressor, package_name = ScikitLearn, ... ) (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... ) (name = UnivariateDiscretizer, package_name = MLJModels, ... ) (name = UnivariateFillImputer, package_name = MLJModels, ... ) (name = UnivariateStandardizer, package_name = MLJModels, ... ) (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... ) (name = XGBoostClassifier, package_name = XGBoost, ... ) (name = XGBoostCount, package_name = XGBoost, ... ) (name = XGBoostRegressor, package_name = XGBoost, ... ) To use these models, you need the corresponding package to be installed and loaded. The @load macro will load the needed package(s) for any model. Lasso = @load LassoRegressor pkg=MLJLinearModels import MLJLinearModels \u2714 MLJLinearModels.LassoRegressor Let\u2019s fit lasso to the same pipeline data as above. lasso = machine(Lasso(lambda=1.0), X, y) train,test = partition(eachindex(y), 0.6, shuffle=true) fit!(lasso, rows=train) yhat = predict(lasso, rows=test) println(yhat[1:10]) println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") Error: UndefVarError: X not defined That doesn\u2019t look very good. All the predictions are zero. This could happen when the regularization parameter, lambda , is too large. However, in this case the problem is something else. The warning messages indicate numeric problems when minimizing the lasso objective function. This can happen when X is poorly scaled. The algorithm used to compute the lasso estimates works best when the coefficients are all roughly the same scale. The existing X \u2019s have wildly different scales, which causes problems. This situation is common, so MLJ.jl has functions to standardize variables. It is likely that the hdm package in R does something similar internally. lasso_stdx = Pipeline(Standardizer(), Lasso(lambda=1.0*std(y[train]), solver=MLJLinearModels.ISTA(max_iter=10000)) ) m = machine(lasso_stdx, X, y) fit!(m, rows=train, force=true) yhat = predict(m , rows=test) println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") # Get the fitted coefficients coef = fitted_params(m).lasso_regressor.coefs intercept = fitted_params(m).lasso_regressor.intercept sum(abs(c[2])>1e-8 for c in coef) # number non-zero Error: UndefVarError: y not defined If we want to tune lambda using cross-validation, we can use the range and TunedModel functions. r = range(lasso_stdx, :(lasso_regressor.lambda), lower=1e1, upper=1e10, scale=:log) t=TunedModel(model=lasso_stdx, resampling=CV(nfolds=5), tuning=Grid(resolution=10), ranges=r, measure=rms) m = machine(t, X, y) fit!(m, rows=train, verbosity=1) yhat = predict(m , rows=test) println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") Error: UndefVarError: lasso_stdx not defined using Plots cvmse = m.report.plotting.measurements \u03bb = Float64.(m.report.plotting.parameter_values[:]) s = sortperm(\u03bb) plot(\u03bb[s], cvmse[s], xlab=\"\u03bb\", ylab=\"CV(MSE)\") Error: UndefVarError: m not defined Flux.jl \u00b6 Flux.jl is another Julia package for machine learning. It seems to be emerging as the leading Julia package for neural networks and deep learning, but other machine learning models can also be implemented using Flux.jl . Let\u2019s create a lasso model in Flux.jl . using Flux, LinearAlgebra # Scale the variables Xstd = Flux.normalise(Matrix(X), dims=1) X_train = Xstd[train,:] X_test = Xstd[test,:] yscale = std(y[train]) ymean = mean(y[train]) ystd = (y .- ymean)./yscale y_train = ystd[train] y_test = ystd[test] # Set up the model parameters and initial values \u03b2ols = (X_train'*X_train) \\ (X_train'*(y_train .- mean(y_train))) \u03b2 = zeros(ncol(X)) b = [mean(y_train)] # Define the loss function \u03c8 = ones(length(\u03b2)) \u03bb = 2.0 pred(x) = b .+ x*\u03b2 mse(x,y) = mean( (pred(x) .- y).^2 ) penalty(y) = \u03bb/length(y)*norm(\u03c8.*\u03b2,1) loss(x,y) = mse(x,y) + penalty(y) @show loss(X_train,y_train) # minimize loss maxiter=2000 obj = zeros(maxiter) mse_train = zeros(maxiter) mse_test = zeros(maxiter) opt = Flux.AMSGrad() for i in 1:maxiter Flux.train!(loss, Flux.params(\u03b2, b), [(X_train, y_train)], opt) mse_train[i] = mse(X_train,y_train) mse_test[i] = mse(X_test, y_test) obj[i] = loss(X_train,y_train) end lo = 1 hi = 2000 plot(obj[lo:hi], ylab=\"Loss=MSE + \u03bb/n*||\u03b2||\u2081\", xlab=\"Iteration\") Error: UndefVarError: X not defined plot(lo:hi, [mse_train[lo:hi] mse_test[lo:hi]], ylab=\"MSE\", xaxis=(\"Iteration\"), lab=[\"Train\" \"Test\"]) Error: UndefVarError: lo not defined The minimization methods in Flux.train! are all variants of gradient descent. Each call to Flux.train! runs one iteration of the specified solver. To find a local minimum, Flux.train! can be called repeatedly until progress stops. The above loop is a simple way to do this. The @epoch macro can also be useful. Gradient descent works well for neural networks, but is not ideal for Lasso. Without further adjustment, gradient descent gets stuck in a cycle as jumps from one side of the other of the absolute value in the lasso penalty. Nonetheless, the results are near the true minimum, even though it never exactly gets there. Lux.jl \u00b6 A promising alternative to Flux.jl is Lux.jl . 1 Lux.jl and Flux.jl share many features and backend code. Lux.jl has a more function focused interface with explicit parameter passing. This is a more \u201cJulian\u201d style of programming. 2 For comparison, let\u2019s implement the same Lasso model in Lux. import Lux # Lux shares many function names with Flux, so import instead of using to avoid confusion import Random, Zygote using Test # Seeding rng = Random.default_rng() Random.seed!(rng, 0) # define the model X_train = Matrix(X)[train,:] X_test = Matrix(X)[test,:] y_train = y[train] y_test = y[test] function standardizer(xtrain) m = std(xtrain, dims=1) s = std(xtrain, dims=1) (x->(x .- m)./s , xs->xs.*s .+ m) end stdizex, _ = standardizer(X_train) stdizey, unstdizey = standardizer(y_train) @test unstdizey(stdizey(y_test)) \u2248 y_test ys = stdizey(y_train) Xs = stdizex(X_train) # Set up the model parameters and initial values \u03b2ols = (Xs'*Xs) \\ (Xs'*(ys .- mean(ys))) b = [mean(ys)] m = Lux.Chain(X->stdizex(X)', Lux.Dense(size(X_train,2), 1, init_weight=zeros, init_bias=zeros) ) ps, st = Lux.setup(rng, m) ps.layer_2.weight .= \u03b2ols' ps.layer_2.bias .= b mse(m, ps, st, X, y) = mean(abs2, m(X, ps, st)[1]' .- stdizey(y)) mseraw(m,ps,st,X,y) = mean(abs2, unstdizey(m(X, ps, st)[1]') .- y) \u2113 = let \u03bb = 2.0, \u03c8 = ones(size(\u03b2ols')), st=st penalty(ps,y) = \u03bb/length(y)*norm(\u03c8.*ps.layer_2.weight,1) loss(ps, X, y, m) = mse(m, ps, st, X, y) + penalty(ps,y) end @show \u2113(ps,X_train,y_train,m) # minimize loss opt = Lux.Optimisers.AMSGrad() #ADAM(0.001) optstate = Lux.Optimisers.setup(opt, ps) maxiter=2000 obj = zeros(maxiter) mse_train = zeros(maxiter) mse_test = zeros(maxiter) for i in 1:maxiter # Compute the gradient gs = Zygote.gradient(ps->\u2113(ps, X_train, y_train, m), ps)[1] # Perform parameter update optstate, ps = Lux.Optimisers.update(optstate, ps, gs) mse_train[i] = mse(m, ps, st, X_train,y_train) mse_test[i] = mse(m, ps, st, X_test, y_test) obj[i] = \u2113(ps,X_train,y_train,m) end lo = 1 hi = 250 plot(lo:hi,obj[lo:hi], ylab=\"Loss=MSE + \u03bb/n*||\u03b2||\u2081\", xlab=\"Iteration\") Error: UndefVarError: X not defined Additional Resources \u00b6 @klok2019 Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence References \u00b6 These notes were originally written before Lux.jl existed. If I were starting over, I would use Lux.jl instead of Flux.jl. \u21a9 Flux.jl drew inspiration for its interface from Tensorflow and PyTorch. Implicit parameters makes some sense in an object oriented language like Python, but it is not the most natural style for Julia. \u21a9","title":"With Julia"},{"location":"ml-julia/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min}","title":"About this document"},{"location":"ml-julia/#introduction","text":"This document is a companion to my \u201cMachine learning in economics\u201d . Those notes discuss the recent use of machine learning in economics, with a focus on lasso and random forests. The code in those notes is written in R. This document will look at similar code in Julia.","title":"Introduction"},{"location":"ml-julia/#rcall","text":"If you want to use the methods of Chernozhukov and coauthors implements in the R packaga @hdm or the methods of Athey and coauthors implemented in the R package @grf , then it makes sense to use the R pacakge. You could simply write all your code in R. However, if you prefer using Julia, you can just call the necessary R functions with RCall.jl . Here, we load the pipeline data used in the machine learning methods notes , and do some cleaning in Julia. using RCall, DataFrames, Missings, Statistics R\"load(paste($(docdir),\\\"/rmd/pipelines.Rdata\\\",sep=\\\"\\\"))\" println(R\"ls()\") data = @rget data # data on left is new Julia variable, data on right is the one in R println(R\"summary(data[,1:5])\") println(describe(data[:,1:5])) for c in 59:107 # columns of state mileage, want missing->0 replace!(x->(ismissing(x) || isnan(x)) ? 0.0 : x, data[!,c]) end println(describe(data[:,59:65])) Error: InitError: Try adding /usr/lib64/R/lib to the \"LD_LIBRARY_PATH\" environmental variable and restarting Julia. during initialization of module RCall Suppose we want to estimate the coefficient on transPlant (capital) in a partially linear model with transProfit (profit) as the outcome. This can be done with the R function hdm::rlassoEffects . R\"library(hdm)\" completedata = dropmissing(data,[1:10..., 59:122...], disallowmissing=true) y = completedata[!,:transProfit] inc = .!isnan.(y) y = y[inc] X = completedata[inc,[6:7..., 59:121...]] cols = [std(X[!,c])>0 for c in 1:ncol(X)] X = X[:,cols] est = R\"rlassoEffects($(X), $(y), index=c(1:2))\" R\"summary($est)\" Error: LoadError: UndefVarError: @R_str not defined in expression starting at /home/paul/.julia/dev/NeuralNetworkEconomics/docs/jmd/ml-julia.jmd:2","title":"RCall"},{"location":"ml-julia/#mljjl","text":"MLJ.jl is a machine learning framework for Julia. It gives a unified interface for many machine learning algorithms and tasks. Similar R packages include caret and MLR . scikit-learn is a similar Python package. For more information on MLJ see MLJ.jl docs MLJ tutorials You can see a list of models registered to work with MLJ.jl on github , or by calling MLJ::models() . using MLJ models() 200-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstr act_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hy perparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_ methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteratio n_parameter, :load_path, :package_license, :package_url, :package_uuid, :pr edict_scitype, :prediction_type, :reporting_operations, :reports_feature_im portances, :supports_class_weights, :supports_online, :supports_training_lo sses, :supports_weights, :transform_scitype, :input_scitype, :target_scityp e, :output_scitype)}}: (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... ) (name = ABODDetector, package_name = OutlierDetectionPython, ... ) (name = AEDetector, package_name = OutlierDetectionNetworks, ... ) (name = ARDRegressor, package_name = ScikitLearn, ... ) (name = AdaBoostClassifier, package_name = ScikitLearn, ... ) (name = AdaBoostRegressor, package_name = ScikitLearn, ... ) (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... ) (name = AffinityPropagation, package_name = ScikitLearn, ... ) (name = AgglomerativeClustering, package_name = ScikitLearn, ... ) (name = BM25Transformer, package_name = MLJText, ... ) \u22ee (name = TheilSenRegressor, package_name = ScikitLearn, ... ) (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... ) (name = UnivariateDiscretizer, package_name = MLJModels, ... ) (name = UnivariateFillImputer, package_name = MLJModels, ... ) (name = UnivariateStandardizer, package_name = MLJModels, ... ) (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... ) (name = XGBoostClassifier, package_name = XGBoost, ... ) (name = XGBoostCount, package_name = XGBoost, ... ) (name = XGBoostRegressor, package_name = XGBoost, ... ) To use these models, you need the corresponding package to be installed and loaded. The @load macro will load the needed package(s) for any model. Lasso = @load LassoRegressor pkg=MLJLinearModels import MLJLinearModels \u2714 MLJLinearModels.LassoRegressor Let\u2019s fit lasso to the same pipeline data as above. lasso = machine(Lasso(lambda=1.0), X, y) train,test = partition(eachindex(y), 0.6, shuffle=true) fit!(lasso, rows=train) yhat = predict(lasso, rows=test) println(yhat[1:10]) println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") Error: UndefVarError: X not defined That doesn\u2019t look very good. All the predictions are zero. This could happen when the regularization parameter, lambda , is too large. However, in this case the problem is something else. The warning messages indicate numeric problems when minimizing the lasso objective function. This can happen when X is poorly scaled. The algorithm used to compute the lasso estimates works best when the coefficients are all roughly the same scale. The existing X \u2019s have wildly different scales, which causes problems. This situation is common, so MLJ.jl has functions to standardize variables. It is likely that the hdm package in R does something similar internally. lasso_stdx = Pipeline(Standardizer(), Lasso(lambda=1.0*std(y[train]), solver=MLJLinearModels.ISTA(max_iter=10000)) ) m = machine(lasso_stdx, X, y) fit!(m, rows=train, force=true) yhat = predict(m , rows=test) println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") # Get the fitted coefficients coef = fitted_params(m).lasso_regressor.coefs intercept = fitted_params(m).lasso_regressor.intercept sum(abs(c[2])>1e-8 for c in coef) # number non-zero Error: UndefVarError: y not defined If we want to tune lambda using cross-validation, we can use the range and TunedModel functions. r = range(lasso_stdx, :(lasso_regressor.lambda), lower=1e1, upper=1e10, scale=:log) t=TunedModel(model=lasso_stdx, resampling=CV(nfolds=5), tuning=Grid(resolution=10), ranges=r, measure=rms) m = machine(t, X, y) fit!(m, rows=train, verbosity=1) yhat = predict(m , rows=test) println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") Error: UndefVarError: lasso_stdx not defined using Plots cvmse = m.report.plotting.measurements \u03bb = Float64.(m.report.plotting.parameter_values[:]) s = sortperm(\u03bb) plot(\u03bb[s], cvmse[s], xlab=\"\u03bb\", ylab=\"CV(MSE)\") Error: UndefVarError: m not defined","title":"MLJ.jl"},{"location":"ml-julia/#fluxjl","text":"Flux.jl is another Julia package for machine learning. It seems to be emerging as the leading Julia package for neural networks and deep learning, but other machine learning models can also be implemented using Flux.jl . Let\u2019s create a lasso model in Flux.jl . using Flux, LinearAlgebra # Scale the variables Xstd = Flux.normalise(Matrix(X), dims=1) X_train = Xstd[train,:] X_test = Xstd[test,:] yscale = std(y[train]) ymean = mean(y[train]) ystd = (y .- ymean)./yscale y_train = ystd[train] y_test = ystd[test] # Set up the model parameters and initial values \u03b2ols = (X_train'*X_train) \\ (X_train'*(y_train .- mean(y_train))) \u03b2 = zeros(ncol(X)) b = [mean(y_train)] # Define the loss function \u03c8 = ones(length(\u03b2)) \u03bb = 2.0 pred(x) = b .+ x*\u03b2 mse(x,y) = mean( (pred(x) .- y).^2 ) penalty(y) = \u03bb/length(y)*norm(\u03c8.*\u03b2,1) loss(x,y) = mse(x,y) + penalty(y) @show loss(X_train,y_train) # minimize loss maxiter=2000 obj = zeros(maxiter) mse_train = zeros(maxiter) mse_test = zeros(maxiter) opt = Flux.AMSGrad() for i in 1:maxiter Flux.train!(loss, Flux.params(\u03b2, b), [(X_train, y_train)], opt) mse_train[i] = mse(X_train,y_train) mse_test[i] = mse(X_test, y_test) obj[i] = loss(X_train,y_train) end lo = 1 hi = 2000 plot(obj[lo:hi], ylab=\"Loss=MSE + \u03bb/n*||\u03b2||\u2081\", xlab=\"Iteration\") Error: UndefVarError: X not defined plot(lo:hi, [mse_train[lo:hi] mse_test[lo:hi]], ylab=\"MSE\", xaxis=(\"Iteration\"), lab=[\"Train\" \"Test\"]) Error: UndefVarError: lo not defined The minimization methods in Flux.train! are all variants of gradient descent. Each call to Flux.train! runs one iteration of the specified solver. To find a local minimum, Flux.train! can be called repeatedly until progress stops. The above loop is a simple way to do this. The @epoch macro can also be useful. Gradient descent works well for neural networks, but is not ideal for Lasso. Without further adjustment, gradient descent gets stuck in a cycle as jumps from one side of the other of the absolute value in the lasso penalty. Nonetheless, the results are near the true minimum, even though it never exactly gets there.","title":"Flux.jl"},{"location":"ml-julia/#luxjl","text":"A promising alternative to Flux.jl is Lux.jl . 1 Lux.jl and Flux.jl share many features and backend code. Lux.jl has a more function focused interface with explicit parameter passing. This is a more \u201cJulian\u201d style of programming. 2 For comparison, let\u2019s implement the same Lasso model in Lux. import Lux # Lux shares many function names with Flux, so import instead of using to avoid confusion import Random, Zygote using Test # Seeding rng = Random.default_rng() Random.seed!(rng, 0) # define the model X_train = Matrix(X)[train,:] X_test = Matrix(X)[test,:] y_train = y[train] y_test = y[test] function standardizer(xtrain) m = std(xtrain, dims=1) s = std(xtrain, dims=1) (x->(x .- m)./s , xs->xs.*s .+ m) end stdizex, _ = standardizer(X_train) stdizey, unstdizey = standardizer(y_train) @test unstdizey(stdizey(y_test)) \u2248 y_test ys = stdizey(y_train) Xs = stdizex(X_train) # Set up the model parameters and initial values \u03b2ols = (Xs'*Xs) \\ (Xs'*(ys .- mean(ys))) b = [mean(ys)] m = Lux.Chain(X->stdizex(X)', Lux.Dense(size(X_train,2), 1, init_weight=zeros, init_bias=zeros) ) ps, st = Lux.setup(rng, m) ps.layer_2.weight .= \u03b2ols' ps.layer_2.bias .= b mse(m, ps, st, X, y) = mean(abs2, m(X, ps, st)[1]' .- stdizey(y)) mseraw(m,ps,st,X,y) = mean(abs2, unstdizey(m(X, ps, st)[1]') .- y) \u2113 = let \u03bb = 2.0, \u03c8 = ones(size(\u03b2ols')), st=st penalty(ps,y) = \u03bb/length(y)*norm(\u03c8.*ps.layer_2.weight,1) loss(ps, X, y, m) = mse(m, ps, st, X, y) + penalty(ps,y) end @show \u2113(ps,X_train,y_train,m) # minimize loss opt = Lux.Optimisers.AMSGrad() #ADAM(0.001) optstate = Lux.Optimisers.setup(opt, ps) maxiter=2000 obj = zeros(maxiter) mse_train = zeros(maxiter) mse_test = zeros(maxiter) for i in 1:maxiter # Compute the gradient gs = Zygote.gradient(ps->\u2113(ps, X_train, y_train, m), ps)[1] # Perform parameter update optstate, ps = Lux.Optimisers.update(optstate, ps, gs) mse_train[i] = mse(m, ps, st, X_train,y_train) mse_test[i] = mse(m, ps, st, X_test, y_test) obj[i] = \u2113(ps,X_train,y_train,m) end lo = 1 hi = 250 plot(lo:hi,obj[lo:hi], ylab=\"Loss=MSE + \u03bb/n*||\u03b2||\u2081\", xlab=\"Iteration\") Error: UndefVarError: X not defined","title":"Lux.jl"},{"location":"ml-julia/#additional-resources","text":"@klok2019 Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence","title":"Additional Resources"},{"location":"ml-julia/#references","text":"These notes were originally written before Lux.jl existed. If I were starting over, I would use Lux.jl instead of Flux.jl. \u21a9 Flux.jl drew inspiration for its interface from Tensorflow and PyTorch. Implicit parameters makes some sense in an object oriented language like Python, but it is not the most natural style for Julia. \u21a9","title":"References"},{"location":"ml-methods/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction to machine learning \u00b6 Friedman, Hastie, and Tibshirani ( 2009 ) and James et al. ( 2013 ) are commonly recommended textbooks on machine learning. James et al. ( 2013 ) is less technical of the two, but neither book is especially difficult. Efron and Hastie ( 2016 ) covers similar material and is slightly more advanced. Some prediction examples \u00b6 Machine learning is tailored for prediction, let\u2019s look at some data and see how well it works Predicting house prices \u00b6 Example from Mullainathan and Spiess ( 2017 ) Training on 10000 observations from AHS Predict log house price using 150 variables Holdout sample of 41808 AHS variables [ahs-variables] \u00b6 ahs <- readRDS(\"ahs2011forjep.rdata\")$df print(summary(ahs[,1:20])) ## LOGVALUE REGION METRO METRO3 PHONE KITCHEN ## Min. : 0.00 1: 5773 1:10499 1:11928 -7: 1851 1:51513 ## 1st Qu.:11.56 2:13503 2: 1124 2:39037 1 :49353 2: 295 ## Median :12.10 3:15408 3: 202 9: 843 2 : 604 ## Mean :12.06 4:17124 4: 103 ## 3rd Qu.:12.61 7:39880 ## Max. :15.48 ## MOBILTYP WINTEROVEN WINTERKESP WINTERELSP WINTERWOOD WINTERNONE ## -1:49868 -8: 133 -8: 133 -8: 133 -8: 133 -8: 133 ## 1 : 927 -7: 50 -7: 50 -7: 50 -7: 50 -7: 50 ## 2 : 1013 1 : 446 1 : 813 1 : 8689 1 : 61 1 :41895 ## 2 :51179 2 :50812 2 :42936 2 :51564 2 : 9730 ## ## ## NEWC DISH WASH DRY NUNIT2 BURNER COOK ## -9:50485 1:42221 1:50456 1:49880 1:44922 -6:51567 1:51567 ## 1 : 1323 2: 9587 2: 1352 2: 1928 2: 2634 1 : 87 2: 241 ## 3: 2307 2 : 154 ## 4: 1945 ## ## ## OVEN ## -6:51654 ## 1 : 127 ## 2 : 27 ## ## ## library(GGally) ggpairs(ahs[,c(\"LOGVALUE\",\"ROOMS\", \"LOT\",\"UNITSF\",\"BUILT\")], lower=list(continuous=\"points\", combo=\"facethist\", discrete=\"facetbar\"), diag=list(continuous=\"barDiag\",discrete=\"barDiag\")) + theme_minimal() # use ms-reproduce.R from course git repo to download and run Mullainathon & Spiess data and code to # create jepfittedmodels-table1.csv. Be aware that this will take many hours. tbl <- read.csv(\"jepfittedmodels-table1.csv\") tab <- tbl[,3:ncol(tbl)] rownames(tab) <- tbl[,2] tab <- tab[1:5, c(1,2,3,5)] colnames(tab) <- c(\"in sample MSE\", \"in sample R^2\", \"out of sample MSE\", \"out of sample R^2\") library(kableExtra) kable_styling(kable(tab, caption=\"Performance of different algorithms in predicting housing values\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Performance of different algorithms in predicting housing values in sample MSE in sample R\\^2 out of sample MSE out of sample R\\^2 OLS 0.589 0.473 0.674 0.417 Tree 0.675 0.396 0.758 0.345 Lasso 0.603 0.460 0.656 0.433 Forest 0.166 0.851 0.632 0.454 Ensemble 0.216 0.807 0.625 0.460 library(ggplot2) load(file=\"jeperrorfig.RData\") print(fig) load(file=\"jeperrorfig.RData\") print(fig2) Predicting pipeline revenues \u00b6 Data on US natural gas pipelines Combination of FERC Form 2, EIA Form 176, and other sources, compiled by me 1996-2016, 236 pipeline companies, 1219 company-year observations Predict: $y =$ profits from transmission of natural gas Covariates: year, capital, discovered gas reserves, well head gas price, city gate gas price, heating degree days, state(s) that each pipeline operates in load(\"pipelines.Rdata\") # data has problems before 1996 due to format change data <- subset(data,report_yr>=1996) # replace NA state weights with 0's data[,59:107][is.na(data[,59:107])] <- 0 # spaces in variable names will create problems later names(data) <- gsub(\" \",\".\",names(data)) summary(data[,c(\"transProfit\",\"transPlant_bal_beg_yr\",\"cityPrice\",\"wellPrice\")]) ## transProfit transPlant_bal_beg_yr cityPrice ## Min. : -31622547 Min. :0.000e+00 Min. : 0.4068 ## 1st Qu.: 2586031 1st Qu.:2.404e+07 1st Qu.: 3.8666 ## Median : 23733170 Median :1.957e+08 Median : 5.1297 ## Mean : 93517513 Mean :7.772e+08 Mean : 5.3469 ## 3rd Qu.: 129629013 3rd Qu.:1.016e+09 3rd Qu.: 6.5600 ## Max. :1165050214 Max. :1.439e+10 Max. :12.4646 ## NA's :2817 NA's :2692 NA's :1340 ## wellPrice ## Min. :0.0008 ## 1st Qu.:2.1230 ## Median :3.4370 ## Mean :3.7856 ## 3rd Qu.:5.1795 ## Max. :9.6500 ## NA's :2637 library(GGally) ggpairs(data[,c(\"transProfit\",\"transPlant_bal_beg_yr\",\"cityPrice\",\"wellPrice\")], lower=list(continuous=\"smooth\")) + theme_minimal() Predicting pipeline revenues : methods \u00b6 OLS : 67 covariates (year dummies and state(s) create a lot) Lasso Random forests Randomly choose 75% of sample to fit the models, then look at prediction accuracy in remaining 25% We are focusing on Lasso and random forests because these are the two methods that econometricians have worked on the most. Other methods such as neural nets and support vector machines are also worth exploring. For now, you can think of Lasso and random forests these as black boxes that generate predictions from data. We will go into more detail soon. ## Create X matrix for OLS and random forests xnames <-c(\"transPlant_bal_beg_yr\", \"reserve\", \"wellPrice\", \"cityPrice\", \"plantArea\", \"heatDegDays\", names(data)[59:107] ) yname <- \"transProfit\" fmla <- paste(yname,\"~\",paste(xnames,collapse=\" + \"),\"+ as.factor(report_yr)\") ols <- lm(fmla,data=data,x=TRUE,y=TRUE) X <- ols$x[,!(colnames(ols$x) %in% c(\"(Intercept)\")) & !is.na(ols$coefficients)] y <- ols$y train <- runif(nrow(X))<0.75 # OLS prediction on training set y.t <- y[train] X.t <- X[train,] ols <- lm(y.t ~ X.t) y.hat.ols <- ols$coefficients[1] + X %*% ols$coefficients[2:(length(ols$coef))] df <- data.frame(y=y, y.hat=y.hat.ols, train=train, method=\"ols\") ## Lasso library(glmnet) # Create larger X matrix for lasso fmla.l <- paste(yname,\"~ (\", paste(xnames,collapse=\" + \"),\")*(report_yr + transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays) + \", paste(sprintf(\"I(%s^2)\",xnames[1:6],collapse=\" + \")) ) reg <- lm(fmla.l, data=data, x=TRUE,y=TRUE) Xl <- reg$x[,!(colnames(reg$x) %in% c(\"(Intercept)\")) & !is.na(reg$coefficients)] lasso <- cv.glmnet(Xl[train,],y[train],alpha=1,parallel=FALSE, standardize=TRUE, intercept=TRUE, nfolds = 50) y.hat.lasso <- predict(lasso, Xl, s=lasso$lambda.min, type=\"response\") df <- rbind(df, data.frame(y=y, y.hat=as.vector(y.hat.lasso), train=train, method=\"lasso\")) ## Random forest library(grf) rf <- regression_forest(X[train,],y[train],tune.parameters = TRUE) y.hat.rf <- predict(rf, X)$predictions df <- rbind(df, data.frame(y=y, y.hat=y.hat.rf, train=train, method=\"random forest\")) # Neural network library(RSNNS) n <- nrow(X[train,]) p <- ncol(X) rn <- floor(n^(1/(2*(1+1/(1+p))))/2) xn <- normalizeData(X) yn <- normalizeData(y) nn <- mlp(x=xn[train,], y=yn[train], linOut=TRUE, size=rn) yn.hat.nn <- predict(nn, xn) y.hat.nn <- denormalizeData(yn.hat.nn, getNormParameters(yn)) df <- rbind(df, data.frame(y=y, y.hat=y.hat.nn, train=train, method=\"neural network\")) ggplot(data=df,aes(x=y,y=y.hat,colour=method,shape=train)) + geom_point(alpha=0.5) + geom_line(aes(y=y)) + theme_minimal() df$trainf <- factor(df$train, levels=c(\"TRUE\", \"FALSE\")) df$error <- df$y - df$y.hat ggplot(data=df,aes(x=error,colour=method)) + geom_density() + theme_minimal() + xlim(quantile(df$error,c(0.01,0.99))) + facet_grid(trainf ~ .,labeller=label_both) library(kableExtra) fn <- function(df) with(df,c(mean((y.hat - y)^2)/var(y), mean(abs(y.hat - y))/mean(abs(y-mean(y))))) tab1 <-unlist(by(subset(df,train), df$method[train], FUN=fn)) tab1 <- (matrix(tab1,nrow=2)) rownames(tab1) <- c(\"relative MSE\",\"relative MAE\") colnames(tab1) <- c(\"OLS\",\"Lasso\",\"Random forest\",\"Neural Network\") tab2 <- unlist(by(subset(df,!train), df$method[!train], FUN=fn)) tab2 <- (matrix(tab2,nrow=2)) rownames(tab2) <- c(\"relative MSE\",\"relative MAE\") colnames(tab2) <- c(\"OLS\",\"Lasso\",\"Random forest\",\"Neural Network\") kable_styling(kable(tab1, caption=\"Training sample\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=F) Training sample OLS Lasso Random forest Neural Network relative MSE 0.110 0.031 0.063 0.012 relative MAE 0.265 0.137 0.179 0.111 kable_styling(kable(tab2, caption=\"Hold-out sample\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=F) Hold-out sample OLS Lasso Random forest Neural Network relative MSE 0.125 0.064 0.093 0.092 relative MAE 0.288 0.199 0.232 0.278 In this table, relative MSE is the mean squared error relative to the variance of $y$, that is \\text{relative MSE} = \\frac{\\En[(y_i - \\hat{y}_i)^2]} {\\En[ (y_i - \\bar{y})^2]}. It is equal to $1-R^2$. Similarly, relative MAE is \\text{relative MAE} = \\frac{\\En[|y_i - \\hat{y}_i|]} {\\En[|y_i - \\bar{y}|]}. $\\En$ denotes the empirical expectation, $\\En[y_i] = \\frac{1}{n}\\sum_{i=1}^n y_i$. Lasso \u00b6 Lasso solves a penalized (regularized) regression problem \\hat{\\beta} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] + \\frac{\\lambda}{n} \\norm{ \\hat{\\Psi} \\beta}_1 Penalty parameter $\\lambda$ Diagonal matrix $\\hat{\\Psi} = diag(\\hat{\\psi})$ Dimension of $x_i$ is $p$ and implicitly depends on $n$ can have $p >> n$ We are following the notation used in Chernozhukov, Hansen, and Spindler ( 2016 ). Note that this vignette has been updated since it was published in the R Journal. To obtain the most recent version, install the hdm package in R, load it, and then open the vignette. install.packages(\"hdm\") library(hdm) vignette(\"hdm_introduction\") The choice of penalty (or regularization) parameter, $\\lambda$, is important. When $\\lambda = 0$, Lasso is the same as OLS. As $\\lambda$ increases, the Lasso estimates will shrink toward 0. For large enough $\\lambda$, some components of $\\hat{\\beta}$ become exactly 0. As $\\lambda$ increases more, more and more components of $\\hat{\\beta}$ will be exactly $0$. For some intuition about why Lasso results in some coefficients being zero, note that \\hat{\\beta}^{lasso} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] + \\frac{\\lambda}{n} \\norm{\\beta}_1 is equivalent to \\hat{\\beta}^{lasso} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] \\text{ s.t. } \\norm{\\beta}_1 \\leq s for some $s$. In this problem, the boundary of the constraint set will be a diamond. The level sets of the objective will be elipses. Generically, the solution will lie on one of the corners of the $\\norm{\\beta}_1 = 1$ set. See Friedman, Hastie, and Tibshirani ( 2009 ) or James et al. ( 2013 ) for more details. Most machine learning methods involve some form of regularization with an associated regularization parameter. In choosing the regularization parameter, we face a bias-variance tradeoff. As $\\lambda$ increases, variance decreases, but bias increases. Machine learning algorithms typically choose regularization parameters through cross-validation. Although cross-validation leads to good predictive performance, the statistical properties are not always known. Chernozhukov, Hansen, and Spindler ( 2016 ) say, \u201cIn high dimensional settings cross-validation is very popular; but it lacks a theoretical justification for use in the present context.\u201d However, there has been some recent progress on convergence rates for Lasso with cross-validation, see Chetverikov, Liao, and Chernozhukov ( 2016 ). The diagonal matrix $\\hat{\\Psi}$ is used to make the estimator invariant to scaling of $x_i$, and to allow for heteroskedasticity. If reading about Lasso or using code from other authors, be careful some do not include $\\hat{\\Psi}$ and use $\\lambda$ instead of $\\frac{\\lambda}{n}$. load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) library(glmnet) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) # standardize everything so that coefficients are similar scale when plotted mod$y <- (mod$y - mean(mod$y))/sd(mod$y) for(c in 2:ncol(mod$x)) { mod$x[,c] <- (mod$x[,c] - mean(mod$x[,c]))/sd(mod$x[,c]) } lassoPath <- glmnet(mod$x, mod$y, alpha=1) plot(lassoPath, xvar=\"lambda\", label=TRUE) load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) library(glmnet) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) # standardize everything so that coefficients are similar scale when plotted mod$y <- (mod$y - mean(mod$y))/sd(mod$y) for(c in 2:ncol(mod$x)) { mod$x[,c] <- (mod$x[,c] - mean(mod$x[,c]))/sd(mod$x[,c]) } cv <- cv.glmnet(mod$x, mod$y, alpha=1) plot(cv) Statistical properties of Lasso \u00b6 Model : y_i = x_i'\\beta_0 + \\epsilon_i $\\Er[x_i \\epsilon_i] = 0$ $\\beta_0 \\in \\R^n$ $p$, $\\beta_0$, $x_i$, and $s$ implicitly depend on $n$ $\\log p = o(n^{1/3})$ $p$ may increase with $n$ and can have $p>n$ Sparsity $s$ Exact : $\\norm{\\beta_0}_0 = s = o(n)$ Approximate : $|\\beta_{0,j}| < Aj^{-a}$, $a > 1/2$, $s \\propto n^{1/(2a)}$ $\\norm{\\beta}_0$ is the number of non-zero components of $\\beta$. The approximate sparsity setting means if $|\\beta_{0,j}| < Aj^{-a}$, then, there exists a sparse approximation, say $\\beta_{a}$, with $s$ nonzero elements, such that the approximation error, \\En[(x_i'(\\beta_a - \\beta_0))^2] = c_s^2 will vanish quickly if $s \\propto n^{1/2a}$. Just how quickly will it vanish? An easy upper bound is \\begin{align*} c_s^2 \\leq & \\En\\left[\\left( \\sum_{j={s+1}}^p x_ij \\beta_{0,j} \\right)^2 \\right] \\\\ \\leq & \\En\\left[ \\left(\\sum_{j={s+1}}^p x_ij A j^{-a} \\right)^2 \\right] \\end{align*} To simplify the alegebra, let\u2019s assume $\\En[x_i x_i\u2019] = I_p$, then \\begin{align*} c_s^2 \\leq & \\sum_{j={s+1}}^p A^2 j^{-2a} \\\\ & \\leq \\sum_{j={s+1}}^\\infty A^2 j^{-2a} = A^2 s^{-2a} \\zeta(2a) \\\\ c_s^2 \\lesssim s^{-2a} \\end{align*} where $\\zeta(2a) = \\sum_{j=1}^\\infty j^{-2a}$ is the Riemann Zeta function (all that matters here is that $\\zeta(2a)$ is finite for $2a>1$). Then if $s \\propto n^{(1+\\delta)/2a}$, we would get $c_s \\lesssim n^{-(1+\\delta)/2}$. Importantly, $\\sqrt{n} c_s = o(1)$, so in the sort of expansions that we would do to show that $\\hat{\\theta}$ is $\\sqrt{n}$ asymptotically normal, the bias term would vanish. Rate of convergence \u00b6 With $\\lambda = 2c \\sqrt{n} \\Phi^{-1}(1-\\gamma/(2p))$ \\sqrt{\\En[(x_i'(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\sqrt{ (s/n) \\log (p) }, \\norm{\\hat{\\beta}^{lasso} - \\beta_0}_2 \\lesssim_P \\sqrt{ (s/n) \\log (p) }, and \\norm{\\hat{\\beta}^{lasso} - \\beta_0}_1 \\lesssim_P \\sqrt{ (s^2/n) \\log (p) } Constant $c>1$ Small $\\gamma \\to 0$ with $n$, and $\\log(1/\\gamma) \\lesssim \\log(p)$ Rank like condition on $x_i$ near-oracle rate In the semiparametric estimation problems that we\u2019re focused on, our object of interest is some finite dimensional parameter $\\theta$ that depends on our data some high dimensional parameter, like $\\beta_0$ in the Lasso. To analyze estimates of $\\hat{\\theta}(data, \\hat{\\beta})$, a key step will be to show that we can replace $\\hat{\\beta}$ with $\\beta_0$. The rate of convergence of $\\hat{\\beta}$ will be important for making this possible. Thus, the main thing that we care about for the Lasso and other machine learning estimators will be their rates of converagence. The notation $A_n \\lesssim_P B_n$ is read $A_n$ is bounded in probability by $B_n$ and means that for any $\\epsilon>0$, there exists $M$, $N$ such that $\\Pr(|A_n/B_n| > M) < \\epsilon$ for all $n > N$. This is also often denoted by $A_n = O(B_n)$. These rate results are from Belloni et al. ( 2012 ). Since this setup allows $p>n$, $x$ cannot be assumed to have full rank. Instead, an assumption about the eigenvalues of $X\u2019X$ restricted to the nonzero components of $\\beta_0$, plays a similar. See Belloni et al. ( 2012 ) for details. This convergence rate is called the near-oracle rate, because it is nearly as good as what we get if an oracle told us which components of $\\beta_0$ are nonzero. In that case OLS using just those $s$ components gives the fastest possible rate, which is \\sqrt{\\En[(x_i'(\\hat{\\beta}^{OLS} - \\beta_0))^2]} \\propto \\sqrt{s/n}. Rate of convergence [rate-of-convergence-1] \u00b6 Using cross-validation to choose $\\lambda$ known bounds are worse With Gaussian errors: $\\sqrt{\\En[(x_i\u2019(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\sqrt{ (s/n) \\log (p) } \\log(pn)^{7/8}$, Without Gaussian error $\\sqrt{\\En[(x_i\u2019(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\left( \\frac{s \\log(pn)^2}{n} \\right)^{1/4}$ Chetverikov, Liao, and Chernozhukov ( 2016 ) These results are for an exactly sparse setting. Do they hold under approximate sparsity? Other statistical properties \u00b6 Inference on $\\beta$: not the goal in our motivating examples Difficult, but some recent results See Lee et al. ( 2016 ), Taylor and Tibshirani ( 2017 ), Caner and Kock ( 2018 ) Model selection: not the goal in our motivating examples Under stronger conditions, Lasso correctly selects the nonzero components of $\\beta_0$ See Belloni and Chernozhukov ( 2011 ) In the statistics literature on high dimensional and nonparametric estimation, you will come across the terms \u201cadaptive\u201d and \u201chonest.\u201d Adaptivity of an estimator refers to the situation where the rate of convergence depends on some unknown parameter. In the case of Lasso, the sparsity index of the true model, $s$, is an unknown parameter affecting the rate of convergence. Without knowing or estimating $s$, Lasso attains the above rate of convergence for a wide range of admissable $s$. Thus, Lasso is adaptive to the unknown sparsity index. \u201cHonest\u201d is a property of an inference method. A confidence region is honest if it has correct coverage for a large class of true models. For the Lasso, an honest confidence region would be valid for a wide range of sparsity, $s$. An honest, adaptive confidence region would be one that is valide for a wide range of $s$ and whose size shrinks as quickly as if $s$ were known. Achieving both adaptivity and honesty is impossible in the most general setting. For example, although an $\\ell$ times differentiable function of a $p$ dimensional variable can be adaptively estimated at rate $n^{-\\ell}{2\\ell + p}$, Li ( 1989 ) showed that an honest confidence region can contract at most at rate $n^{-1/4}$ (not adaptive to $\\ell$). However, an adaptive confidence region can be constructed if further restrictions are placed on the set of possible models, see Nickl and Geer ( 2013 ) for such a result for Lasso.. Post-Lasso \u00b6 Two steps : Estimate $\\hat{\\beta}^{lasso}$ ${\\hat{\\beta}}^{post} =$ OLS regression of $y$ on components of $x$ with nonzero $\\hat{\\beta}^{lasso}$ Same rates of convergence as Lasso Under some conditions post-Lasso has lower bias If Lasso selects correct model, post-Lasso converges at the oracle rate Post-Lasso removes some of the regularizaton bias of Lasso. The rate of convergence of post-Lasso is always as fast as Lasso, and under conditions that allow perfect model selection, post-Lasso converges slightly faster (by a factor $\\log(p)$). See Belloni et al. ( 2012 ) for details. Random forests \u00b6 Regression trees \u00b6 $y_i \\in R$ on $x_i \\in \\R^p$ Want to estimate $\\Er[y | x]$ Locally constant estimate \\hat{t}(x) = \\sum_m^M c_m 1\\{x \\in R_m \\} Rectangular regions $R_m$ determined by tree Simulated data \u00b6 n <- 1000 x <- runif(n) y <- runif(n) f <- function(x,z) { 1/3*(sin(5*x)*sqrt(z)*exp(-(z-0.5)^2)) } f0 <- f(x,y) z <- f0 + rnorm(n)*0.1 tree.df <- data.frame(x=x,y=y,z=z) # plot true function and data x.g <- seq(0,1,length.out=100) y.g <- seq(0,1,length.out=100) f0.g <- t(outer(x.g,y.g,f)) library(plotly) fig <- plot_ly( colors=\"YlOrBr\") fig <- add_markers(fig,x=x,y=y,z=z, size=0.3, opacity=0.2) fig <- add_surface(fig, x=x.g, y=y.g, z=f0.g, opacity=1) fig Estimated tree \u00b6 # fit regression tree library(party) tree <- ctree(z ~ x + y, data=tree.df) # plot estimate x.g <- seq(0,1,length.out=100) y.g <- seq(0,1,length.out=100) df <- expand.grid(x.g,y.g) names(df) <- c(\"x\",\"y\") fhat.g <- matrix(predict(tree, newdata=df),nrow=length(x.g), byrow=TRUE) library(plotly) fig <- plot_ly(colors=\"YlOrBr\") fig <- add_markers(fig,x=x,y=y,z=z, size=0.3, opacity=0.2) fig <- add_surface(fig, x=x.g, y=y.g, z=fhat.g, opacity=1) fig Estimated tree \u00b6 plot(tree) Tree algorithm \u00b6 For each region, solve \\min_{j,s} \\left[ \\min_{c_1} \\sum_{i: x_{i,j} \\leq s, x_i \\in R} (y_i - c_1)^2 + \\min_{c_2} \\sum_{i: x_{i,j} > s, x_i \\in R} (y_i - c_2)^2 \\right] Repeat with $R = {x:x_{i,j} \\leq s^ } \\cap R$ and $R = {x:x_{i,j} \\leq s^ } \\cap R$ Stop when $|R| =$ some chosen minimum size Prune tree \\min_{tree \\subset T} \\sum (\\hat{f}(x)-y)^2 + \\alpha|\\text{terminal nodes in tree}| There are many variations on this tree building algorithm. They all share some rule to decide on which variable and where to split. They all have some kind of stopping rule, but not necessarily the same one. For example, some algorithms stop splitting into new branches when the improvement in $R^2$ becomes small. These trees don\u2019t need subsequent pruning, but also may fail to find later splits that might be important. As with lasso, regression trees involve some regularization. In the above description, both the minimum leaf size and $\\alpha$ in the pruning step serve as regularization parameters. A potential advantage of regression trees is that their output might be interpretable, especially if there are not many branches. Some disadvantages are that they often are not very good predictors, and small perturbations in data can lead to seemingly large changes in the tree. Random forests \u00b6 Average randomized regression trees Trees randomized by Bootstrap or subsampling Randomize branches: \\min_{j \\in S,s} \\left[ \\min_{c_1} \\sum_{i: x_{i,j} \\leq s, x_i \\in R} (y_i - c_1)^2 + \\min_{c_2} \\sum_{i: x_{i,j} > s, x_i \\in R} (y_i - c_2)^2 \\right] where $S$ is random subset of ${1, \u2026, p}$ Variance reduction Rate of convergence: regression tree \u00b6 $x \\in [0,1]^p$, $\\Er[y|x]$ Lipschitz in $x$ Crude calculation for single tree, let denote $R_i$ node that contains $x_i$ \\begin{align*} \\Er(\\hat{t}(x_i) - \\Er[y|x_i])^2 = & \\overbrace{\\Er(\\hat{t}(x_i) - \\Er[y|x\\in R_i])^2}^{variance} + \\overbrace{(\\Er[y|x \\in R_i] - \\Er[y|x])^2}^{bias^2} \\\\ = & O_p(1/m) + O\\left(L^2 \\left(\\frac{m}{n}\\right)^{2/p}\\right) \\end{align*} optimal $m = O(n^{2/(2+p)})$ gives \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2] = O_p(n^{\\frac{-2}{2+p}}) By a crude calculation, I mean lets treat the tree as fixed. The the variance term is simply from estimating a conditional mean. This analysis could be made more rigorous by assuming the tree was estimated by sample splitting \u2014 use half the data to construct the tree and the remaining half to estimate the mean of $y$ in each node. Athey and Wager, and others, refer to such trees as \u201chonest.\u201d I suppose that this is because sample splitting facilitates honest inference afterward. The order of the bias term comes from considering the width of the pieces of a $p$ dimensional cube split evenly into $n/m$ pieces. Remember that for our motivating semiparametric problems, we need $\\sqrt{n} \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2]$ to vanish. The above rate convergence is too slow for $p>2$. The calculation of the above was admittedly crude, and may not be exact. However, Stone ( 1982 ) showed that if $\\Er[y|x]$ is $\\ell$ times differentiable, the fastest possible rate of convergence for any estimator is $n^{\\frac{-\\ell}{2\\ell + p}}$. To have any hope of a fast enough rate, we need to assume the function we\u2019re estimating is very smooth (high $\\ell$), or place some other restriction on the class of functions we allow (like sparsity for the Lasso). Lipschitz continuity is slightly weaker than once differentiable on a compact set, so it should come as no surprise that the rate of convergence would be slow. Rate of convergence: random forest \u00b6 Result from Biau ( 2012 ) Assume $\\Er[y|x]=\\Er[y|x_{(s)}]$, $x_{(s)}$ subset of $s$ variables, then \\Er[(\\hat{r}(x_i) - \\Er[y|x_i])^2] = O_p\\left(\\frac{1}{m\\log(n/m)^{s/2p}}\\right) + O_p\\left(\\left(\\frac{m}{n}\\right)^{\\frac{0.75}{s\\log 2}} \\right) or with optimal $m$ \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2] = O_p(n^{\\frac{-0.75}{s\\log 2+0.75}}) This result from Biau ( 2012 ) assumes the forest is estimated with sample splitting. This avoids the difficult to analyze correlation between the nodes and $y$. Wager and Walther ( 2015 ) analyze what happens when the same data is used to construct the tree and average in each node. They get a slightly higher upper bound for the variance of $\\frac{\\log(p)\\log(n)}{m}$. Wager and Walther ( 2015 ) also allow $p$ to increase with $n$, whereas the previous analysis treated $p$ as fixed. These convergence rate results for random forests are not fast enough for our purpose. Does this mean that random forests should not be used in semiparametric estimation? Not necessarily. We\u2019re asking too much of random forests. There is no estimator for an arbitrary Lipschitz function that can have fast enough a rate of convergence. A restriction on the set of possible functions is needed to reduce the approximation bias. With Lasso, the assumption of (approximate) sparsity played that role. Chernozhukov et al. ( 2018 ) advise that random forests could be a good choice for semiparametric estimation when the function of interest is \u201cwell-approximated by a random forest.\u201d Unfortunately, there does not appear to be a clean mathematical way to describe the class of functions well-approximated by a forest. Other statistical properties \u00b6 Pointwise asymptotic normality : Wager and Athey ( 2018 ) Simulation study \u00b6 Partially linear model DGP : $x_i \\in \\R^p$ with $x_{ij} \\sim U(0,1)$ $d_i = m(x_i) + v_i$ $y_i = d_i\\theta + f(x_i) + \\epsilon_i$ $m()$, $f()$ either linear or step functions Estimate by OLS, Lasso, and random forest Lasso & random forest use orthogonal moments \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0 The point of this simulation is to see whether the slower convergence rate of random forests matters for the semiparametric problems we have in mind. Our theory results suggest that estimates of $\\theta$ using random forests with $p>2$ will be asymptotically biased. Specifically, the term d_n = \\En[(m(x_i) - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] will be $O_p(n^{\\frac{-2}{2+p}})$, so $\\sqrt{n} d_n = O_p(n^{\\frac{p-2}{2(2+p)}})$. However, this calculation is only an upper bound on $d_n$. For a given DGP, $d_n$ might be smaller. In this simulation exercise, when $m()$ and $f()$ are linear, they are not easy to approximate by a regression tree, so I expect the random forest estimator to behave relatively poorly. OLS and Lasso on the other hand will do very well, and are included mainly as benchmarks. When $m()$ and $f()$ are step functions (specifically, $f(x) = m(x) = \\sum_{j=1}^p 1(x_{j}>1/2)$), I thought they would be well approximated by a regression tree (and random forest). For OLS and Lasso, $x$ is still only included linearly in the estimation, so those estimators will do poorly in the step function DGP. Throughout the simulation $p$ is much less than $n$, so Lasso and OLS will generally give very similar results. rm(list=ls()) p <- 4 # number of x's mu.linear <- function(x) x%*%rep(1,p) m.linear <- function(x) x%*%rep(2,p) mu.step <- function(x) (x>0.5)%*%rep(1,p) m.step <- function(x) (x>0.5)%*%rep(2,p) theta <- 1 simulate <- function(n,p,mu,m) { theta <- 1 x <- matrix(runif(n*p), ncol=p) d <- m(x) + rnorm(n) y <- theta*d + mu(x) + rnorm(n) data.frame(y=y,d=d,x=x) } library(grf) library(hdm) df <- simulate(100,p,mu.linear,m.linear) mrfparams <- NULL murfparams <- NULL n.save <- NULL partial.linear.rf <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] if (is.null(mrfparams) || n.save!=nrow(df)) { # to save time, we only tune once per cluster worker and data set # size cat(\"tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=1000, tune.parameters=TRUE) mrfparams <<- m.rf$tunable.params mu.rf <- regression_forest(df[,x.names], df$y, num.trees=1000, tune.parameters=TRUE) n.save <<- nrow(df) murfparams <<- mu.rf$tunable.params } else { cat(\"not tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(mrfparams[\"min.node.size\"]), alpha = as.numeric(mrfparams[\"alpha\"]), imbalance.penalty=as.numeric(mrfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(mrfparams[\"sample.fraction\"]), mtry=as.numeric(mrfparams[\"mtry\"])) mu.rf <- regression_forest(df[,x.names], df$y, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(murfparams[\"min.node.size\"]), alpha = as.numeric(murfparams[\"alpha\"]), imbalance.penalty=as.numeric(murfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(murfparams[\"sample.fraction\"]), mtry=as.numeric(murfparams[\"mtry\"])) } vhat <- df$d - predict(m.rf)$predictions ehat <- df$y - predict(mu.rf)$predictions lm(ehat ~ vhat) } ## Manual sample splitting --- this turns out to be unneccessary. The ## default behavior of predict.regression_forest is to return ## predictions on the training data using only trees that were not fit ## on each observation. In other words, regression_forest already does ## the sample splitting for us. ## ## rf.tuneOnce <- function(x.names, y.name) { ## parms <- NULL ## function(df) { ## if (is.null(parms)) { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=500, ## tune.parameters=TRUE) ## parms <<- rf$tunable.params ## rf ## } else { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=200, ## tune.parameters=FALSE, ## honesty=FALSE, ## min.node.size = ## as.numeric(parms[\"min.node.size\"]), ## alpha = as.numeric(parms[\"alpha\"]), ## imbalance.penalty=as.numeric(parms[\"imbalance.penalty\"]), ## sample.fraction = as.numeric(parms[\"sample.fraction\"]), ## mtry=as.numeric(parms[\"mtry\"])) ## } ## } ## } ## n.save.split <- NULL ## m.hat.rf <- NULL ## mu.hat.rf <- NULL ## partial.linear.split.rf <- function(df , splits=3) { ## x.names <- names(df)[grep(\"x.\",names(df))] ## if (is.null(n.save.split) || n.save.split != nrow(df)) { ## n.save.split <<- nrow(df) ## m.hat.rf <<- rf.tuneOnce(x.names,\"d\") ## mu.hat.rf <<- rf.tuneOnce(x.names,\"y\") ## } ## df$group <- sample(1:splits, nrow(df), replace=TRUE) ## vhat <- df$d ## ehat <- df$y ## for(g in 1:splits) { ## sdf <- subset(df, group!=g) ## m <- m.hat.rf(sdf) ## mu <- mu.hat.rf(sdf) ## vhat[df$group==g] <- df$d[df$group==g] - ## predict(m, newx=df[df$group==g,x.names])$predictions ## ehat[df$group==g] <- df$y[df$group==g] - ## predict(mu, newx=df[df$group==g,x.names])$predictions ## } ## lm(ehat ~ vhat) ## } partial.linear.lasso <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) rlassoEffects(fmla, data=df, I = ~ d) } #summary(partial.linear.lasso(df)) # simulate a bunch of times in parallel simulations <- 500 # number of simulations library(parallel) cl <- makeCluster(detectCores()/2) # change as you see fit clusterEvalQ(cl,library(hdm)) clusterEvalQ(cl,library(grf)) # R Socket cluster spawns new R sessions with empty environments, we # need to make sure they load any needed libraries and have access to # things from the main environment that they use design <- c(\"linear\") #,\"step\") sim.df <- data.frame() start.time <- Sys.time() for (d in design) { if (d==\"linear\") { m <- m.linear mu <- mu.linear } else { m <- m.step mu <- mu.step } for (p in c(2,4,6,8)) { for (n in c(100, 200, 400, 800, 1600)) { clusterExport(cl,c(\"simulate\",\"partial.linear.lasso\", \"partial.linear.rf\",\"p\",\"mu\",\"m\", \"mrfparams\",\"murfparams\", \"n.save\")) # \"partial.linear.split.rf\", \"n.save.split\", # \"m.hat.rf\",\"mu.hat.rf\",\"rf.tuneOnce\")) thetas <- parSapply(cl, rep(n,simulations), function(n) { df <- simulate(n, p, mu, m) x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) c(lm(fmla,data=df)$coef[2], partial.linear.rf(df)$coef[2], #partial.linear.split.rf(df)$coef[2], partial.linear.lasso(df)$coefficients) } ) tmp <- (data.frame(t(thetas)) - 1)*sqrt(n) names(tmp) <- c(\"OLS\",\"Random.Forest\",\"Lasso\") tmp$n <- n tmp$p <- p tmp$design <- d sim.df <- rbind(sim.df, tmp) cat(\"finished sample size \",n,\"\\n\") cat(\"Elapsed time \", Sys.time()-start.time,\"\\n\") } cat(\"finished p = \",p,\"\\n\") } cat(\"finished design = \", d,\"\\n\") } stopCluster(cl) save(sim.df, file=\"partialLinearSim.Rdata\") library(ggplot2) library(reshape2) library(latex2exp) TeX <- latex2exp::TeX load(\"partialLinearSim.Rdata\") # see partialLinearSim.R for simulation # code df <- melt(sim.df, measure.vars=c(\"OLS\",\"Random.Forest\",\"Lasso\")) ggplot(subset(df,p==2), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(TeX('$\\\\sqrt{n}(\\\\hat{\\\\theta}-\\\\theta_0)$')) + ggtitle(\"p=2\") ggplot(subset(df,p==4), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=4\") ggplot(subset(df,p==6), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=6\") ggplot(subset(df,p==8), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=8\") Random forests do not seem to work very well in this context. Even when the functions being estimated are step functions, random forests do not produce a good estimate of $\\theta$. One caveat here is that I was not very careful about the tuning parameters for the random forests. It\u2019s possible that there exists a careful choice of tuning parameters that results in a better estimator. Research idea: create a generalization of random forests that is adaptive to the smoothness of the function being estimated. Two classic papers on adaptive regression estimators are Speckman ( 1985 ) and Donoho and Johnstone ( 1995 ). Friedberg et al. ( 2018 ) develop a local linear forest estimator. Combining their idea of using forests to form local neighborhoods with a smoothness adaptive variant of kernel or local polynomial regression should lead to a smoothness adaptive forest. Neural Networks \u00b6 Target function $f: \\R^p \\to \\R$ e.g. $f(x) = \\Er[y|x]$ Approximate with single hidden layer neural network : \\hat{f}(x) = \\sum_{j=1}^r \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j) Activation function $\\psi$ Examples: Sigmoid $\\psi(t) = 1/(1+e^{-t})$, Tanh $\\psi(t) = \\frac{e^t -e^{-t}}{e^t + e^{-t}}$, Heavyside $\\psi(t) = t 1(t\\geq 0)$ Weights $a_j$ Bias $b_j$ Able to approximate any $f$, Hornik, Stinchcombe, and White ( 1989 ) library(RSNNS) library(devtools) # download plot.nnet function from github source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r') load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) xn <- normalizeData(mod$x[,2:ncol(mod$x)]) yn <- normalizeData(mod$y) nn <- mlp(x=xn, y=yn, linOut=TRUE, size=c(10)) plot.nnet(nn, x.lab=colnames(mod$x)[2:ncol(mod$x)], y.lab=\"transProfit\") Deep Neural Networks \u00b6 Many hidden layers $x^{(0)} = x$ $x^{(\\ell)}_j = \\psi(a_j^{(\\ell)} x^{(\\ell-1)} + b_j^{(\\ell)})$ nn <- mlp(x=xn, y=yn, linOut=TRUE, size=c(5,10,3,5, 6)) plot.nnet(nn, x.lab=colnames(mod$x)[2:ncol(mod$x)], y.lab=\"transProfit\") Rate of convergence \u00b6 Chen and White ( 1999 ) $f(x) = \\Er[y|x]$ with Fourier representation f(x) = \\int e^{i a'x} d\\sigma_f(a) where $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$ Network sieve : \\begin{align*} \\mathcal{G}_n = \\{ & g: g(x) = \\sum_{j=1}^{r_n} \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j), \\\\ & \\norm{\\beta}_1 \\leq B_n \\} \\end{align*} The setup in Chen and White ( 1999 ) is more general. They consider estimating both $f$ and its first $m$ derivatives. Here, we focus on the case of just estimating $f$. Chen and White ( 1999 ) also consider estimation of functions other than conditional expectations. The restriction on $f$ in the second bullet is used to control approximation error. The second bullet says that $f$ is the inverse Fourier transform of measure $\\sigma_f$. The bite of the restriction on $f$ comes from the requirement that $\\sigma_f$ be absolutely integral, $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$. It would be a good exercise to check whether this restriction is satisfied by some familiar types of functions. Barron ( 1993 ) first showed that neural networks approximate this class of functions well, and compares the approximation rate of neural networks to other function approximation results. Rate of convergence [rate-of-convergence-3] \u00b6 Estimate \\hat{f} = \\argmin_{g \\in \\mathcal{G}_n} \\En [(y_i - g(x_i))^2] For fixed $p$, if $r_n^{2(1+1/(1+p))} \\log(r_n) = O(n)$, $B_n \\geq$ some constant \\Er[(\\hat{f}(x) - f(x))^2] = O\\left((n/\\log(n))^{\\frac{-(1 + 2/(p+1))} {2(1+1/(p+1))}}\\right) It is easy to see that regardless of $p$, $\\sqrt{n}\\Er[(\\hat{f}(x) - f(x))^2] \\to 0$. Therefore, neural networks would be suitable for estimating the nuisance functions in our examples above. There is a gap between applied use of neural networks and this statistical theory. These rate results are for networks with a single hidden layer. In prediction applications, the best performance is typically achieved by deep neural networks with many hidden layers. Intuitively, multiple hidden layers should do at least as well as a single hidden layer. There are some recent theoretical results that formalize this intuition. FIXME: ADD CITATIONS. Simulation Study \u00b6 Same setup as for random forests earlier Partially linear model DGP : $x_i \\in \\R^p$ with $x_{ij} \\sim U(0,1)$ $d_i = m(x_i) + v_i$ $y_i = d_i\\theta + f(x_i) + \\epsilon_i$ $m()$, $f()$ either linear or step functions Estimate by OLS, Neural network with & without cross-fitting Using orthogonal moments \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0 rm(list=ls()) p <- 4 # number of x's mu.linear <- function(x) x%*%rep(1,p) m.linear <- function(x) x%*%rep(2,p) mu.step <- function(x) (x>0.5)%*%rep(1,p) m.step <- function(x) (x>0.5)%*%rep(2,p) theta <- 1 simulate <- function(n,p,mu,m) { theta <- 1 x <- matrix(runif(n*p), ncol=p) d <- m(x) + rnorm(n) y <- theta*d + mu(x) + rnorm(n) data.frame(y=y,d=d,x=x) } library(grf) library(hdm) df <- simulate(100,p,mu.linear,m.linear) mrfparams <- NULL murfparams <- NULL n.save <- NULL partial.linear.rf <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] if (is.null(mrfparams) || n.save!=nrow(df)) { # to save time, we only tune once per cluster worker and data set # size cat(\"tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=1000, tune.parameters=TRUE) mrfparams <<- m.rf$tunable.params mu.rf <- regression_forest(df[,x.names], df$y, num.trees=1000, tune.parameters=TRUE) n.save <<- nrow(df) murfparams <<- mu.rf$tunable.params } else { cat(\"not tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(mrfparams[\"min.node.size\"]), alpha = as.numeric(mrfparams[\"alpha\"]), imbalance.penalty=as.numeric(mrfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(mrfparams[\"sample.fraction\"]), mtry=as.numeric(mrfparams[\"mtry\"])) mu.rf <- regression_forest(df[,x.names], df$y, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(murfparams[\"min.node.size\"]), alpha = as.numeric(murfparams[\"alpha\"]), imbalance.penalty=as.numeric(murfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(murfparams[\"sample.fraction\"]), mtry=as.numeric(murfparams[\"mtry\"])) } vhat <- df$d - predict(m.rf)$predictions ehat <- df$y - predict(mu.rf)$predictions lm(ehat ~ vhat) } ## Manual sample splitting --- this turns out to be unneccessary. The ## default behavior of predict.regression_forest is to return ## predictions on the training data using only trees that were not fit ## on each observation. In other words, regression_forest already does ## the sample splitting for us. ## ## rf.tuneOnce <- function(x.names, y.name) { ## parms <- NULL ## function(df) { ## if (is.null(parms)) { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=500, ## tune.parameters=TRUE) ## parms <<- rf$tunable.params ## rf ## } else { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=200, ## tune.parameters=FALSE, ## honesty=FALSE, ## min.node.size = ## as.numeric(parms[\"min.node.size\"]), ## alpha = as.numeric(parms[\"alpha\"]), ## imbalance.penalty=as.numeric(parms[\"imbalance.penalty\"]), ## sample.fraction = as.numeric(parms[\"sample.fraction\"]), ## mtry=as.numeric(parms[\"mtry\"])) ## } ## } ## } ## n.save.split <- NULL ## m.hat.rf <- NULL ## mu.hat.rf <- NULL ## partial.linear.split.rf <- function(df , splits=3) { ## x.names <- names(df)[grep(\"x.\",names(df))] ## if (is.null(n.save.split) || n.save.split != nrow(df)) { ## n.save.split <<- nrow(df) ## m.hat.rf <<- rf.tuneOnce(x.names,\"d\") ## mu.hat.rf <<- rf.tuneOnce(x.names,\"y\") ## } ## df$group <- sample(1:splits, nrow(df), replace=TRUE) ## vhat <- df$d ## ehat <- df$y ## for(g in 1:splits) { ## sdf <- subset(df, group!=g) ## m <- m.hat.rf(sdf) ## mu <- mu.hat.rf(sdf) ## vhat[df$group==g] <- df$d[df$group==g] - ## predict(m, newx=df[df$group==g,x.names])$predictions ## ehat[df$group==g] <- df$y[df$group==g] - ## predict(mu, newx=df[df$group==g,x.names])$predictions ## } ## lm(ehat ~ vhat) ## } partial.linear.lasso <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) rlassoEffects(fmla, data=df, I = ~ d) } #summary(partial.linear.lasso(df)) # simulate a bunch of times in parallel simulations <- 500 # number of simulations library(parallel) cl <- makeCluster(detectCores()/2) # change as you see fit clusterEvalQ(cl,library(hdm)) clusterEvalQ(cl,library(grf)) # R Socket cluster spawns new R sessions with empty environments, we # need to make sure they load any needed libraries and have access to # things from the main environment that they use design <- c(\"linear\") #,\"step\") sim.df <- data.frame() start.time <- Sys.time() for (d in design) { if (d==\"linear\") { m <- m.linear mu <- mu.linear } else { m <- m.step mu <- mu.step } for (p in c(2,4,6,8)) { for (n in c(100, 200, 400, 800, 1600)) { clusterExport(cl,c(\"simulate\",\"partial.linear.lasso\", \"partial.linear.rf\",\"p\",\"mu\",\"m\", \"mrfparams\",\"murfparams\", \"n.save\")) # \"partial.linear.split.rf\", \"n.save.split\", # \"m.hat.rf\",\"mu.hat.rf\",\"rf.tuneOnce\")) thetas <- parSapply(cl, rep(n,simulations), function(n) { df <- simulate(n, p, mu, m) x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) c(lm(fmla,data=df)$coef[2], partial.linear.rf(df)$coef[2], #partial.linear.split.rf(df)$coef[2], partial.linear.lasso(df)$coefficients) } ) tmp <- (data.frame(t(thetas)) - 1)*sqrt(n) names(tmp) <- c(\"OLS\",\"Random.Forest\",\"Lasso\") tmp$n <- n tmp$p <- p tmp$design <- d sim.df <- rbind(sim.df, tmp) cat(\"finished sample size \",n,\"\\n\") cat(\"Elapsed time \", Sys.time()-start.time,\"\\n\") } cat(\"finished p = \",p,\"\\n\") } cat(\"finished design = \", d,\"\\n\") } stopCluster(cl) save(sim.df, file=\"partialLinearSim.Rdata\") library(ggplot2) library(reshape2) library(latex2exp) TeX <- latex2exp::TeX load(\"partialLinearSimNet.Rdata\") # see partialLinearSim.R for simulation # code df <- melt(sim.df, measure.vars=names(sim.df)[1:3]) ggplot(subset(df,p==2), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(TeX('$\\\\sqrt{n}(\\\\hat{\\\\theta}-\\\\theta_0)$')) + ggtitle(\"p=2\") ggplot(subset(df,p==4), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=4\") ggplot(subset(df,p==6), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=6\") ggplot(subset(df,p==8), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=8\") The performance of the neural network estimator appears okay, but not outstanding in these simulations. In the linear model, the neural network estimator performs slightly worse than OLS. In the step function model, the neural network estimator performs slight better than the misspecified OLS, but neither appears to work well. In both cases, it appears that the neural network estimator produces occassional outliers. I believe that this is related to the fact that the minimization problem defining the neural network is actually very difficult to solve. In the simulation above, I suspect the outlying estimates are due to minimization problems. In the simulations, I simply set $r_n = n^{1/(2(1+1/(1+p)))}$. It\u2019s likely that a more careful choice of $r_n$, perhaps using cross-validation, would give better results. Bibliography \u00b6 Barron, A. R. 1993. \u201cUniversal Approximation Bounds for Superpositions of a Sigmoidal Function.\u201d IEEE Transactions on Information Theory 39 (3): 930\u201345. https://doi.org/10.1109/18.256500 . Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen. 2012. \u201cSparse Models and Methods for Optimal Instruments with an Application to Eminent Domain.\u201d Econometrica 80 (6): 2369\u20132429. https://doi.org/10.3982/ECTA9626 . Belloni, Alexandre, and Victor Chernozhukov. 2011. \u201cHigh Dimensional Sparse Econometric Models: An Introduction.\u201d In Inverse Problems and High-Dimensional Estimation: Stats in the Ch\u00e2teau Summer School, August 31 - September 4, 2009 , edited by Pierre Alquier, Eric Gautier, and Gilles Stoltz, 121\u201356. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-19989-9_3 . Biau, G\u00e9rard. 2012. \u201cAnalysis of a Random Forests Model.\u201d Journal of Machine Learning Research 13 (Apr): 1063\u201395. http://www.jmlr.org/papers/v13/biau12a.html . Caner, Mehmet, and Anders Bredahl Kock. 2018. \u201cAsymptotically Honest Confidence Regions for High Dimensional Parameters by the Desparsified Conservative Lasso.\u201d Journal of Econometrics 203 (1): 143\u201368. https://doi.org/https://doi.org/10.1016/j.jeconom.2017.11.005 . Chen, Xiaohong, and H. White. 1999. \u201cImproved Rates and Asymptotic Normality for Nonparametric Neural Network Estimators.\u201d IEEE Transactions on Information Theory 45 (2): 682\u201391. https://doi.org/10.1109/18.749011 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. \u201chdm: High-Dimensional Metrics.\u201d R Journal 8 (2): 185\u201399. https://journal.r-project.org/archive/2016/RJ-2016-040/index.html . Chetverikov, Denis, Zhipeng Liao, and Victor Chernozhukov. 2016. \u201cOn Cross-Validated Lasso.\u201d https://arxiv.org/abs/1605.02214 . Donoho, David L., and Iain M. Johnstone. 1995. \u201cAdapting to Unknown Smoothness via Wavelet Shrinkage.\u201d Journal of the American Statistical Association 90 (432): 1200\u20131224. http://www.jstor.org/stable/2291512 . Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference . Vol. 5. Cambridge University Press. . Friedberg, Rina, Julie Tibshirani, Susan Athey, and Stefan Wager. 2018. \u201cLocal Linear Forests.\u201d https://arxiv.org/abs/1807.11408 . Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning . Springer series in statistics. . Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. \u201cMultilayer Feedforward Networks Are Universal Approximators.\u201d Neural Networks 2 (5): 359\u201366. https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8 . James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning . Vol. 112. Springer. . Lee, Jason D., Dennis L. Sun, Yuekai Sun, and Jonathan E. Taylor. 2016. \u201cExact Post-Selection Inference, with Application to the Lasso.\u201d Ann. Statist. 44 (3): 907\u201327. https://doi.org/10.1214/15-AOS1371 . Li, Ker-Chau. 1989. \u201cHonest Confidence Regions for Nonparametric Regression.\u201d Ann. Statist. 17 (3): 1001\u20138. https://doi.org/10.1214/aos/1176347253 . Mullainathan, Sendhil, and Jann Spiess. 2017. \u201cMachine Learning: An Applied Econometric Approach.\u201d Journal of Economic Perspectives 31 (2): 87\u2013106. https://doi.org/10.1257/jep.31.2.87 . Nickl, Richard, and Sara van de Geer. 2013. \u201cConfidence Sets in Sparse Regression.\u201d Ann. Statist. 41 (6): 2852\u201376. https://doi.org/10.1214/13-AOS1170 . Speckman, Paul. 1985. \u201cSpline Smoothing and Optimal Rates of Convergence in Nonparametric Regression Models.\u201d The Annals of Statistics 13 (3): 970\u201383. http://www.jstor.org/stable/2241119 . Stone, Charles J. 1982. \u201cOptimal Global Rates of Convergence for Nonparametric Regression.\u201d The Annals of Statistics 10 (4): 1040\u201353. http://www.jstor.org/stable/2240707 . Taylor, Jonathan, and Robert Tibshirani. 2017. \u201cPost-Selection Inference for -Penalized Likelihood Models.\u201d Canadian Journal of Statistics 46 (1): 41\u201361. https://doi.org/10.1002/cjs.11313 . Wager, Stefan, and Susan Athey. 2018. \u201cEstimation and Inference of Heterogeneous Treatment Effects Using Random Forests.\u201d Journal of the American Statistical Association 0 (0): 1\u201315. https://doi.org/10.1080/01621459.2017.1319839 . Wager, Stefan, and Guenther Walther. 2015. \u201cAdaptive Concentration of Regression Trees, with Application to Random Forests.\u201d https://arxiv.org/abs/1503.06388 .","title":"Methods"},{"location":"ml-methods/#introduction-to-machine-learning","text":"Friedman, Hastie, and Tibshirani ( 2009 ) and James et al. ( 2013 ) are commonly recommended textbooks on machine learning. James et al. ( 2013 ) is less technical of the two, but neither book is especially difficult. Efron and Hastie ( 2016 ) covers similar material and is slightly more advanced.","title":"Introduction to machine learning"},{"location":"ml-methods/#some-prediction-examples","text":"Machine learning is tailored for prediction, let\u2019s look at some data and see how well it works","title":"Some prediction examples"},{"location":"ml-methods/#predicting-house-prices","text":"Example from Mullainathan and Spiess ( 2017 ) Training on 10000 observations from AHS Predict log house price using 150 variables Holdout sample of 41808","title":"Predicting house prices"},{"location":"ml-methods/#ahs-variables-ahs-variables","text":"ahs <- readRDS(\"ahs2011forjep.rdata\")$df print(summary(ahs[,1:20])) ## LOGVALUE REGION METRO METRO3 PHONE KITCHEN ## Min. : 0.00 1: 5773 1:10499 1:11928 -7: 1851 1:51513 ## 1st Qu.:11.56 2:13503 2: 1124 2:39037 1 :49353 2: 295 ## Median :12.10 3:15408 3: 202 9: 843 2 : 604 ## Mean :12.06 4:17124 4: 103 ## 3rd Qu.:12.61 7:39880 ## Max. :15.48 ## MOBILTYP WINTEROVEN WINTERKESP WINTERELSP WINTERWOOD WINTERNONE ## -1:49868 -8: 133 -8: 133 -8: 133 -8: 133 -8: 133 ## 1 : 927 -7: 50 -7: 50 -7: 50 -7: 50 -7: 50 ## 2 : 1013 1 : 446 1 : 813 1 : 8689 1 : 61 1 :41895 ## 2 :51179 2 :50812 2 :42936 2 :51564 2 : 9730 ## ## ## NEWC DISH WASH DRY NUNIT2 BURNER COOK ## -9:50485 1:42221 1:50456 1:49880 1:44922 -6:51567 1:51567 ## 1 : 1323 2: 9587 2: 1352 2: 1928 2: 2634 1 : 87 2: 241 ## 3: 2307 2 : 154 ## 4: 1945 ## ## ## OVEN ## -6:51654 ## 1 : 127 ## 2 : 27 ## ## ## library(GGally) ggpairs(ahs[,c(\"LOGVALUE\",\"ROOMS\", \"LOT\",\"UNITSF\",\"BUILT\")], lower=list(continuous=\"points\", combo=\"facethist\", discrete=\"facetbar\"), diag=list(continuous=\"barDiag\",discrete=\"barDiag\")) + theme_minimal() # use ms-reproduce.R from course git repo to download and run Mullainathon & Spiess data and code to # create jepfittedmodels-table1.csv. Be aware that this will take many hours. tbl <- read.csv(\"jepfittedmodels-table1.csv\") tab <- tbl[,3:ncol(tbl)] rownames(tab) <- tbl[,2] tab <- tab[1:5, c(1,2,3,5)] colnames(tab) <- c(\"in sample MSE\", \"in sample R^2\", \"out of sample MSE\", \"out of sample R^2\") library(kableExtra) kable_styling(kable(tab, caption=\"Performance of different algorithms in predicting housing values\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Performance of different algorithms in predicting housing values in sample MSE in sample R\\^2 out of sample MSE out of sample R\\^2 OLS 0.589 0.473 0.674 0.417 Tree 0.675 0.396 0.758 0.345 Lasso 0.603 0.460 0.656 0.433 Forest 0.166 0.851 0.632 0.454 Ensemble 0.216 0.807 0.625 0.460 library(ggplot2) load(file=\"jeperrorfig.RData\") print(fig) load(file=\"jeperrorfig.RData\") print(fig2)","title":"AHS variables [ahs-variables]"},{"location":"ml-methods/#predicting-pipeline-revenues","text":"Data on US natural gas pipelines Combination of FERC Form 2, EIA Form 176, and other sources, compiled by me 1996-2016, 236 pipeline companies, 1219 company-year observations Predict: $y =$ profits from transmission of natural gas Covariates: year, capital, discovered gas reserves, well head gas price, city gate gas price, heating degree days, state(s) that each pipeline operates in load(\"pipelines.Rdata\") # data has problems before 1996 due to format change data <- subset(data,report_yr>=1996) # replace NA state weights with 0's data[,59:107][is.na(data[,59:107])] <- 0 # spaces in variable names will create problems later names(data) <- gsub(\" \",\".\",names(data)) summary(data[,c(\"transProfit\",\"transPlant_bal_beg_yr\",\"cityPrice\",\"wellPrice\")]) ## transProfit transPlant_bal_beg_yr cityPrice ## Min. : -31622547 Min. :0.000e+00 Min. : 0.4068 ## 1st Qu.: 2586031 1st Qu.:2.404e+07 1st Qu.: 3.8666 ## Median : 23733170 Median :1.957e+08 Median : 5.1297 ## Mean : 93517513 Mean :7.772e+08 Mean : 5.3469 ## 3rd Qu.: 129629013 3rd Qu.:1.016e+09 3rd Qu.: 6.5600 ## Max. :1165050214 Max. :1.439e+10 Max. :12.4646 ## NA's :2817 NA's :2692 NA's :1340 ## wellPrice ## Min. :0.0008 ## 1st Qu.:2.1230 ## Median :3.4370 ## Mean :3.7856 ## 3rd Qu.:5.1795 ## Max. :9.6500 ## NA's :2637 library(GGally) ggpairs(data[,c(\"transProfit\",\"transPlant_bal_beg_yr\",\"cityPrice\",\"wellPrice\")], lower=list(continuous=\"smooth\")) + theme_minimal()","title":"Predicting pipeline revenues"},{"location":"ml-methods/#predicting-pipeline-revenues-methods","text":"OLS : 67 covariates (year dummies and state(s) create a lot) Lasso Random forests Randomly choose 75% of sample to fit the models, then look at prediction accuracy in remaining 25% We are focusing on Lasso and random forests because these are the two methods that econometricians have worked on the most. Other methods such as neural nets and support vector machines are also worth exploring. For now, you can think of Lasso and random forests these as black boxes that generate predictions from data. We will go into more detail soon. ## Create X matrix for OLS and random forests xnames <-c(\"transPlant_bal_beg_yr\", \"reserve\", \"wellPrice\", \"cityPrice\", \"plantArea\", \"heatDegDays\", names(data)[59:107] ) yname <- \"transProfit\" fmla <- paste(yname,\"~\",paste(xnames,collapse=\" + \"),\"+ as.factor(report_yr)\") ols <- lm(fmla,data=data,x=TRUE,y=TRUE) X <- ols$x[,!(colnames(ols$x) %in% c(\"(Intercept)\")) & !is.na(ols$coefficients)] y <- ols$y train <- runif(nrow(X))<0.75 # OLS prediction on training set y.t <- y[train] X.t <- X[train,] ols <- lm(y.t ~ X.t) y.hat.ols <- ols$coefficients[1] + X %*% ols$coefficients[2:(length(ols$coef))] df <- data.frame(y=y, y.hat=y.hat.ols, train=train, method=\"ols\") ## Lasso library(glmnet) # Create larger X matrix for lasso fmla.l <- paste(yname,\"~ (\", paste(xnames,collapse=\" + \"),\")*(report_yr + transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays) + \", paste(sprintf(\"I(%s^2)\",xnames[1:6],collapse=\" + \")) ) reg <- lm(fmla.l, data=data, x=TRUE,y=TRUE) Xl <- reg$x[,!(colnames(reg$x) %in% c(\"(Intercept)\")) & !is.na(reg$coefficients)] lasso <- cv.glmnet(Xl[train,],y[train],alpha=1,parallel=FALSE, standardize=TRUE, intercept=TRUE, nfolds = 50) y.hat.lasso <- predict(lasso, Xl, s=lasso$lambda.min, type=\"response\") df <- rbind(df, data.frame(y=y, y.hat=as.vector(y.hat.lasso), train=train, method=\"lasso\")) ## Random forest library(grf) rf <- regression_forest(X[train,],y[train],tune.parameters = TRUE) y.hat.rf <- predict(rf, X)$predictions df <- rbind(df, data.frame(y=y, y.hat=y.hat.rf, train=train, method=\"random forest\")) # Neural network library(RSNNS) n <- nrow(X[train,]) p <- ncol(X) rn <- floor(n^(1/(2*(1+1/(1+p))))/2) xn <- normalizeData(X) yn <- normalizeData(y) nn <- mlp(x=xn[train,], y=yn[train], linOut=TRUE, size=rn) yn.hat.nn <- predict(nn, xn) y.hat.nn <- denormalizeData(yn.hat.nn, getNormParameters(yn)) df <- rbind(df, data.frame(y=y, y.hat=y.hat.nn, train=train, method=\"neural network\")) ggplot(data=df,aes(x=y,y=y.hat,colour=method,shape=train)) + geom_point(alpha=0.5) + geom_line(aes(y=y)) + theme_minimal() df$trainf <- factor(df$train, levels=c(\"TRUE\", \"FALSE\")) df$error <- df$y - df$y.hat ggplot(data=df,aes(x=error,colour=method)) + geom_density() + theme_minimal() + xlim(quantile(df$error,c(0.01,0.99))) + facet_grid(trainf ~ .,labeller=label_both) library(kableExtra) fn <- function(df) with(df,c(mean((y.hat - y)^2)/var(y), mean(abs(y.hat - y))/mean(abs(y-mean(y))))) tab1 <-unlist(by(subset(df,train), df$method[train], FUN=fn)) tab1 <- (matrix(tab1,nrow=2)) rownames(tab1) <- c(\"relative MSE\",\"relative MAE\") colnames(tab1) <- c(\"OLS\",\"Lasso\",\"Random forest\",\"Neural Network\") tab2 <- unlist(by(subset(df,!train), df$method[!train], FUN=fn)) tab2 <- (matrix(tab2,nrow=2)) rownames(tab2) <- c(\"relative MSE\",\"relative MAE\") colnames(tab2) <- c(\"OLS\",\"Lasso\",\"Random forest\",\"Neural Network\") kable_styling(kable(tab1, caption=\"Training sample\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=F) Training sample OLS Lasso Random forest Neural Network relative MSE 0.110 0.031 0.063 0.012 relative MAE 0.265 0.137 0.179 0.111 kable_styling(kable(tab2, caption=\"Hold-out sample\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=F) Hold-out sample OLS Lasso Random forest Neural Network relative MSE 0.125 0.064 0.093 0.092 relative MAE 0.288 0.199 0.232 0.278 In this table, relative MSE is the mean squared error relative to the variance of $y$, that is \\text{relative MSE} = \\frac{\\En[(y_i - \\hat{y}_i)^2]} {\\En[ (y_i - \\bar{y})^2]}. It is equal to $1-R^2$. Similarly, relative MAE is \\text{relative MAE} = \\frac{\\En[|y_i - \\hat{y}_i|]} {\\En[|y_i - \\bar{y}|]}. $\\En$ denotes the empirical expectation, $\\En[y_i] = \\frac{1}{n}\\sum_{i=1}^n y_i$.","title":"Predicting pipeline revenues : methods"},{"location":"ml-methods/#lasso","text":"Lasso solves a penalized (regularized) regression problem \\hat{\\beta} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] + \\frac{\\lambda}{n} \\norm{ \\hat{\\Psi} \\beta}_1 Penalty parameter $\\lambda$ Diagonal matrix $\\hat{\\Psi} = diag(\\hat{\\psi})$ Dimension of $x_i$ is $p$ and implicitly depends on $n$ can have $p >> n$ We are following the notation used in Chernozhukov, Hansen, and Spindler ( 2016 ). Note that this vignette has been updated since it was published in the R Journal. To obtain the most recent version, install the hdm package in R, load it, and then open the vignette. install.packages(\"hdm\") library(hdm) vignette(\"hdm_introduction\") The choice of penalty (or regularization) parameter, $\\lambda$, is important. When $\\lambda = 0$, Lasso is the same as OLS. As $\\lambda$ increases, the Lasso estimates will shrink toward 0. For large enough $\\lambda$, some components of $\\hat{\\beta}$ become exactly 0. As $\\lambda$ increases more, more and more components of $\\hat{\\beta}$ will be exactly $0$. For some intuition about why Lasso results in some coefficients being zero, note that \\hat{\\beta}^{lasso} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] + \\frac{\\lambda}{n} \\norm{\\beta}_1 is equivalent to \\hat{\\beta}^{lasso} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] \\text{ s.t. } \\norm{\\beta}_1 \\leq s for some $s$. In this problem, the boundary of the constraint set will be a diamond. The level sets of the objective will be elipses. Generically, the solution will lie on one of the corners of the $\\norm{\\beta}_1 = 1$ set. See Friedman, Hastie, and Tibshirani ( 2009 ) or James et al. ( 2013 ) for more details. Most machine learning methods involve some form of regularization with an associated regularization parameter. In choosing the regularization parameter, we face a bias-variance tradeoff. As $\\lambda$ increases, variance decreases, but bias increases. Machine learning algorithms typically choose regularization parameters through cross-validation. Although cross-validation leads to good predictive performance, the statistical properties are not always known. Chernozhukov, Hansen, and Spindler ( 2016 ) say, \u201cIn high dimensional settings cross-validation is very popular; but it lacks a theoretical justification for use in the present context.\u201d However, there has been some recent progress on convergence rates for Lasso with cross-validation, see Chetverikov, Liao, and Chernozhukov ( 2016 ). The diagonal matrix $\\hat{\\Psi}$ is used to make the estimator invariant to scaling of $x_i$, and to allow for heteroskedasticity. If reading about Lasso or using code from other authors, be careful some do not include $\\hat{\\Psi}$ and use $\\lambda$ instead of $\\frac{\\lambda}{n}$. load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) library(glmnet) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) # standardize everything so that coefficients are similar scale when plotted mod$y <- (mod$y - mean(mod$y))/sd(mod$y) for(c in 2:ncol(mod$x)) { mod$x[,c] <- (mod$x[,c] - mean(mod$x[,c]))/sd(mod$x[,c]) } lassoPath <- glmnet(mod$x, mod$y, alpha=1) plot(lassoPath, xvar=\"lambda\", label=TRUE) load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) library(glmnet) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) # standardize everything so that coefficients are similar scale when plotted mod$y <- (mod$y - mean(mod$y))/sd(mod$y) for(c in 2:ncol(mod$x)) { mod$x[,c] <- (mod$x[,c] - mean(mod$x[,c]))/sd(mod$x[,c]) } cv <- cv.glmnet(mod$x, mod$y, alpha=1) plot(cv)","title":"Lasso"},{"location":"ml-methods/#statistical-properties-of-lasso","text":"Model : y_i = x_i'\\beta_0 + \\epsilon_i $\\Er[x_i \\epsilon_i] = 0$ $\\beta_0 \\in \\R^n$ $p$, $\\beta_0$, $x_i$, and $s$ implicitly depend on $n$ $\\log p = o(n^{1/3})$ $p$ may increase with $n$ and can have $p>n$ Sparsity $s$ Exact : $\\norm{\\beta_0}_0 = s = o(n)$ Approximate : $|\\beta_{0,j}| < Aj^{-a}$, $a > 1/2$, $s \\propto n^{1/(2a)}$ $\\norm{\\beta}_0$ is the number of non-zero components of $\\beta$. The approximate sparsity setting means if $|\\beta_{0,j}| < Aj^{-a}$, then, there exists a sparse approximation, say $\\beta_{a}$, with $s$ nonzero elements, such that the approximation error, \\En[(x_i'(\\beta_a - \\beta_0))^2] = c_s^2 will vanish quickly if $s \\propto n^{1/2a}$. Just how quickly will it vanish? An easy upper bound is \\begin{align*} c_s^2 \\leq & \\En\\left[\\left( \\sum_{j={s+1}}^p x_ij \\beta_{0,j} \\right)^2 \\right] \\\\ \\leq & \\En\\left[ \\left(\\sum_{j={s+1}}^p x_ij A j^{-a} \\right)^2 \\right] \\end{align*} To simplify the alegebra, let\u2019s assume $\\En[x_i x_i\u2019] = I_p$, then \\begin{align*} c_s^2 \\leq & \\sum_{j={s+1}}^p A^2 j^{-2a} \\\\ & \\leq \\sum_{j={s+1}}^\\infty A^2 j^{-2a} = A^2 s^{-2a} \\zeta(2a) \\\\ c_s^2 \\lesssim s^{-2a} \\end{align*} where $\\zeta(2a) = \\sum_{j=1}^\\infty j^{-2a}$ is the Riemann Zeta function (all that matters here is that $\\zeta(2a)$ is finite for $2a>1$). Then if $s \\propto n^{(1+\\delta)/2a}$, we would get $c_s \\lesssim n^{-(1+\\delta)/2}$. Importantly, $\\sqrt{n} c_s = o(1)$, so in the sort of expansions that we would do to show that $\\hat{\\theta}$ is $\\sqrt{n}$ asymptotically normal, the bias term would vanish.","title":"Statistical properties of Lasso"},{"location":"ml-methods/#rate-of-convergence","text":"With $\\lambda = 2c \\sqrt{n} \\Phi^{-1}(1-\\gamma/(2p))$ \\sqrt{\\En[(x_i'(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\sqrt{ (s/n) \\log (p) }, \\norm{\\hat{\\beta}^{lasso} - \\beta_0}_2 \\lesssim_P \\sqrt{ (s/n) \\log (p) }, and \\norm{\\hat{\\beta}^{lasso} - \\beta_0}_1 \\lesssim_P \\sqrt{ (s^2/n) \\log (p) } Constant $c>1$ Small $\\gamma \\to 0$ with $n$, and $\\log(1/\\gamma) \\lesssim \\log(p)$ Rank like condition on $x_i$ near-oracle rate In the semiparametric estimation problems that we\u2019re focused on, our object of interest is some finite dimensional parameter $\\theta$ that depends on our data some high dimensional parameter, like $\\beta_0$ in the Lasso. To analyze estimates of $\\hat{\\theta}(data, \\hat{\\beta})$, a key step will be to show that we can replace $\\hat{\\beta}$ with $\\beta_0$. The rate of convergence of $\\hat{\\beta}$ will be important for making this possible. Thus, the main thing that we care about for the Lasso and other machine learning estimators will be their rates of converagence. The notation $A_n \\lesssim_P B_n$ is read $A_n$ is bounded in probability by $B_n$ and means that for any $\\epsilon>0$, there exists $M$, $N$ such that $\\Pr(|A_n/B_n| > M) < \\epsilon$ for all $n > N$. This is also often denoted by $A_n = O(B_n)$. These rate results are from Belloni et al. ( 2012 ). Since this setup allows $p>n$, $x$ cannot be assumed to have full rank. Instead, an assumption about the eigenvalues of $X\u2019X$ restricted to the nonzero components of $\\beta_0$, plays a similar. See Belloni et al. ( 2012 ) for details. This convergence rate is called the near-oracle rate, because it is nearly as good as what we get if an oracle told us which components of $\\beta_0$ are nonzero. In that case OLS using just those $s$ components gives the fastest possible rate, which is \\sqrt{\\En[(x_i'(\\hat{\\beta}^{OLS} - \\beta_0))^2]} \\propto \\sqrt{s/n}.","title":"Rate of convergence"},{"location":"ml-methods/#rate-of-convergence-rate-of-convergence-1","text":"Using cross-validation to choose $\\lambda$ known bounds are worse With Gaussian errors: $\\sqrt{\\En[(x_i\u2019(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\sqrt{ (s/n) \\log (p) } \\log(pn)^{7/8}$, Without Gaussian error $\\sqrt{\\En[(x_i\u2019(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\left( \\frac{s \\log(pn)^2}{n} \\right)^{1/4}$ Chetverikov, Liao, and Chernozhukov ( 2016 ) These results are for an exactly sparse setting. Do they hold under approximate sparsity?","title":"Rate of convergence [rate-of-convergence-1]"},{"location":"ml-methods/#other-statistical-properties","text":"Inference on $\\beta$: not the goal in our motivating examples Difficult, but some recent results See Lee et al. ( 2016 ), Taylor and Tibshirani ( 2017 ), Caner and Kock ( 2018 ) Model selection: not the goal in our motivating examples Under stronger conditions, Lasso correctly selects the nonzero components of $\\beta_0$ See Belloni and Chernozhukov ( 2011 ) In the statistics literature on high dimensional and nonparametric estimation, you will come across the terms \u201cadaptive\u201d and \u201chonest.\u201d Adaptivity of an estimator refers to the situation where the rate of convergence depends on some unknown parameter. In the case of Lasso, the sparsity index of the true model, $s$, is an unknown parameter affecting the rate of convergence. Without knowing or estimating $s$, Lasso attains the above rate of convergence for a wide range of admissable $s$. Thus, Lasso is adaptive to the unknown sparsity index. \u201cHonest\u201d is a property of an inference method. A confidence region is honest if it has correct coverage for a large class of true models. For the Lasso, an honest confidence region would be valid for a wide range of sparsity, $s$. An honest, adaptive confidence region would be one that is valide for a wide range of $s$ and whose size shrinks as quickly as if $s$ were known. Achieving both adaptivity and honesty is impossible in the most general setting. For example, although an $\\ell$ times differentiable function of a $p$ dimensional variable can be adaptively estimated at rate $n^{-\\ell}{2\\ell + p}$, Li ( 1989 ) showed that an honest confidence region can contract at most at rate $n^{-1/4}$ (not adaptive to $\\ell$). However, an adaptive confidence region can be constructed if further restrictions are placed on the set of possible models, see Nickl and Geer ( 2013 ) for such a result for Lasso..","title":"Other statistical properties"},{"location":"ml-methods/#post-lasso","text":"Two steps : Estimate $\\hat{\\beta}^{lasso}$ ${\\hat{\\beta}}^{post} =$ OLS regression of $y$ on components of $x$ with nonzero $\\hat{\\beta}^{lasso}$ Same rates of convergence as Lasso Under some conditions post-Lasso has lower bias If Lasso selects correct model, post-Lasso converges at the oracle rate Post-Lasso removes some of the regularizaton bias of Lasso. The rate of convergence of post-Lasso is always as fast as Lasso, and under conditions that allow perfect model selection, post-Lasso converges slightly faster (by a factor $\\log(p)$). See Belloni et al. ( 2012 ) for details.","title":"Post-Lasso"},{"location":"ml-methods/#random-forests","text":"","title":"Random forests"},{"location":"ml-methods/#regression-trees","text":"$y_i \\in R$ on $x_i \\in \\R^p$ Want to estimate $\\Er[y | x]$ Locally constant estimate \\hat{t}(x) = \\sum_m^M c_m 1\\{x \\in R_m \\} Rectangular regions $R_m$ determined by tree","title":"Regression trees"},{"location":"ml-methods/#simulated-data","text":"n <- 1000 x <- runif(n) y <- runif(n) f <- function(x,z) { 1/3*(sin(5*x)*sqrt(z)*exp(-(z-0.5)^2)) } f0 <- f(x,y) z <- f0 + rnorm(n)*0.1 tree.df <- data.frame(x=x,y=y,z=z) # plot true function and data x.g <- seq(0,1,length.out=100) y.g <- seq(0,1,length.out=100) f0.g <- t(outer(x.g,y.g,f)) library(plotly) fig <- plot_ly( colors=\"YlOrBr\") fig <- add_markers(fig,x=x,y=y,z=z, size=0.3, opacity=0.2) fig <- add_surface(fig, x=x.g, y=y.g, z=f0.g, opacity=1) fig","title":"Simulated data"},{"location":"ml-methods/#estimated-tree","text":"# fit regression tree library(party) tree <- ctree(z ~ x + y, data=tree.df) # plot estimate x.g <- seq(0,1,length.out=100) y.g <- seq(0,1,length.out=100) df <- expand.grid(x.g,y.g) names(df) <- c(\"x\",\"y\") fhat.g <- matrix(predict(tree, newdata=df),nrow=length(x.g), byrow=TRUE) library(plotly) fig <- plot_ly(colors=\"YlOrBr\") fig <- add_markers(fig,x=x,y=y,z=z, size=0.3, opacity=0.2) fig <- add_surface(fig, x=x.g, y=y.g, z=fhat.g, opacity=1) fig","title":"Estimated tree"},{"location":"ml-methods/#estimated-tree_1","text":"plot(tree)","title":"Estimated tree"},{"location":"ml-methods/#tree-algorithm","text":"For each region, solve \\min_{j,s} \\left[ \\min_{c_1} \\sum_{i: x_{i,j} \\leq s, x_i \\in R} (y_i - c_1)^2 + \\min_{c_2} \\sum_{i: x_{i,j} > s, x_i \\in R} (y_i - c_2)^2 \\right] Repeat with $R = {x:x_{i,j} \\leq s^ } \\cap R$ and $R = {x:x_{i,j} \\leq s^ } \\cap R$ Stop when $|R| =$ some chosen minimum size Prune tree \\min_{tree \\subset T} \\sum (\\hat{f}(x)-y)^2 + \\alpha|\\text{terminal nodes in tree}| There are many variations on this tree building algorithm. They all share some rule to decide on which variable and where to split. They all have some kind of stopping rule, but not necessarily the same one. For example, some algorithms stop splitting into new branches when the improvement in $R^2$ becomes small. These trees don\u2019t need subsequent pruning, but also may fail to find later splits that might be important. As with lasso, regression trees involve some regularization. In the above description, both the minimum leaf size and $\\alpha$ in the pruning step serve as regularization parameters. A potential advantage of regression trees is that their output might be interpretable, especially if there are not many branches. Some disadvantages are that they often are not very good predictors, and small perturbations in data can lead to seemingly large changes in the tree.","title":"Tree algorithm"},{"location":"ml-methods/#random-forests_1","text":"Average randomized regression trees Trees randomized by Bootstrap or subsampling Randomize branches: \\min_{j \\in S,s} \\left[ \\min_{c_1} \\sum_{i: x_{i,j} \\leq s, x_i \\in R} (y_i - c_1)^2 + \\min_{c_2} \\sum_{i: x_{i,j} > s, x_i \\in R} (y_i - c_2)^2 \\right] where $S$ is random subset of ${1, \u2026, p}$ Variance reduction","title":"Random forests"},{"location":"ml-methods/#rate-of-convergence-regression-tree","text":"$x \\in [0,1]^p$, $\\Er[y|x]$ Lipschitz in $x$ Crude calculation for single tree, let denote $R_i$ node that contains $x_i$ \\begin{align*} \\Er(\\hat{t}(x_i) - \\Er[y|x_i])^2 = & \\overbrace{\\Er(\\hat{t}(x_i) - \\Er[y|x\\in R_i])^2}^{variance} + \\overbrace{(\\Er[y|x \\in R_i] - \\Er[y|x])^2}^{bias^2} \\\\ = & O_p(1/m) + O\\left(L^2 \\left(\\frac{m}{n}\\right)^{2/p}\\right) \\end{align*} optimal $m = O(n^{2/(2+p)})$ gives \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2] = O_p(n^{\\frac{-2}{2+p}}) By a crude calculation, I mean lets treat the tree as fixed. The the variance term is simply from estimating a conditional mean. This analysis could be made more rigorous by assuming the tree was estimated by sample splitting \u2014 use half the data to construct the tree and the remaining half to estimate the mean of $y$ in each node. Athey and Wager, and others, refer to such trees as \u201chonest.\u201d I suppose that this is because sample splitting facilitates honest inference afterward. The order of the bias term comes from considering the width of the pieces of a $p$ dimensional cube split evenly into $n/m$ pieces. Remember that for our motivating semiparametric problems, we need $\\sqrt{n} \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2]$ to vanish. The above rate convergence is too slow for $p>2$. The calculation of the above was admittedly crude, and may not be exact. However, Stone ( 1982 ) showed that if $\\Er[y|x]$ is $\\ell$ times differentiable, the fastest possible rate of convergence for any estimator is $n^{\\frac{-\\ell}{2\\ell + p}}$. To have any hope of a fast enough rate, we need to assume the function we\u2019re estimating is very smooth (high $\\ell$), or place some other restriction on the class of functions we allow (like sparsity for the Lasso). Lipschitz continuity is slightly weaker than once differentiable on a compact set, so it should come as no surprise that the rate of convergence would be slow.","title":"Rate of convergence: regression tree"},{"location":"ml-methods/#rate-of-convergence-random-forest","text":"Result from Biau ( 2012 ) Assume $\\Er[y|x]=\\Er[y|x_{(s)}]$, $x_{(s)}$ subset of $s$ variables, then \\Er[(\\hat{r}(x_i) - \\Er[y|x_i])^2] = O_p\\left(\\frac{1}{m\\log(n/m)^{s/2p}}\\right) + O_p\\left(\\left(\\frac{m}{n}\\right)^{\\frac{0.75}{s\\log 2}} \\right) or with optimal $m$ \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2] = O_p(n^{\\frac{-0.75}{s\\log 2+0.75}}) This result from Biau ( 2012 ) assumes the forest is estimated with sample splitting. This avoids the difficult to analyze correlation between the nodes and $y$. Wager and Walther ( 2015 ) analyze what happens when the same data is used to construct the tree and average in each node. They get a slightly higher upper bound for the variance of $\\frac{\\log(p)\\log(n)}{m}$. Wager and Walther ( 2015 ) also allow $p$ to increase with $n$, whereas the previous analysis treated $p$ as fixed. These convergence rate results for random forests are not fast enough for our purpose. Does this mean that random forests should not be used in semiparametric estimation? Not necessarily. We\u2019re asking too much of random forests. There is no estimator for an arbitrary Lipschitz function that can have fast enough a rate of convergence. A restriction on the set of possible functions is needed to reduce the approximation bias. With Lasso, the assumption of (approximate) sparsity played that role. Chernozhukov et al. ( 2018 ) advise that random forests could be a good choice for semiparametric estimation when the function of interest is \u201cwell-approximated by a random forest.\u201d Unfortunately, there does not appear to be a clean mathematical way to describe the class of functions well-approximated by a forest.","title":"Rate of convergence: random forest"},{"location":"ml-methods/#other-statistical-properties_1","text":"Pointwise asymptotic normality : Wager and Athey ( 2018 )","title":"Other statistical properties"},{"location":"ml-methods/#simulation-study","text":"Partially linear model DGP : $x_i \\in \\R^p$ with $x_{ij} \\sim U(0,1)$ $d_i = m(x_i) + v_i$ $y_i = d_i\\theta + f(x_i) + \\epsilon_i$ $m()$, $f()$ either linear or step functions Estimate by OLS, Lasso, and random forest Lasso & random forest use orthogonal moments \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0 The point of this simulation is to see whether the slower convergence rate of random forests matters for the semiparametric problems we have in mind. Our theory results suggest that estimates of $\\theta$ using random forests with $p>2$ will be asymptotically biased. Specifically, the term d_n = \\En[(m(x_i) - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] will be $O_p(n^{\\frac{-2}{2+p}})$, so $\\sqrt{n} d_n = O_p(n^{\\frac{p-2}{2(2+p)}})$. However, this calculation is only an upper bound on $d_n$. For a given DGP, $d_n$ might be smaller. In this simulation exercise, when $m()$ and $f()$ are linear, they are not easy to approximate by a regression tree, so I expect the random forest estimator to behave relatively poorly. OLS and Lasso on the other hand will do very well, and are included mainly as benchmarks. When $m()$ and $f()$ are step functions (specifically, $f(x) = m(x) = \\sum_{j=1}^p 1(x_{j}>1/2)$), I thought they would be well approximated by a regression tree (and random forest). For OLS and Lasso, $x$ is still only included linearly in the estimation, so those estimators will do poorly in the step function DGP. Throughout the simulation $p$ is much less than $n$, so Lasso and OLS will generally give very similar results. rm(list=ls()) p <- 4 # number of x's mu.linear <- function(x) x%*%rep(1,p) m.linear <- function(x) x%*%rep(2,p) mu.step <- function(x) (x>0.5)%*%rep(1,p) m.step <- function(x) (x>0.5)%*%rep(2,p) theta <- 1 simulate <- function(n,p,mu,m) { theta <- 1 x <- matrix(runif(n*p), ncol=p) d <- m(x) + rnorm(n) y <- theta*d + mu(x) + rnorm(n) data.frame(y=y,d=d,x=x) } library(grf) library(hdm) df <- simulate(100,p,mu.linear,m.linear) mrfparams <- NULL murfparams <- NULL n.save <- NULL partial.linear.rf <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] if (is.null(mrfparams) || n.save!=nrow(df)) { # to save time, we only tune once per cluster worker and data set # size cat(\"tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=1000, tune.parameters=TRUE) mrfparams <<- m.rf$tunable.params mu.rf <- regression_forest(df[,x.names], df$y, num.trees=1000, tune.parameters=TRUE) n.save <<- nrow(df) murfparams <<- mu.rf$tunable.params } else { cat(\"not tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(mrfparams[\"min.node.size\"]), alpha = as.numeric(mrfparams[\"alpha\"]), imbalance.penalty=as.numeric(mrfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(mrfparams[\"sample.fraction\"]), mtry=as.numeric(mrfparams[\"mtry\"])) mu.rf <- regression_forest(df[,x.names], df$y, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(murfparams[\"min.node.size\"]), alpha = as.numeric(murfparams[\"alpha\"]), imbalance.penalty=as.numeric(murfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(murfparams[\"sample.fraction\"]), mtry=as.numeric(murfparams[\"mtry\"])) } vhat <- df$d - predict(m.rf)$predictions ehat <- df$y - predict(mu.rf)$predictions lm(ehat ~ vhat) } ## Manual sample splitting --- this turns out to be unneccessary. The ## default behavior of predict.regression_forest is to return ## predictions on the training data using only trees that were not fit ## on each observation. In other words, regression_forest already does ## the sample splitting for us. ## ## rf.tuneOnce <- function(x.names, y.name) { ## parms <- NULL ## function(df) { ## if (is.null(parms)) { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=500, ## tune.parameters=TRUE) ## parms <<- rf$tunable.params ## rf ## } else { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=200, ## tune.parameters=FALSE, ## honesty=FALSE, ## min.node.size = ## as.numeric(parms[\"min.node.size\"]), ## alpha = as.numeric(parms[\"alpha\"]), ## imbalance.penalty=as.numeric(parms[\"imbalance.penalty\"]), ## sample.fraction = as.numeric(parms[\"sample.fraction\"]), ## mtry=as.numeric(parms[\"mtry\"])) ## } ## } ## } ## n.save.split <- NULL ## m.hat.rf <- NULL ## mu.hat.rf <- NULL ## partial.linear.split.rf <- function(df , splits=3) { ## x.names <- names(df)[grep(\"x.\",names(df))] ## if (is.null(n.save.split) || n.save.split != nrow(df)) { ## n.save.split <<- nrow(df) ## m.hat.rf <<- rf.tuneOnce(x.names,\"d\") ## mu.hat.rf <<- rf.tuneOnce(x.names,\"y\") ## } ## df$group <- sample(1:splits, nrow(df), replace=TRUE) ## vhat <- df$d ## ehat <- df$y ## for(g in 1:splits) { ## sdf <- subset(df, group!=g) ## m <- m.hat.rf(sdf) ## mu <- mu.hat.rf(sdf) ## vhat[df$group==g] <- df$d[df$group==g] - ## predict(m, newx=df[df$group==g,x.names])$predictions ## ehat[df$group==g] <- df$y[df$group==g] - ## predict(mu, newx=df[df$group==g,x.names])$predictions ## } ## lm(ehat ~ vhat) ## } partial.linear.lasso <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) rlassoEffects(fmla, data=df, I = ~ d) } #summary(partial.linear.lasso(df)) # simulate a bunch of times in parallel simulations <- 500 # number of simulations library(parallel) cl <- makeCluster(detectCores()/2) # change as you see fit clusterEvalQ(cl,library(hdm)) clusterEvalQ(cl,library(grf)) # R Socket cluster spawns new R sessions with empty environments, we # need to make sure they load any needed libraries and have access to # things from the main environment that they use design <- c(\"linear\") #,\"step\") sim.df <- data.frame() start.time <- Sys.time() for (d in design) { if (d==\"linear\") { m <- m.linear mu <- mu.linear } else { m <- m.step mu <- mu.step } for (p in c(2,4,6,8)) { for (n in c(100, 200, 400, 800, 1600)) { clusterExport(cl,c(\"simulate\",\"partial.linear.lasso\", \"partial.linear.rf\",\"p\",\"mu\",\"m\", \"mrfparams\",\"murfparams\", \"n.save\")) # \"partial.linear.split.rf\", \"n.save.split\", # \"m.hat.rf\",\"mu.hat.rf\",\"rf.tuneOnce\")) thetas <- parSapply(cl, rep(n,simulations), function(n) { df <- simulate(n, p, mu, m) x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) c(lm(fmla,data=df)$coef[2], partial.linear.rf(df)$coef[2], #partial.linear.split.rf(df)$coef[2], partial.linear.lasso(df)$coefficients) } ) tmp <- (data.frame(t(thetas)) - 1)*sqrt(n) names(tmp) <- c(\"OLS\",\"Random.Forest\",\"Lasso\") tmp$n <- n tmp$p <- p tmp$design <- d sim.df <- rbind(sim.df, tmp) cat(\"finished sample size \",n,\"\\n\") cat(\"Elapsed time \", Sys.time()-start.time,\"\\n\") } cat(\"finished p = \",p,\"\\n\") } cat(\"finished design = \", d,\"\\n\") } stopCluster(cl) save(sim.df, file=\"partialLinearSim.Rdata\") library(ggplot2) library(reshape2) library(latex2exp) TeX <- latex2exp::TeX load(\"partialLinearSim.Rdata\") # see partialLinearSim.R for simulation # code df <- melt(sim.df, measure.vars=c(\"OLS\",\"Random.Forest\",\"Lasso\")) ggplot(subset(df,p==2), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(TeX('$\\\\sqrt{n}(\\\\hat{\\\\theta}-\\\\theta_0)$')) + ggtitle(\"p=2\") ggplot(subset(df,p==4), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=4\") ggplot(subset(df,p==6), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=6\") ggplot(subset(df,p==8), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=8\") Random forests do not seem to work very well in this context. Even when the functions being estimated are step functions, random forests do not produce a good estimate of $\\theta$. One caveat here is that I was not very careful about the tuning parameters for the random forests. It\u2019s possible that there exists a careful choice of tuning parameters that results in a better estimator. Research idea: create a generalization of random forests that is adaptive to the smoothness of the function being estimated. Two classic papers on adaptive regression estimators are Speckman ( 1985 ) and Donoho and Johnstone ( 1995 ). Friedberg et al. ( 2018 ) develop a local linear forest estimator. Combining their idea of using forests to form local neighborhoods with a smoothness adaptive variant of kernel or local polynomial regression should lead to a smoothness adaptive forest.","title":"Simulation study"},{"location":"ml-methods/#neural-networks","text":"Target function $f: \\R^p \\to \\R$ e.g. $f(x) = \\Er[y|x]$ Approximate with single hidden layer neural network : \\hat{f}(x) = \\sum_{j=1}^r \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j) Activation function $\\psi$ Examples: Sigmoid $\\psi(t) = 1/(1+e^{-t})$, Tanh $\\psi(t) = \\frac{e^t -e^{-t}}{e^t + e^{-t}}$, Heavyside $\\psi(t) = t 1(t\\geq 0)$ Weights $a_j$ Bias $b_j$ Able to approximate any $f$, Hornik, Stinchcombe, and White ( 1989 ) library(RSNNS) library(devtools) # download plot.nnet function from github source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r') load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) xn <- normalizeData(mod$x[,2:ncol(mod$x)]) yn <- normalizeData(mod$y) nn <- mlp(x=xn, y=yn, linOut=TRUE, size=c(10)) plot.nnet(nn, x.lab=colnames(mod$x)[2:ncol(mod$x)], y.lab=\"transProfit\")","title":"Neural Networks"},{"location":"ml-methods/#deep-neural-networks","text":"Many hidden layers $x^{(0)} = x$ $x^{(\\ell)}_j = \\psi(a_j^{(\\ell)} x^{(\\ell-1)} + b_j^{(\\ell)})$ nn <- mlp(x=xn, y=yn, linOut=TRUE, size=c(5,10,3,5, 6)) plot.nnet(nn, x.lab=colnames(mod$x)[2:ncol(mod$x)], y.lab=\"transProfit\")","title":"Deep Neural Networks"},{"location":"ml-methods/#rate-of-convergence_1","text":"Chen and White ( 1999 ) $f(x) = \\Er[y|x]$ with Fourier representation f(x) = \\int e^{i a'x} d\\sigma_f(a) where $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$ Network sieve : \\begin{align*} \\mathcal{G}_n = \\{ & g: g(x) = \\sum_{j=1}^{r_n} \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j), \\\\ & \\norm{\\beta}_1 \\leq B_n \\} \\end{align*} The setup in Chen and White ( 1999 ) is more general. They consider estimating both $f$ and its first $m$ derivatives. Here, we focus on the case of just estimating $f$. Chen and White ( 1999 ) also consider estimation of functions other than conditional expectations. The restriction on $f$ in the second bullet is used to control approximation error. The second bullet says that $f$ is the inverse Fourier transform of measure $\\sigma_f$. The bite of the restriction on $f$ comes from the requirement that $\\sigma_f$ be absolutely integral, $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$. It would be a good exercise to check whether this restriction is satisfied by some familiar types of functions. Barron ( 1993 ) first showed that neural networks approximate this class of functions well, and compares the approximation rate of neural networks to other function approximation results.","title":"Rate of convergence"},{"location":"ml-methods/#rate-of-convergence-rate-of-convergence-3","text":"Estimate \\hat{f} = \\argmin_{g \\in \\mathcal{G}_n} \\En [(y_i - g(x_i))^2] For fixed $p$, if $r_n^{2(1+1/(1+p))} \\log(r_n) = O(n)$, $B_n \\geq$ some constant \\Er[(\\hat{f}(x) - f(x))^2] = O\\left((n/\\log(n))^{\\frac{-(1 + 2/(p+1))} {2(1+1/(p+1))}}\\right) It is easy to see that regardless of $p$, $\\sqrt{n}\\Er[(\\hat{f}(x) - f(x))^2] \\to 0$. Therefore, neural networks would be suitable for estimating the nuisance functions in our examples above. There is a gap between applied use of neural networks and this statistical theory. These rate results are for networks with a single hidden layer. In prediction applications, the best performance is typically achieved by deep neural networks with many hidden layers. Intuitively, multiple hidden layers should do at least as well as a single hidden layer. There are some recent theoretical results that formalize this intuition. FIXME: ADD CITATIONS.","title":"Rate of convergence [rate-of-convergence-3]"},{"location":"ml-methods/#simulation-study_1","text":"Same setup as for random forests earlier Partially linear model DGP : $x_i \\in \\R^p$ with $x_{ij} \\sim U(0,1)$ $d_i = m(x_i) + v_i$ $y_i = d_i\\theta + f(x_i) + \\epsilon_i$ $m()$, $f()$ either linear or step functions Estimate by OLS, Neural network with & without cross-fitting Using orthogonal moments \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0 rm(list=ls()) p <- 4 # number of x's mu.linear <- function(x) x%*%rep(1,p) m.linear <- function(x) x%*%rep(2,p) mu.step <- function(x) (x>0.5)%*%rep(1,p) m.step <- function(x) (x>0.5)%*%rep(2,p) theta <- 1 simulate <- function(n,p,mu,m) { theta <- 1 x <- matrix(runif(n*p), ncol=p) d <- m(x) + rnorm(n) y <- theta*d + mu(x) + rnorm(n) data.frame(y=y,d=d,x=x) } library(grf) library(hdm) df <- simulate(100,p,mu.linear,m.linear) mrfparams <- NULL murfparams <- NULL n.save <- NULL partial.linear.rf <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] if (is.null(mrfparams) || n.save!=nrow(df)) { # to save time, we only tune once per cluster worker and data set # size cat(\"tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=1000, tune.parameters=TRUE) mrfparams <<- m.rf$tunable.params mu.rf <- regression_forest(df[,x.names], df$y, num.trees=1000, tune.parameters=TRUE) n.save <<- nrow(df) murfparams <<- mu.rf$tunable.params } else { cat(\"not tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(mrfparams[\"min.node.size\"]), alpha = as.numeric(mrfparams[\"alpha\"]), imbalance.penalty=as.numeric(mrfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(mrfparams[\"sample.fraction\"]), mtry=as.numeric(mrfparams[\"mtry\"])) mu.rf <- regression_forest(df[,x.names], df$y, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(murfparams[\"min.node.size\"]), alpha = as.numeric(murfparams[\"alpha\"]), imbalance.penalty=as.numeric(murfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(murfparams[\"sample.fraction\"]), mtry=as.numeric(murfparams[\"mtry\"])) } vhat <- df$d - predict(m.rf)$predictions ehat <- df$y - predict(mu.rf)$predictions lm(ehat ~ vhat) } ## Manual sample splitting --- this turns out to be unneccessary. The ## default behavior of predict.regression_forest is to return ## predictions on the training data using only trees that were not fit ## on each observation. In other words, regression_forest already does ## the sample splitting for us. ## ## rf.tuneOnce <- function(x.names, y.name) { ## parms <- NULL ## function(df) { ## if (is.null(parms)) { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=500, ## tune.parameters=TRUE) ## parms <<- rf$tunable.params ## rf ## } else { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=200, ## tune.parameters=FALSE, ## honesty=FALSE, ## min.node.size = ## as.numeric(parms[\"min.node.size\"]), ## alpha = as.numeric(parms[\"alpha\"]), ## imbalance.penalty=as.numeric(parms[\"imbalance.penalty\"]), ## sample.fraction = as.numeric(parms[\"sample.fraction\"]), ## mtry=as.numeric(parms[\"mtry\"])) ## } ## } ## } ## n.save.split <- NULL ## m.hat.rf <- NULL ## mu.hat.rf <- NULL ## partial.linear.split.rf <- function(df , splits=3) { ## x.names <- names(df)[grep(\"x.\",names(df))] ## if (is.null(n.save.split) || n.save.split != nrow(df)) { ## n.save.split <<- nrow(df) ## m.hat.rf <<- rf.tuneOnce(x.names,\"d\") ## mu.hat.rf <<- rf.tuneOnce(x.names,\"y\") ## } ## df$group <- sample(1:splits, nrow(df), replace=TRUE) ## vhat <- df$d ## ehat <- df$y ## for(g in 1:splits) { ## sdf <- subset(df, group!=g) ## m <- m.hat.rf(sdf) ## mu <- mu.hat.rf(sdf) ## vhat[df$group==g] <- df$d[df$group==g] - ## predict(m, newx=df[df$group==g,x.names])$predictions ## ehat[df$group==g] <- df$y[df$group==g] - ## predict(mu, newx=df[df$group==g,x.names])$predictions ## } ## lm(ehat ~ vhat) ## } partial.linear.lasso <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) rlassoEffects(fmla, data=df, I = ~ d) } #summary(partial.linear.lasso(df)) # simulate a bunch of times in parallel simulations <- 500 # number of simulations library(parallel) cl <- makeCluster(detectCores()/2) # change as you see fit clusterEvalQ(cl,library(hdm)) clusterEvalQ(cl,library(grf)) # R Socket cluster spawns new R sessions with empty environments, we # need to make sure they load any needed libraries and have access to # things from the main environment that they use design <- c(\"linear\") #,\"step\") sim.df <- data.frame() start.time <- Sys.time() for (d in design) { if (d==\"linear\") { m <- m.linear mu <- mu.linear } else { m <- m.step mu <- mu.step } for (p in c(2,4,6,8)) { for (n in c(100, 200, 400, 800, 1600)) { clusterExport(cl,c(\"simulate\",\"partial.linear.lasso\", \"partial.linear.rf\",\"p\",\"mu\",\"m\", \"mrfparams\",\"murfparams\", \"n.save\")) # \"partial.linear.split.rf\", \"n.save.split\", # \"m.hat.rf\",\"mu.hat.rf\",\"rf.tuneOnce\")) thetas <- parSapply(cl, rep(n,simulations), function(n) { df <- simulate(n, p, mu, m) x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) c(lm(fmla,data=df)$coef[2], partial.linear.rf(df)$coef[2], #partial.linear.split.rf(df)$coef[2], partial.linear.lasso(df)$coefficients) } ) tmp <- (data.frame(t(thetas)) - 1)*sqrt(n) names(tmp) <- c(\"OLS\",\"Random.Forest\",\"Lasso\") tmp$n <- n tmp$p <- p tmp$design <- d sim.df <- rbind(sim.df, tmp) cat(\"finished sample size \",n,\"\\n\") cat(\"Elapsed time \", Sys.time()-start.time,\"\\n\") } cat(\"finished p = \",p,\"\\n\") } cat(\"finished design = \", d,\"\\n\") } stopCluster(cl) save(sim.df, file=\"partialLinearSim.Rdata\") library(ggplot2) library(reshape2) library(latex2exp) TeX <- latex2exp::TeX load(\"partialLinearSimNet.Rdata\") # see partialLinearSim.R for simulation # code df <- melt(sim.df, measure.vars=names(sim.df)[1:3]) ggplot(subset(df,p==2), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(TeX('$\\\\sqrt{n}(\\\\hat{\\\\theta}-\\\\theta_0)$')) + ggtitle(\"p=2\") ggplot(subset(df,p==4), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=4\") ggplot(subset(df,p==6), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=6\") ggplot(subset(df,p==8), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=8\") The performance of the neural network estimator appears okay, but not outstanding in these simulations. In the linear model, the neural network estimator performs slightly worse than OLS. In the step function model, the neural network estimator performs slight better than the misspecified OLS, but neither appears to work well. In both cases, it appears that the neural network estimator produces occassional outliers. I believe that this is related to the fact that the minimization problem defining the neural network is actually very difficult to solve. In the simulation above, I suspect the outlying estimates are due to minimization problems. In the simulations, I simply set $r_n = n^{1/(2(1+1/(1+p)))}$. It\u2019s likely that a more careful choice of $r_n$, perhaps using cross-validation, would give better results.","title":"Simulation Study"},{"location":"ml-methods/#bibliography","text":"Barron, A. R. 1993. \u201cUniversal Approximation Bounds for Superpositions of a Sigmoidal Function.\u201d IEEE Transactions on Information Theory 39 (3): 930\u201345. https://doi.org/10.1109/18.256500 . Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen. 2012. \u201cSparse Models and Methods for Optimal Instruments with an Application to Eminent Domain.\u201d Econometrica 80 (6): 2369\u20132429. https://doi.org/10.3982/ECTA9626 . Belloni, Alexandre, and Victor Chernozhukov. 2011. \u201cHigh Dimensional Sparse Econometric Models: An Introduction.\u201d In Inverse Problems and High-Dimensional Estimation: Stats in the Ch\u00e2teau Summer School, August 31 - September 4, 2009 , edited by Pierre Alquier, Eric Gautier, and Gilles Stoltz, 121\u201356. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-19989-9_3 . Biau, G\u00e9rard. 2012. \u201cAnalysis of a Random Forests Model.\u201d Journal of Machine Learning Research 13 (Apr): 1063\u201395. http://www.jmlr.org/papers/v13/biau12a.html . Caner, Mehmet, and Anders Bredahl Kock. 2018. \u201cAsymptotically Honest Confidence Regions for High Dimensional Parameters by the Desparsified Conservative Lasso.\u201d Journal of Econometrics 203 (1): 143\u201368. https://doi.org/https://doi.org/10.1016/j.jeconom.2017.11.005 . Chen, Xiaohong, and H. White. 1999. \u201cImproved Rates and Asymptotic Normality for Nonparametric Neural Network Estimators.\u201d IEEE Transactions on Information Theory 45 (2): 682\u201391. https://doi.org/10.1109/18.749011 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. \u201chdm: High-Dimensional Metrics.\u201d R Journal 8 (2): 185\u201399. https://journal.r-project.org/archive/2016/RJ-2016-040/index.html . Chetverikov, Denis, Zhipeng Liao, and Victor Chernozhukov. 2016. \u201cOn Cross-Validated Lasso.\u201d https://arxiv.org/abs/1605.02214 . Donoho, David L., and Iain M. Johnstone. 1995. \u201cAdapting to Unknown Smoothness via Wavelet Shrinkage.\u201d Journal of the American Statistical Association 90 (432): 1200\u20131224. http://www.jstor.org/stable/2291512 . Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference . Vol. 5. Cambridge University Press. . Friedberg, Rina, Julie Tibshirani, Susan Athey, and Stefan Wager. 2018. \u201cLocal Linear Forests.\u201d https://arxiv.org/abs/1807.11408 . Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning . Springer series in statistics. . Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. \u201cMultilayer Feedforward Networks Are Universal Approximators.\u201d Neural Networks 2 (5): 359\u201366. https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8 . James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning . Vol. 112. Springer. . Lee, Jason D., Dennis L. Sun, Yuekai Sun, and Jonathan E. Taylor. 2016. \u201cExact Post-Selection Inference, with Application to the Lasso.\u201d Ann. Statist. 44 (3): 907\u201327. https://doi.org/10.1214/15-AOS1371 . Li, Ker-Chau. 1989. \u201cHonest Confidence Regions for Nonparametric Regression.\u201d Ann. Statist. 17 (3): 1001\u20138. https://doi.org/10.1214/aos/1176347253 . Mullainathan, Sendhil, and Jann Spiess. 2017. \u201cMachine Learning: An Applied Econometric Approach.\u201d Journal of Economic Perspectives 31 (2): 87\u2013106. https://doi.org/10.1257/jep.31.2.87 . Nickl, Richard, and Sara van de Geer. 2013. \u201cConfidence Sets in Sparse Regression.\u201d Ann. Statist. 41 (6): 2852\u201376. https://doi.org/10.1214/13-AOS1170 . Speckman, Paul. 1985. \u201cSpline Smoothing and Optimal Rates of Convergence in Nonparametric Regression Models.\u201d The Annals of Statistics 13 (3): 970\u201383. http://www.jstor.org/stable/2241119 . Stone, Charles J. 1982. \u201cOptimal Global Rates of Convergence for Nonparametric Regression.\u201d The Annals of Statistics 10 (4): 1040\u201353. http://www.jstor.org/stable/2240707 . Taylor, Jonathan, and Robert Tibshirani. 2017. \u201cPost-Selection Inference for -Penalized Likelihood Models.\u201d Canadian Journal of Statistics 46 (1): 41\u201361. https://doi.org/10.1002/cjs.11313 . Wager, Stefan, and Susan Athey. 2018. \u201cEstimation and Inference of Heterogeneous Treatment Effects Using Random Forests.\u201d Journal of the American Statistical Association 0 (0): 1\u201315. https://doi.org/10.1080/01621459.2017.1319839 . Wager, Stefan, and Guenther Walther. 2015. \u201cAdaptive Concentration of Regression Trees, with Application to Random Forests.\u201d https://arxiv.org/abs/1503.06388 .","title":"Bibliography"},{"location":"mlExamplePKH/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 Background \u00b6 Program Keluarga Harapan : pilot conditional cash transfer program in Indonesia Alatas et al. ( 2011 ), Triyana ( 2016 ) Conditional cash transfer: receive cash if Expectant women: 4 prenatal visits, iron supplement, delivery by doctor or midwife, 2 postnatal visits Children under 5: weighed monthly, vaccinated, vitamin A Quarterly transfer of 600,000-2,200,000 rupiah (\\$60 - \\$220) depending on household composition (15-20% of quaterly consumption) Randomized at subdistrict level : want to capture supply side effects that would occur if policy implemented everywhere Baseline characteristics \u00b6 if (!file.exists(\"Data_AEJPol-2014-0048/data_final/price_women.dta\")) { stop(\"Please download the data from https://www.aeaweb.org/articles?id=10.1257/pol.20140048 and unzip it in the current directory\") } if (!require(foreign)) install.packages(\"foreign\") library(foreign) all.data <- read.dta(\"Data_AEJPol-2014-0048/data_final/price_women.dta\") # Variables we will use vars <- c(\"rid_panel\",\"prov\",\"Location_ID\",\"dist\", \"wave\", \"edu\",\"agecat\",\"log_xp_percap\",#\"hh_land\",\"hh_home\", \"rhr031\", \"rhr032\", \"rhr034\", \"rhr036\",# \"hh_xp\", \"death_ch\",\"bw\",\"bw_low\", \"birthfac\", \"good_assisted_delivery\", \"dummy_A\", \"dummy_B\", \"dummy_D\", \"delivery_fees\", \"delivery_fees_prof\", \"delivery_fees_doc\", \"delivery_fees_midw\", \"delivery_fees_trad\", \"control_pkh\", \"pkh_kec_ever\", \"pkh_ever\", \"bw_kec\", \"bw_low_kec\", \"death_ch_kec\", \"dummy_A_base\", \"dummy_B_base\", \"dummy_D_base\", \"bid_pkh1\", \"bid_pkh1_base\" , \"delivery_fees_top\", \"delivery_fees_prof_top\", \"delivery_fees_doc_top\", \"delivery_fees_midw_top\", \"delivery_fees_trad_top\", \"delivery_fees_top_base\", \"delivery_fees_prof_top_base\", \"delivery_fees_doc_top_base\", \"delivery_fees_midw_top_base\", \"delivery_fees_trad_top_base\", names(all.data)[grep(\"^hh_\",names(all.data))], \"tv\", \"parabola\" , \"fridge\" , \"motorbike\", \"car\" , \"pig\" , \"goat\" , \"cow\" , \"horse\" ) pw <- all.data[,vars] ## Rename some variables names(pw)[names(pw)==\"dummy_A\"] <- \"doctor_birth\" names(pw)[names(pw)==\"dummy_A_base\"] <- \"doctor_birth_base\" names(pw)[names(pw)==\"dummy_B\"] <- \"midwife_birth\" names(pw)[names(pw)==\"dummy_B_base\"] <- \"midwife_birth_base\" names(pw)[names(pw)==\"dummy_D\"] <- \"traditional_birth\" names(pw)[names(pw)==\"dummy_D_base\"] <- \"traditional_birth_base\" # Set household variables to their pretreatment (wave=1) values for (v in c(\"edu\",\"agecat\",\"log_xp_percap\", \"rhr031\", \"rhr032\", \"rhr034\", \"rhr036\", names(pw)[grep(\"^hh_\",names(pw))], \"tv\",\"parabola\",\"fridge\",\"motorbike\",\"car\",\"pig\",\"goat\",\"cow\",\"horse\")) { temp <- ifelse(pw$wave==1,pw[,v],NA) temp0 <- ave(temp, pw$rid_panel, FUN=function(x) max(x,na.rm=TRUE)) pw[,v] <- ifelse(is.finite(temp0), temp0, pw[,v]) } pw$delivery_fees_trad_top[is.na(pw$delivery_fees_trad_top) & !is.na(pw$delivery_fees_midw_top)] <- 0 adf <- aggregate(subset(pw,wave==1), by=list(subset(pw,wave==1)$control_pkh), FUN=function(x) mean(x,na.rm=TRUE)) adf ## Group.1 rid_panel prov Location_ID dist wave edu agecat ## 1 0 NA NA NA NA 1 15.50180 3.405191 ## 2 1 NA NA NA NA 1 14.59808 3.428927 ## log_xp_percap rhr031 rhr032 rhr034 rhr036 death_ch ## 1 11.98384 0.3907714 0.5313627 0.4354722 0.4826965 0.01333814 ## 2 12.00990 0.4068941 0.5458422 0.4427861 0.4765458 0.01137171 ## bw bw_low birthfac good_assisted_delivery doctor_birth ## 1 3166.346 0.08163265 0.4353201 0.6392936 0.07902870 ## 2 3180.763 0.07678883 0.4402434 0.6232073 0.08778792 ## midwife_birth traditional_birth delivery_fees delivery_fees_prof ## 1 0.5876380 0.4189845 284483.6 243728.8 ## 2 0.5619296 0.4315515 298738.9 256254.9 ## delivery_fees_doc delivery_fees_midw delivery_fees_trad control_pkh ## 1 82076.82 178308.9 39173.57 0 ## 2 102721.64 176275.7 40711.49 1 ## pkh_kec_ever pkh_ever bw_kec bw_low_kec death_ch_kec doctor_birth_base ## 1 0 0 3165.103 0.08319263 0.01333814 0.07897035 ## 2 0 0 3180.449 0.08287012 0.01137171 0.08718138 ## midwife_birth_base traditional_birth_base bid_pkh1 bid_pkh1_base ## 1 0.5848753 0.4195088 3.574443e-02 3.574443e-02 ## 2 0.5585186 0.4353818 -1.469066e-17 -2.999183e-18 ## delivery_fees_top delivery_fees_prof_top delivery_fees_doc_top ## 1 274278.3 233523.5 64174.41 ## 2 278295.7 235811.7 72279.24 ## delivery_fees_midw_top delivery_fees_trad_top delivery_fees_top_base ## 1 167942.7 36862.44 273320.4 ## 2 162837.3 37162.07 275470.0 ## delivery_fees_prof_top_base delivery_fees_doc_top_base ## 1 232340.5 63631.20 ## 2 232639.0 70302.13 ## delivery_fees_midw_top_base delivery_fees_trad_top_base hh_serial_no_w1 ## 1 166914.5 37767.86 2.898702 ## 2 162082.5 38660.42 2.871713 ## hh_survey_date_w1 hh_adult hh_5_15 hh_u5 hh_serial_no_w2 ## 1 17368.93 3.426614 1.858130 1.631684 NaN ## 2 17367.48 3.363539 1.887016 1.638240 NaN ## hh_serial_no_w3 hh_u2 hh_3_6 hh_7_15 hh_phone hh_rf_tile ## 1 NaN 1.231982 1.496021 2.336163 0.1167988 0.7119683 ## 2 NaN 1.232099 1.467532 2.340625 0.1122957 0.7157072 ## hh_rf_shingle hh_rf_fiber hh_rf_oth hh_wall_plaster hh_wall_brick ## 1 0.2025955 0.1304975 0.04578226 0.3183129 0.1503244 ## 2 0.1943852 0.1339730 0.04442075 0.3471926 0.1368159 ## hh_wall_wood hh_wall_fiber hh_wall_oth hh_fl_tile hh_fl_plaster ## 1 0.2599135 0.3604903 0.03208363 0.2058399 0.3713050 ## 2 0.2356077 0.3681592 0.03020611 0.2160625 0.3901919 ## hh_fl_wood hh_fl_dirt hh_fl_oth hh_drink_pam hh_drink_mechwell ## 1 0.1398702 0.3738284 0.000000000 0.1528479 0.2025955 ## 2 0.1346837 0.3471926 0.000355366 0.1464108 0.2174840 ## hh_drink_well hh_drink_spring hh_drink_oth hh_drinkhome hh_drinkdist ## 1 0.4913482 0.1870944 0.08723864 0.2826244 1052.188 ## 2 0.4626866 0.1915423 0.09985785 0.2782516 1064.803 ## hh_waterdrink hh_water_pam hh_water_mechwell hh_water_well ## 1 0.8280461 0.1171593 0.2004326 0.4783706 ## 2 0.8312011 0.1055437 0.2235252 0.4552239 ## hh_water_spring hh_water_river hh_water_oth hh_waterhome hh_waterdist ## 1 0.1856525 0.09841384 0.04109589 0.05839942 610.1139 ## 2 0.1847903 0.09630419 0.04157783 0.05934613 634.7447 ## hh_toilet_own hh_toilet_pub hh_toilet_none hh_toilet_1 hh_toilet_2 ## 1 0.4340303 0.1784427 0.4480894 0.2811824 0.1049027 ## 2 0.4648188 0.1844350 0.4097370 0.3280028 0.1080313 ## hh_toilet_3 hh_waste_tank hh_waste_hole hh_waste_river hh_waste_field ## 1 0.2505407 0.2606345 0.2487383 0.3695025 0.2119683 ## 2 0.2313433 0.3120114 0.2341862 0.3454158 0.1968728 ## hh_waste_oth hh_listrik hh_pln hh_kitchen hh_cook_wood ## 1 0 0.8601298 0.8417448 0.8936554 0.8136265 ## 2 0 0.8681592 0.8464819 0.9033404 0.7782516 ## hh_cook_kerosene hh_cook_gas hh_cook_oth hh_kec hh_land hh_home ## 1 0.2105263 0.03640952 0 9400.184 0.3734679 0.8846431 ## 2 0.2469794 0.03375977 0 9618.802 0.3848614 0.8749112 ## hh_land_miss hh_home_miss hh_xp hh_xp_all tv parabola ## 1 0 0.03028118 3.024874 2.601298 0.5313627 0.03893295 ## 2 0 0.02949538 3.096304 2.676617 0.5458422 0.04015636 ## fridge motorbike car pig goat cow horse ## 1 0.05803893 0.2011536 0.03496756 0.1196828 0.1456381 0.1121125 0.03640952 ## 2 0.05685856 0.1986496 0.03269367 0.1364606 0.1552950 0.1186923 0.03802416 Main results \u00b6 Delivery attendant usage \u00b6 library(lfe) lhs <- \"pkh_kec_ever + doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | 0 | Location_ID\" itt <- list(felm(as.formula(paste(\"doctor_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"midwife_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"traditional_birth ~\",lhs)),data=pw)) lhs <- \"doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | (pkh_ever ~ pkh_kec_ever) | Location_ID\" iv <- list(felm(as.formula(paste(\"doctor_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"midwife_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"traditional_birth ~\",lhs)),data=pw)) library(stargazer) stargazer(itt, type=\"html\", title=\"ITT estimates\", dep.var.labels = c(\"Doctor\",\"Midwife\",\"Traditional\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) ITT estimates Dependent variable: Doctor Midwife Traditional (1) (2) (3) pkh_kec_ever 0.043 *** 0.094 *** -0.090 *** (0.012) (0.017) (0.016) Observations 6,629 6,629 6,629 Delivery attendant usage [delivery-attendant-usage-1] \u00b6 stargazer(iv, type=\"html\", title=\"IV effect of program participation\", dep.var.labels = c(\"Doctor\",\"Midwife\",\"Traditional\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) IV effect of program participation Dependent variable: Doctor Midwife Traditional (1) (2) (3) pkh_ever(fit) 0.091 *** 0.198 *** -0.189 *** (0.025) (0.036) (0.035) Observations 6,629 6,629 6,629 Health outcomes \u00b6 library(lfe) lhs <- \"pkh_kec_ever + doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | 0 | Location_ID\" itt <- list(felm(as.formula(paste(\"death_ch ~\",lhs)),data=pw), felm(as.formula(paste(\"bw ~\",lhs)),data=pw), felm(as.formula(paste(\"bw_low ~\",lhs)),data=pw)) lhs <- \"doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | (pkh_ever ~ pkh_kec_ever) | Location_ID\" iv <- list(felm(as.formula(paste(\"death_ch ~\",lhs)),data=pw), felm(as.formula(paste(\"bw ~\",lhs)),data=pw), felm(as.formula(paste(\"bw_low ~\",lhs)),data=pw)) library(stargazer) stargazer(itt, type=\"html\", title=\"ITT estimates\", dep.var.labels = c(\"Infant mortality\",\"Birthweight\",\"Low birthweight\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) ITT estimates Dependent variable: Infant mortality Birthweight Low birthweight (1) (2) (3) pkh_kec_ever 0.005 -5.559 0.017 (0.004) (23.805) (0.012) Observations 8,303 4,988 4,988 Health outcomes [health-outcomes-1] \u00b6 stargazer(iv, type=\"html\", title=\"IV effect of program participation\", dep.var.labels = c(\"Infant mortality\",\"Birthweight\",\"Lowbirthweigt\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) IV effect of program participation Dependent variable: Infant mortality Birthweight Lowbirthweigt (1) (2) (3) pkh_ever(fit) 0.011 -12.674 0.039 (0.008) (54.269) (0.026) Observations 8,303 4,988 4,988 Exploring heterogeneity \u00b6 Machine learning as proxy \u00b6 Generic machine learning approach of Chernozhukov et al. ( 2018 ) Estimate machine learning proxies for $B(x) = \\Er[y(0)|x]$ and $S(x) = \\Er[y(1) - y(0) |x]$ Use proxies to : Estimate best linear projection on true $\\Er[y(1) - y(0)|x]$ Estimate $\\Er[y(1) - y(0) | groups]$ Heterogeneity in CATE for Birthweight \u00b6 ## Function for Generic machine learning of Chernozhukov, Demirer, Duflo, & Fernandez-Val (2018) genericML <- function(x,y,treat, fit.function, predict.function, n.split=10, n.group=5, clusterid=NULL) { if (!is.null(clusterid)) require(sandwich) blp <- matrix(NA, nrow=n.split, ncol=2) blp.se <- blp gate <- matrix(NA, nrow=n.split, ncol=n.group) gate.se <- gate baseline <- matrix(NA, nrow=nrow(x), ncol=n.split) cate <- matrix(NA, nrow=nrow(x), ncol=n.split) Lambda <- matrix(NA, nrow=n.split, ncol=2) for(i in 1:n.split) { main <- runif(nrow(x))>0.5 fit1 <- fit.function(x[!main & treat==1,], y[!main & treat==1]) fit0 <- fit.function(x[!main & treat==0,], y[!main & treat==0]) B <- as.vector(predict.function(fit0,x)) S <- as.vector(predict.function(fit1,x)) - B baseline[,i] <- B cate[,i] <- S ES <- mean(S) ## BLP # assume P(treat|x) = P(treat) = mean(treat) p <- mean(treat) df <- data.frame(y, B, treat, S, main) reg <- lm(y ~ B + I(treat-p) + I((treat-p)*(S-ES)), data=subset(df, main)) blp[i,] <- reg$coef[3:4] if (is.null(clusterid)) blp.se[i,] <- sqrt(diag(vcovHC(reg))[3:4]) else blp.se[i,] <- sqrt(diag(vcovCL(reg, clusterid[main]))[3:4]) Lambda[i,1] <- reg$coefficient[4]^2*var(S) ## GATES cut <- quantile(S, probs=seq(0,1,length.out=(n.group+1))) cut[n.group+1] <- cut[n.group+1] + 1 for(k in 1:n.group) { df[,sprintf(\"G.%d\",k)] <- (cut[k]<=S & S<cut[k+1]) } greg <- lm(as.formula(paste(c(\"y ~ B \", sprintf(\"I((treat-p)*G.%d)\",1:n.group)), collapse=\" + \")), data=subset(df,main)) gc <- grep(\"G\", names(greg$coefficients)) gate[i,] <- greg$coefficients[gc] if (is.null(clusterid)) gate.se[i,] <- sqrt(diag(vcovHC(greg))[gc]) else gate.se[i,] <- sqrt(diag(vcovCL(greg, clusterid[main]))[gc]) Lambda[i,2] <- sum(gate[i,]^2)/n.group } out <- list( gate=gate, gate.se=gate.se, blp=blp, blp.se=blp.se, Lambda=Lambda, baseline=baseline, cate=cate) } genericML.summary <- function(gml) { blp <- apply(gml$blp, 2, function(x) median(x, na.rm=TRUE)) blp.se <- apply(gml$blp.se, 2, function(x) median(x, na.rm=TRUE)) gate <- apply(gml$gate, 2, function(x) median(x, na.rm=TRUE)) gate.se <- apply(gml$gate.se, 2, function(x) median(x, na.rm=TRUE)) Lambda <- apply(gml$Lambda, 2, function(x) median(x, na.rm=TRUE)) return(list(blp=blp, blp.se=blp.se, gate=gate, gate.se=gate.se, Lambda=Lambda)) } library(glmnet) # create x matrix fmla.l <- bw ~ pkh_kec_ever + as.factor(edu)*as.factor(agecat) + log_xp_percap + hh_land + hh_home + as.factor(dist) + hh_phone + hh_rf_tile + hh_rf_shingle + hh_rf_fiber + hh_wall_plaster + hh_wall_brick + hh_wall_wood + hh_wall_fiber + hh_fl_tile + hh_fl_plaster + hh_fl_wood + hh_fl_dirt + hh_water_pam + hh_water_mechwell + hh_water_well + hh_water_spring + hh_water_river + hh_waterhome + hh_toilet_own + hh_toilet_pub + hh_toilet_none + hh_waste_tank + hh_waste_hole + hh_waste_river + hh_waste_field + hh_kitchen + hh_cook_wood + hh_cook_kerosene + hh_cook_gas + tv + fridge + motorbike + car + goat + cow + horse m <- lm(fmla.l, data=pw, x=TRUE, y=TRUE) treat <- m$x[,2] Xl <- m$x[,3:ncol(m$x)] scale <- sd(m$y) center <- mean(m$y) yl <- (m$y-center)/scale lid <- as.factor(pw[as.numeric(rownames(m$x)),]$Location_ID) gml.lasso <- genericML(Xl,m$y, treat, function(x,y) cv.glmnet(x,(y-center)/scale,alpha=1,parallel=FALSE, intercept=TRUE, nfolds=20), function(model, x) { predict(model, x, s=model$lambda.min, type=\"response\")*scale + center }, n.split=11,n.group=5, clusterid=lid) library(grf) gml.rf <- genericML(Xl,m$y, treat, function(x,y) regression_forest(x, (y-center)/scale, tune.parameters=TRUE), function(model, x) { predict(model,x)$predictions*scale + center}, n.split=11,n.group=5, clusterid=lid) library(RSNNS) gml.nn <- genericML(Xl,m$y, treat, function(x,y) mlp(x,(y-center)/scale,linOut=TRUE, size=c(10,10), learnFunc=\"SCG\"), function(model, x) { predict(model,x)*scale + center}, n.split=11,n.group=5, clusterid=lid) library(GGally) df <- data.frame(Lasso=apply(gml.lasso$cate,1,median), Forest=apply(gml.rf$cate,1,median), Neural=apply(gml.nn$cate,1,median)) ggpairs(df, lower=list(continuous=\"smooth\")) + theme_minimal() Best linear projection of CATE \u00b6 Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]} Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\beta_0 (d-P(x)) + \\beta_1 (d-P(x))(S(x) - \\Er[S(x)]) + \\epsilon $\\hat{\\beta} 0, \\hat{\\beta}_1 \\to_p \\argmin \\Er[(s_0(x) - b_0 - b_1 (S(x)-E[S(x)]))^2]$ $\\Lambda = \\beta_1^2 Var(S(x)) = corr(s_0(x),S(X))^2 Var(s_0(x))$ library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$blp[1], s$blp.se[1], s$blp[2], s$blp.se[2], s$Lambda[1]) } tbl <- cbind(colfn(gml.lasso), colfn(gml.rf), colfn(gml.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(\"ATE=b0\",\"se\",\"b1\",\"se\",\"Lambda\") kable_styling(kable(tbl, caption=\"Machine learning proxies as BLP of CATE on Birthweight\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Machine learning proxies as BLP of CATE on Birthweight Lasso Regression forest Neural network ATE=b0 -11.381 -1.751 6.185 se 31.414 32.285 32.309 b1 0.371 0.369 0.044 se 0.547 0.491 0.115 Lambda 601.155 683.726 372.192 Group average treatment effects [group-average-treatment-effects] \u00b6 Define $G_k = 1{\\ell_{k-1} \\leq S(x) \\leq \\ell_k}$ with $\\ell_k = k/5$ quantile of $S(x)$ Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\sum_k \\gamma_k (d-P(X)) 1(G_k) + \\epsilon $\\hat{\\gamma}_k \\to_p \\Er[y(1) - y(0) | G_k]$ $\\bar{\\Lambda} = \\frac{1}{K} \\sum_k \\gamma_k^2$ library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$gate[1], s$gate.se[1], s$gate[2], s$gate.se[2], s$gate[3], s$gate.se[3], s$gate[4], s$gate.se[4], s$gate[5], s$gate.se[5], s$Lambda[2]) } tbl <- cbind(colfn(gml.lasso), colfn(gml.rf), colfn(gml.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(as.vector(sapply(1:5, function(x) c(sprintf(\"GATE %d\",x),\"se\"))),\"Lambda\") kable_styling(kable(tbl, caption=\"GATE on Birthweight\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) GATE on Birthweight Lasso Regression forest Neural network GATE 1 -48.985 -3.950 19.586 se 69.276 69.646 62.126 GATE 2 21.303 -21.106 -30.948 se 74.528 60.761 67.161 GATE 3 -9.369 -44.090 -11.807 se 65.917 66.654 66.981 GATE 4 1.730 10.772 -13.668 se 66.391 67.695 66.693 GATE 5 -22.329 5.736 52.466 se 66.335 77.828 77.496 Lambda 5998.909 4268.074 3912.912 Heterogeneity in CATE on Midwife utilization \u00b6 # create x matrix mwb <- pw[as.numeric(rownames(m$x)),]$midwife_birth mw.lasso <- genericML(Xl,mwb, treat, function(x,y) cv.glmnet(x,y,family=\"binomial\", alpha=1,parallel=FALSE, intercept=TRUE, nfolds=20), function(model, x) { predict(model, x, s=model$lambda.min, type=\"response\") }, n.split=11,n.group=5, clusterid=lid) library(grf) mw.rf <- genericML(Xl,mwb, treat, function(x,y) regression_forest(x, y, tune.parameters=TRUE), function(model, x) { predict(model,x)$predictions }, n.split=11,n.group=5, clusterid=lid) library(RSNNS) mw.nn <- genericML(Xl,mwb, treat, function(x,y) mlp(x, y, linOut=FALSE, size=c(10,10), learnFunc=\"SCG\"), function(model, x) { predict(model,x) }, n.split=11,n.group=5, clusterid=lid) library(GGally) df <- data.frame(Lasso=apply(mw.lasso$cate,1,median), Forest=apply(mw.rf$cate,1,median), Neural=apply(mw.nn$cate,1,median)) ggpairs(df, lower=list(continuous=\"smooth\")) + theme_minimal() library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$blp[1], s$blp.se[1], s$blp[2], s$blp.se[2], s$Lambda[1]) } tbl <- cbind(colfn(mw.lasso), colfn(mw.rf), colfn(mw.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(\"ATE=b0\",\"se\",\"b1\",\"se\",\"Lambda\") kable_styling(kable(tbl, caption=\"Machine learning proxies as BLP of CATE on Midwife Use\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Machine learning proxies as BLP of CATE on Midwife Use Lasso Regression forest Neural network ATE=b0 0.048 0.056 0.043 se 0.023 0.023 0.024 b1 0.466 0.625 0.207 se 0.189 0.240 0.089 Lambda 0.003 0.003 0.003 library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$gate[1], s$gate.se[1], s$gate[2], s$gate.se[2], s$gate[3], s$gate.se[3], s$gate[4], s$gate.se[4], s$gate[5], s$gate.se[5], s$Lambda[2]) } tbl <- cbind(colfn(mw.lasso), colfn(mw.rf), colfn(mw.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(as.vector(sapply(1:5, function(x) c(sprintf(\"GATE %d\",x),\"se\"))),\"Lambda\") kable_styling(kable(tbl, caption=\"GATE on Midwife Use\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) GATE on Midwife Use Lasso Regression forest Neural network GATE 1 -0.002 0.030 -0.027 se 0.045 0.054 0.060 GATE 2 0.017 -0.024 0.035 se 0.050 0.054 0.052 GATE 3 0.010 0.038 0.003 se 0.054 0.050 0.050 GATE 4 0.089 0.080 0.058 se 0.053 0.046 0.050 GATE 5 0.166 0.189 0.136 se 0.053 0.052 0.052 Lambda 0.007 0.011 0.006 Covariate means by group [covariate-means-by-group] \u00b6 df <- pw[as.numeric(rownames(m$x)),] df$edu99 <- df$edu==99 df$educ <- df$edu df$educ[df$educ==99] <- NA vars <- c(\"log_xp_percap\",\"agecat\",\"educ\",\"tv\",\"goat\", \"hh_toilet_own\",\"motorbike\",\"hh_cook_wood\",\"pkh_ever\") tmp <- data.frame() groupMeans <- function(var, gml, clusterid) { n.group <- ncol(gml$gate) gate <- matrix(NA, nrow=nrow(gml$gate), ncol=ncol(gml$gate)) gate.se <- gate dat <- data.frame(y=var) for (i in 1:ncol(gml$cate)) { S <- gml$cate[,i] cut <- quantile(S, probs=seq(0,1,length.out=(n.group+1))) cut[n.group+1] <- cut[n.group+1] + 1 for(k in 1:n.group) { dat[,sprintf(\"G.%d\",k)] <- 1*(cut[k]<=S & S<cut[k+1]) } greg <- lm(as.formula(paste(c(\"y ~ -1\", sprintf(\"G.%d\",1:n.group)), collapse=\" + \")), data=dat) gc <- grep(\"G\", names(greg$coefficients)) gate[i,] <- greg$coefficients[gc] if (is.null(clusterid)) gate.se[i,] <- sqrt(diag(vcovHC(greg))[gc]) else gate.se[i,] <- sqrt(diag(vcovCL(greg,clusterid))[gc]) } return(list(mean=apply(gate, 2, function(x) median(x,na.rm=TRUE)), se = apply(gate.se, 2, function(x) median(x,na.rm=TRUE)))) } methods <- c(\"Lasso\",\"Forest\",\"Neural\") gmls <- list(mw.lasso,mw.rf,mw.nn) for(v in vars) { for (m in 1:length(methods)) { gm <- groupMeans(df[,v], gmls[[m]], lid) tmp <- rbind(tmp, data.frame(group=1:length(gm$mean),variable=v, method=methods[m], mean=gm$mean, se=gm$se)) } } library(ggplot2) fig <- ggplot(data=tmp, aes(x=group, y=mean, colour=method)) + geom_line() + geom_line(aes(y=(mean+1.96*se), colour=method), linetype=2) + geom_line(aes(y=(mean-1.96*se), colour=method), linetype=2) + facet_wrap(~ variable,scales=\"free_y\") + theme_minimal() print(fig) References \u00b6 Alatas, Vivi, Nur Cahyadi, Elisabeth Ekasari, Sarah Harmoun, Budi Hidayat, Edgar Janz, Jon Jellema, H Tuhiman, and M Wai-Poi. 2011. \u201cProgram Keluarga Harapan : Impact Evaluation of Indonesia\u2019s Pilot Household Conditional Cash Transfer Program.\u201d World Bank. http://documents.worldbank.org/curated/en/589171468266179965/Program-Keluarga-Harapan-impact-evaluation-of-Indonesias-Pilot-Household-Conditional-Cash-Transfer-Program . Chernozhukov, Victor, Mert Demirer, Esther Duflo, and Iv\u00e1n Fern\u00e1ndez-Val. 2018. \u201cGeneric Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experimentsxo.\u201d Working Paper 24678. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w24678 . Triyana, Margaret. 2016. \u201cDo Health Care Providers Respond to Demand-Side Incentives? Evidence from Indonesia.\u201d American Economic Journal: Economic Policy 8 (4): 255\u201388. https://doi.org/10.1257/pol.20140048 .","title":"Detecting heterogeneity"},{"location":"mlExamplePKH/#introduction","text":"","title":"Introduction"},{"location":"mlExamplePKH/#background","text":"Program Keluarga Harapan : pilot conditional cash transfer program in Indonesia Alatas et al. ( 2011 ), Triyana ( 2016 ) Conditional cash transfer: receive cash if Expectant women: 4 prenatal visits, iron supplement, delivery by doctor or midwife, 2 postnatal visits Children under 5: weighed monthly, vaccinated, vitamin A Quarterly transfer of 600,000-2,200,000 rupiah (\\$60 - \\$220) depending on household composition (15-20% of quaterly consumption) Randomized at subdistrict level : want to capture supply side effects that would occur if policy implemented everywhere","title":"Background"},{"location":"mlExamplePKH/#baseline-characteristics","text":"if (!file.exists(\"Data_AEJPol-2014-0048/data_final/price_women.dta\")) { stop(\"Please download the data from https://www.aeaweb.org/articles?id=10.1257/pol.20140048 and unzip it in the current directory\") } if (!require(foreign)) install.packages(\"foreign\") library(foreign) all.data <- read.dta(\"Data_AEJPol-2014-0048/data_final/price_women.dta\") # Variables we will use vars <- c(\"rid_panel\",\"prov\",\"Location_ID\",\"dist\", \"wave\", \"edu\",\"agecat\",\"log_xp_percap\",#\"hh_land\",\"hh_home\", \"rhr031\", \"rhr032\", \"rhr034\", \"rhr036\",# \"hh_xp\", \"death_ch\",\"bw\",\"bw_low\", \"birthfac\", \"good_assisted_delivery\", \"dummy_A\", \"dummy_B\", \"dummy_D\", \"delivery_fees\", \"delivery_fees_prof\", \"delivery_fees_doc\", \"delivery_fees_midw\", \"delivery_fees_trad\", \"control_pkh\", \"pkh_kec_ever\", \"pkh_ever\", \"bw_kec\", \"bw_low_kec\", \"death_ch_kec\", \"dummy_A_base\", \"dummy_B_base\", \"dummy_D_base\", \"bid_pkh1\", \"bid_pkh1_base\" , \"delivery_fees_top\", \"delivery_fees_prof_top\", \"delivery_fees_doc_top\", \"delivery_fees_midw_top\", \"delivery_fees_trad_top\", \"delivery_fees_top_base\", \"delivery_fees_prof_top_base\", \"delivery_fees_doc_top_base\", \"delivery_fees_midw_top_base\", \"delivery_fees_trad_top_base\", names(all.data)[grep(\"^hh_\",names(all.data))], \"tv\", \"parabola\" , \"fridge\" , \"motorbike\", \"car\" , \"pig\" , \"goat\" , \"cow\" , \"horse\" ) pw <- all.data[,vars] ## Rename some variables names(pw)[names(pw)==\"dummy_A\"] <- \"doctor_birth\" names(pw)[names(pw)==\"dummy_A_base\"] <- \"doctor_birth_base\" names(pw)[names(pw)==\"dummy_B\"] <- \"midwife_birth\" names(pw)[names(pw)==\"dummy_B_base\"] <- \"midwife_birth_base\" names(pw)[names(pw)==\"dummy_D\"] <- \"traditional_birth\" names(pw)[names(pw)==\"dummy_D_base\"] <- \"traditional_birth_base\" # Set household variables to their pretreatment (wave=1) values for (v in c(\"edu\",\"agecat\",\"log_xp_percap\", \"rhr031\", \"rhr032\", \"rhr034\", \"rhr036\", names(pw)[grep(\"^hh_\",names(pw))], \"tv\",\"parabola\",\"fridge\",\"motorbike\",\"car\",\"pig\",\"goat\",\"cow\",\"horse\")) { temp <- ifelse(pw$wave==1,pw[,v],NA) temp0 <- ave(temp, pw$rid_panel, FUN=function(x) max(x,na.rm=TRUE)) pw[,v] <- ifelse(is.finite(temp0), temp0, pw[,v]) } pw$delivery_fees_trad_top[is.na(pw$delivery_fees_trad_top) & !is.na(pw$delivery_fees_midw_top)] <- 0 adf <- aggregate(subset(pw,wave==1), by=list(subset(pw,wave==1)$control_pkh), FUN=function(x) mean(x,na.rm=TRUE)) adf ## Group.1 rid_panel prov Location_ID dist wave edu agecat ## 1 0 NA NA NA NA 1 15.50180 3.405191 ## 2 1 NA NA NA NA 1 14.59808 3.428927 ## log_xp_percap rhr031 rhr032 rhr034 rhr036 death_ch ## 1 11.98384 0.3907714 0.5313627 0.4354722 0.4826965 0.01333814 ## 2 12.00990 0.4068941 0.5458422 0.4427861 0.4765458 0.01137171 ## bw bw_low birthfac good_assisted_delivery doctor_birth ## 1 3166.346 0.08163265 0.4353201 0.6392936 0.07902870 ## 2 3180.763 0.07678883 0.4402434 0.6232073 0.08778792 ## midwife_birth traditional_birth delivery_fees delivery_fees_prof ## 1 0.5876380 0.4189845 284483.6 243728.8 ## 2 0.5619296 0.4315515 298738.9 256254.9 ## delivery_fees_doc delivery_fees_midw delivery_fees_trad control_pkh ## 1 82076.82 178308.9 39173.57 0 ## 2 102721.64 176275.7 40711.49 1 ## pkh_kec_ever pkh_ever bw_kec bw_low_kec death_ch_kec doctor_birth_base ## 1 0 0 3165.103 0.08319263 0.01333814 0.07897035 ## 2 0 0 3180.449 0.08287012 0.01137171 0.08718138 ## midwife_birth_base traditional_birth_base bid_pkh1 bid_pkh1_base ## 1 0.5848753 0.4195088 3.574443e-02 3.574443e-02 ## 2 0.5585186 0.4353818 -1.469066e-17 -2.999183e-18 ## delivery_fees_top delivery_fees_prof_top delivery_fees_doc_top ## 1 274278.3 233523.5 64174.41 ## 2 278295.7 235811.7 72279.24 ## delivery_fees_midw_top delivery_fees_trad_top delivery_fees_top_base ## 1 167942.7 36862.44 273320.4 ## 2 162837.3 37162.07 275470.0 ## delivery_fees_prof_top_base delivery_fees_doc_top_base ## 1 232340.5 63631.20 ## 2 232639.0 70302.13 ## delivery_fees_midw_top_base delivery_fees_trad_top_base hh_serial_no_w1 ## 1 166914.5 37767.86 2.898702 ## 2 162082.5 38660.42 2.871713 ## hh_survey_date_w1 hh_adult hh_5_15 hh_u5 hh_serial_no_w2 ## 1 17368.93 3.426614 1.858130 1.631684 NaN ## 2 17367.48 3.363539 1.887016 1.638240 NaN ## hh_serial_no_w3 hh_u2 hh_3_6 hh_7_15 hh_phone hh_rf_tile ## 1 NaN 1.231982 1.496021 2.336163 0.1167988 0.7119683 ## 2 NaN 1.232099 1.467532 2.340625 0.1122957 0.7157072 ## hh_rf_shingle hh_rf_fiber hh_rf_oth hh_wall_plaster hh_wall_brick ## 1 0.2025955 0.1304975 0.04578226 0.3183129 0.1503244 ## 2 0.1943852 0.1339730 0.04442075 0.3471926 0.1368159 ## hh_wall_wood hh_wall_fiber hh_wall_oth hh_fl_tile hh_fl_plaster ## 1 0.2599135 0.3604903 0.03208363 0.2058399 0.3713050 ## 2 0.2356077 0.3681592 0.03020611 0.2160625 0.3901919 ## hh_fl_wood hh_fl_dirt hh_fl_oth hh_drink_pam hh_drink_mechwell ## 1 0.1398702 0.3738284 0.000000000 0.1528479 0.2025955 ## 2 0.1346837 0.3471926 0.000355366 0.1464108 0.2174840 ## hh_drink_well hh_drink_spring hh_drink_oth hh_drinkhome hh_drinkdist ## 1 0.4913482 0.1870944 0.08723864 0.2826244 1052.188 ## 2 0.4626866 0.1915423 0.09985785 0.2782516 1064.803 ## hh_waterdrink hh_water_pam hh_water_mechwell hh_water_well ## 1 0.8280461 0.1171593 0.2004326 0.4783706 ## 2 0.8312011 0.1055437 0.2235252 0.4552239 ## hh_water_spring hh_water_river hh_water_oth hh_waterhome hh_waterdist ## 1 0.1856525 0.09841384 0.04109589 0.05839942 610.1139 ## 2 0.1847903 0.09630419 0.04157783 0.05934613 634.7447 ## hh_toilet_own hh_toilet_pub hh_toilet_none hh_toilet_1 hh_toilet_2 ## 1 0.4340303 0.1784427 0.4480894 0.2811824 0.1049027 ## 2 0.4648188 0.1844350 0.4097370 0.3280028 0.1080313 ## hh_toilet_3 hh_waste_tank hh_waste_hole hh_waste_river hh_waste_field ## 1 0.2505407 0.2606345 0.2487383 0.3695025 0.2119683 ## 2 0.2313433 0.3120114 0.2341862 0.3454158 0.1968728 ## hh_waste_oth hh_listrik hh_pln hh_kitchen hh_cook_wood ## 1 0 0.8601298 0.8417448 0.8936554 0.8136265 ## 2 0 0.8681592 0.8464819 0.9033404 0.7782516 ## hh_cook_kerosene hh_cook_gas hh_cook_oth hh_kec hh_land hh_home ## 1 0.2105263 0.03640952 0 9400.184 0.3734679 0.8846431 ## 2 0.2469794 0.03375977 0 9618.802 0.3848614 0.8749112 ## hh_land_miss hh_home_miss hh_xp hh_xp_all tv parabola ## 1 0 0.03028118 3.024874 2.601298 0.5313627 0.03893295 ## 2 0 0.02949538 3.096304 2.676617 0.5458422 0.04015636 ## fridge motorbike car pig goat cow horse ## 1 0.05803893 0.2011536 0.03496756 0.1196828 0.1456381 0.1121125 0.03640952 ## 2 0.05685856 0.1986496 0.03269367 0.1364606 0.1552950 0.1186923 0.03802416","title":"Baseline characteristics"},{"location":"mlExamplePKH/#main-results","text":"","title":"Main results"},{"location":"mlExamplePKH/#delivery-attendant-usage","text":"library(lfe) lhs <- \"pkh_kec_ever + doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | 0 | Location_ID\" itt <- list(felm(as.formula(paste(\"doctor_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"midwife_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"traditional_birth ~\",lhs)),data=pw)) lhs <- \"doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | (pkh_ever ~ pkh_kec_ever) | Location_ID\" iv <- list(felm(as.formula(paste(\"doctor_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"midwife_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"traditional_birth ~\",lhs)),data=pw)) library(stargazer) stargazer(itt, type=\"html\", title=\"ITT estimates\", dep.var.labels = c(\"Doctor\",\"Midwife\",\"Traditional\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) ITT estimates Dependent variable: Doctor Midwife Traditional (1) (2) (3) pkh_kec_ever 0.043 *** 0.094 *** -0.090 *** (0.012) (0.017) (0.016) Observations 6,629 6,629 6,629","title":"Delivery attendant usage"},{"location":"mlExamplePKH/#delivery-attendant-usage-delivery-attendant-usage-1","text":"stargazer(iv, type=\"html\", title=\"IV effect of program participation\", dep.var.labels = c(\"Doctor\",\"Midwife\",\"Traditional\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) IV effect of program participation Dependent variable: Doctor Midwife Traditional (1) (2) (3) pkh_ever(fit) 0.091 *** 0.198 *** -0.189 *** (0.025) (0.036) (0.035) Observations 6,629 6,629 6,629","title":"Delivery attendant usage [delivery-attendant-usage-1]"},{"location":"mlExamplePKH/#health-outcomes","text":"library(lfe) lhs <- \"pkh_kec_ever + doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | 0 | Location_ID\" itt <- list(felm(as.formula(paste(\"death_ch ~\",lhs)),data=pw), felm(as.formula(paste(\"bw ~\",lhs)),data=pw), felm(as.formula(paste(\"bw_low ~\",lhs)),data=pw)) lhs <- \"doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | (pkh_ever ~ pkh_kec_ever) | Location_ID\" iv <- list(felm(as.formula(paste(\"death_ch ~\",lhs)),data=pw), felm(as.formula(paste(\"bw ~\",lhs)),data=pw), felm(as.formula(paste(\"bw_low ~\",lhs)),data=pw)) library(stargazer) stargazer(itt, type=\"html\", title=\"ITT estimates\", dep.var.labels = c(\"Infant mortality\",\"Birthweight\",\"Low birthweight\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) ITT estimates Dependent variable: Infant mortality Birthweight Low birthweight (1) (2) (3) pkh_kec_ever 0.005 -5.559 0.017 (0.004) (23.805) (0.012) Observations 8,303 4,988 4,988","title":"Health outcomes"},{"location":"mlExamplePKH/#health-outcomes-health-outcomes-1","text":"stargazer(iv, type=\"html\", title=\"IV effect of program participation\", dep.var.labels = c(\"Infant mortality\",\"Birthweight\",\"Lowbirthweigt\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) IV effect of program participation Dependent variable: Infant mortality Birthweight Lowbirthweigt (1) (2) (3) pkh_ever(fit) 0.011 -12.674 0.039 (0.008) (54.269) (0.026) Observations 8,303 4,988 4,988","title":"Health outcomes [health-outcomes-1]"},{"location":"mlExamplePKH/#exploring-heterogeneity","text":"","title":"Exploring heterogeneity"},{"location":"mlExamplePKH/#machine-learning-as-proxy","text":"Generic machine learning approach of Chernozhukov et al. ( 2018 ) Estimate machine learning proxies for $B(x) = \\Er[y(0)|x]$ and $S(x) = \\Er[y(1) - y(0) |x]$ Use proxies to : Estimate best linear projection on true $\\Er[y(1) - y(0)|x]$ Estimate $\\Er[y(1) - y(0) | groups]$","title":"Machine learning as proxy"},{"location":"mlExamplePKH/#heterogeneity-in-cate-for-birthweight","text":"## Function for Generic machine learning of Chernozhukov, Demirer, Duflo, & Fernandez-Val (2018) genericML <- function(x,y,treat, fit.function, predict.function, n.split=10, n.group=5, clusterid=NULL) { if (!is.null(clusterid)) require(sandwich) blp <- matrix(NA, nrow=n.split, ncol=2) blp.se <- blp gate <- matrix(NA, nrow=n.split, ncol=n.group) gate.se <- gate baseline <- matrix(NA, nrow=nrow(x), ncol=n.split) cate <- matrix(NA, nrow=nrow(x), ncol=n.split) Lambda <- matrix(NA, nrow=n.split, ncol=2) for(i in 1:n.split) { main <- runif(nrow(x))>0.5 fit1 <- fit.function(x[!main & treat==1,], y[!main & treat==1]) fit0 <- fit.function(x[!main & treat==0,], y[!main & treat==0]) B <- as.vector(predict.function(fit0,x)) S <- as.vector(predict.function(fit1,x)) - B baseline[,i] <- B cate[,i] <- S ES <- mean(S) ## BLP # assume P(treat|x) = P(treat) = mean(treat) p <- mean(treat) df <- data.frame(y, B, treat, S, main) reg <- lm(y ~ B + I(treat-p) + I((treat-p)*(S-ES)), data=subset(df, main)) blp[i,] <- reg$coef[3:4] if (is.null(clusterid)) blp.se[i,] <- sqrt(diag(vcovHC(reg))[3:4]) else blp.se[i,] <- sqrt(diag(vcovCL(reg, clusterid[main]))[3:4]) Lambda[i,1] <- reg$coefficient[4]^2*var(S) ## GATES cut <- quantile(S, probs=seq(0,1,length.out=(n.group+1))) cut[n.group+1] <- cut[n.group+1] + 1 for(k in 1:n.group) { df[,sprintf(\"G.%d\",k)] <- (cut[k]<=S & S<cut[k+1]) } greg <- lm(as.formula(paste(c(\"y ~ B \", sprintf(\"I((treat-p)*G.%d)\",1:n.group)), collapse=\" + \")), data=subset(df,main)) gc <- grep(\"G\", names(greg$coefficients)) gate[i,] <- greg$coefficients[gc] if (is.null(clusterid)) gate.se[i,] <- sqrt(diag(vcovHC(greg))[gc]) else gate.se[i,] <- sqrt(diag(vcovCL(greg, clusterid[main]))[gc]) Lambda[i,2] <- sum(gate[i,]^2)/n.group } out <- list( gate=gate, gate.se=gate.se, blp=blp, blp.se=blp.se, Lambda=Lambda, baseline=baseline, cate=cate) } genericML.summary <- function(gml) { blp <- apply(gml$blp, 2, function(x) median(x, na.rm=TRUE)) blp.se <- apply(gml$blp.se, 2, function(x) median(x, na.rm=TRUE)) gate <- apply(gml$gate, 2, function(x) median(x, na.rm=TRUE)) gate.se <- apply(gml$gate.se, 2, function(x) median(x, na.rm=TRUE)) Lambda <- apply(gml$Lambda, 2, function(x) median(x, na.rm=TRUE)) return(list(blp=blp, blp.se=blp.se, gate=gate, gate.se=gate.se, Lambda=Lambda)) } library(glmnet) # create x matrix fmla.l <- bw ~ pkh_kec_ever + as.factor(edu)*as.factor(agecat) + log_xp_percap + hh_land + hh_home + as.factor(dist) + hh_phone + hh_rf_tile + hh_rf_shingle + hh_rf_fiber + hh_wall_plaster + hh_wall_brick + hh_wall_wood + hh_wall_fiber + hh_fl_tile + hh_fl_plaster + hh_fl_wood + hh_fl_dirt + hh_water_pam + hh_water_mechwell + hh_water_well + hh_water_spring + hh_water_river + hh_waterhome + hh_toilet_own + hh_toilet_pub + hh_toilet_none + hh_waste_tank + hh_waste_hole + hh_waste_river + hh_waste_field + hh_kitchen + hh_cook_wood + hh_cook_kerosene + hh_cook_gas + tv + fridge + motorbike + car + goat + cow + horse m <- lm(fmla.l, data=pw, x=TRUE, y=TRUE) treat <- m$x[,2] Xl <- m$x[,3:ncol(m$x)] scale <- sd(m$y) center <- mean(m$y) yl <- (m$y-center)/scale lid <- as.factor(pw[as.numeric(rownames(m$x)),]$Location_ID) gml.lasso <- genericML(Xl,m$y, treat, function(x,y) cv.glmnet(x,(y-center)/scale,alpha=1,parallel=FALSE, intercept=TRUE, nfolds=20), function(model, x) { predict(model, x, s=model$lambda.min, type=\"response\")*scale + center }, n.split=11,n.group=5, clusterid=lid) library(grf) gml.rf <- genericML(Xl,m$y, treat, function(x,y) regression_forest(x, (y-center)/scale, tune.parameters=TRUE), function(model, x) { predict(model,x)$predictions*scale + center}, n.split=11,n.group=5, clusterid=lid) library(RSNNS) gml.nn <- genericML(Xl,m$y, treat, function(x,y) mlp(x,(y-center)/scale,linOut=TRUE, size=c(10,10), learnFunc=\"SCG\"), function(model, x) { predict(model,x)*scale + center}, n.split=11,n.group=5, clusterid=lid) library(GGally) df <- data.frame(Lasso=apply(gml.lasso$cate,1,median), Forest=apply(gml.rf$cate,1,median), Neural=apply(gml.nn$cate,1,median)) ggpairs(df, lower=list(continuous=\"smooth\")) + theme_minimal()","title":"Heterogeneity in CATE for Birthweight"},{"location":"mlExamplePKH/#best-linear-projection-of-cate","text":"Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]} Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\beta_0 (d-P(x)) + \\beta_1 (d-P(x))(S(x) - \\Er[S(x)]) + \\epsilon $\\hat{\\beta} 0, \\hat{\\beta}_1 \\to_p \\argmin \\Er[(s_0(x) - b_0 - b_1 (S(x)-E[S(x)]))^2]$ $\\Lambda = \\beta_1^2 Var(S(x)) = corr(s_0(x),S(X))^2 Var(s_0(x))$ library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$blp[1], s$blp.se[1], s$blp[2], s$blp.se[2], s$Lambda[1]) } tbl <- cbind(colfn(gml.lasso), colfn(gml.rf), colfn(gml.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(\"ATE=b0\",\"se\",\"b1\",\"se\",\"Lambda\") kable_styling(kable(tbl, caption=\"Machine learning proxies as BLP of CATE on Birthweight\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Machine learning proxies as BLP of CATE on Birthweight Lasso Regression forest Neural network ATE=b0 -11.381 -1.751 6.185 se 31.414 32.285 32.309 b1 0.371 0.369 0.044 se 0.547 0.491 0.115 Lambda 601.155 683.726 372.192","title":"Best linear projection of CATE"},{"location":"mlExamplePKH/#group-average-treatment-effects-group-average-treatment-effects","text":"Define $G_k = 1{\\ell_{k-1} \\leq S(x) \\leq \\ell_k}$ with $\\ell_k = k/5$ quantile of $S(x)$ Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\sum_k \\gamma_k (d-P(X)) 1(G_k) + \\epsilon $\\hat{\\gamma}_k \\to_p \\Er[y(1) - y(0) | G_k]$ $\\bar{\\Lambda} = \\frac{1}{K} \\sum_k \\gamma_k^2$ library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$gate[1], s$gate.se[1], s$gate[2], s$gate.se[2], s$gate[3], s$gate.se[3], s$gate[4], s$gate.se[4], s$gate[5], s$gate.se[5], s$Lambda[2]) } tbl <- cbind(colfn(gml.lasso), colfn(gml.rf), colfn(gml.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(as.vector(sapply(1:5, function(x) c(sprintf(\"GATE %d\",x),\"se\"))),\"Lambda\") kable_styling(kable(tbl, caption=\"GATE on Birthweight\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) GATE on Birthweight Lasso Regression forest Neural network GATE 1 -48.985 -3.950 19.586 se 69.276 69.646 62.126 GATE 2 21.303 -21.106 -30.948 se 74.528 60.761 67.161 GATE 3 -9.369 -44.090 -11.807 se 65.917 66.654 66.981 GATE 4 1.730 10.772 -13.668 se 66.391 67.695 66.693 GATE 5 -22.329 5.736 52.466 se 66.335 77.828 77.496 Lambda 5998.909 4268.074 3912.912","title":"Group average treatment effects [group-average-treatment-effects]"},{"location":"mlExamplePKH/#heterogeneity-in-cate-on-midwife-utilization","text":"# create x matrix mwb <- pw[as.numeric(rownames(m$x)),]$midwife_birth mw.lasso <- genericML(Xl,mwb, treat, function(x,y) cv.glmnet(x,y,family=\"binomial\", alpha=1,parallel=FALSE, intercept=TRUE, nfolds=20), function(model, x) { predict(model, x, s=model$lambda.min, type=\"response\") }, n.split=11,n.group=5, clusterid=lid) library(grf) mw.rf <- genericML(Xl,mwb, treat, function(x,y) regression_forest(x, y, tune.parameters=TRUE), function(model, x) { predict(model,x)$predictions }, n.split=11,n.group=5, clusterid=lid) library(RSNNS) mw.nn <- genericML(Xl,mwb, treat, function(x,y) mlp(x, y, linOut=FALSE, size=c(10,10), learnFunc=\"SCG\"), function(model, x) { predict(model,x) }, n.split=11,n.group=5, clusterid=lid) library(GGally) df <- data.frame(Lasso=apply(mw.lasso$cate,1,median), Forest=apply(mw.rf$cate,1,median), Neural=apply(mw.nn$cate,1,median)) ggpairs(df, lower=list(continuous=\"smooth\")) + theme_minimal() library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$blp[1], s$blp.se[1], s$blp[2], s$blp.se[2], s$Lambda[1]) } tbl <- cbind(colfn(mw.lasso), colfn(mw.rf), colfn(mw.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(\"ATE=b0\",\"se\",\"b1\",\"se\",\"Lambda\") kable_styling(kable(tbl, caption=\"Machine learning proxies as BLP of CATE on Midwife Use\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Machine learning proxies as BLP of CATE on Midwife Use Lasso Regression forest Neural network ATE=b0 0.048 0.056 0.043 se 0.023 0.023 0.024 b1 0.466 0.625 0.207 se 0.189 0.240 0.089 Lambda 0.003 0.003 0.003 library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$gate[1], s$gate.se[1], s$gate[2], s$gate.se[2], s$gate[3], s$gate.se[3], s$gate[4], s$gate.se[4], s$gate[5], s$gate.se[5], s$Lambda[2]) } tbl <- cbind(colfn(mw.lasso), colfn(mw.rf), colfn(mw.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(as.vector(sapply(1:5, function(x) c(sprintf(\"GATE %d\",x),\"se\"))),\"Lambda\") kable_styling(kable(tbl, caption=\"GATE on Midwife Use\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) GATE on Midwife Use Lasso Regression forest Neural network GATE 1 -0.002 0.030 -0.027 se 0.045 0.054 0.060 GATE 2 0.017 -0.024 0.035 se 0.050 0.054 0.052 GATE 3 0.010 0.038 0.003 se 0.054 0.050 0.050 GATE 4 0.089 0.080 0.058 se 0.053 0.046 0.050 GATE 5 0.166 0.189 0.136 se 0.053 0.052 0.052 Lambda 0.007 0.011 0.006","title":"Heterogeneity in CATE on Midwife utilization"},{"location":"mlExamplePKH/#covariate-means-by-group-covariate-means-by-group","text":"df <- pw[as.numeric(rownames(m$x)),] df$edu99 <- df$edu==99 df$educ <- df$edu df$educ[df$educ==99] <- NA vars <- c(\"log_xp_percap\",\"agecat\",\"educ\",\"tv\",\"goat\", \"hh_toilet_own\",\"motorbike\",\"hh_cook_wood\",\"pkh_ever\") tmp <- data.frame() groupMeans <- function(var, gml, clusterid) { n.group <- ncol(gml$gate) gate <- matrix(NA, nrow=nrow(gml$gate), ncol=ncol(gml$gate)) gate.se <- gate dat <- data.frame(y=var) for (i in 1:ncol(gml$cate)) { S <- gml$cate[,i] cut <- quantile(S, probs=seq(0,1,length.out=(n.group+1))) cut[n.group+1] <- cut[n.group+1] + 1 for(k in 1:n.group) { dat[,sprintf(\"G.%d\",k)] <- 1*(cut[k]<=S & S<cut[k+1]) } greg <- lm(as.formula(paste(c(\"y ~ -1\", sprintf(\"G.%d\",1:n.group)), collapse=\" + \")), data=dat) gc <- grep(\"G\", names(greg$coefficients)) gate[i,] <- greg$coefficients[gc] if (is.null(clusterid)) gate.se[i,] <- sqrt(diag(vcovHC(greg))[gc]) else gate.se[i,] <- sqrt(diag(vcovCL(greg,clusterid))[gc]) } return(list(mean=apply(gate, 2, function(x) median(x,na.rm=TRUE)), se = apply(gate.se, 2, function(x) median(x,na.rm=TRUE)))) } methods <- c(\"Lasso\",\"Forest\",\"Neural\") gmls <- list(mw.lasso,mw.rf,mw.nn) for(v in vars) { for (m in 1:length(methods)) { gm <- groupMeans(df[,v], gmls[[m]], lid) tmp <- rbind(tmp, data.frame(group=1:length(gm$mean),variable=v, method=methods[m], mean=gm$mean, se=gm$se)) } } library(ggplot2) fig <- ggplot(data=tmp, aes(x=group, y=mean, colour=method)) + geom_line() + geom_line(aes(y=(mean+1.96*se), colour=method), linetype=2) + geom_line(aes(y=(mean-1.96*se), colour=method), linetype=2) + facet_wrap(~ variable,scales=\"free_y\") + theme_minimal() print(fig)","title":"Covariate means by group [covariate-means-by-group]"},{"location":"mlExamplePKH/#references","text":"Alatas, Vivi, Nur Cahyadi, Elisabeth Ekasari, Sarah Harmoun, Budi Hidayat, Edgar Janz, Jon Jellema, H Tuhiman, and M Wai-Poi. 2011. \u201cProgram Keluarga Harapan : Impact Evaluation of Indonesia\u2019s Pilot Household Conditional Cash Transfer Program.\u201d World Bank. http://documents.worldbank.org/curated/en/589171468266179965/Program-Keluarga-Harapan-impact-evaluation-of-Indonesias-Pilot-Household-Conditional-Cash-Transfer-Program . Chernozhukov, Victor, Mert Demirer, Esther Duflo, and Iv\u00e1n Fern\u00e1ndez-Val. 2018. \u201cGeneric Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experimentsxo.\u201d Working Paper 24678. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w24678 . Triyana, Margaret. 2016. \u201cDo Health Care Providers Respond to Demand-Side Incentives? Evidence from Indonesia.\u201d American Economic Journal: Economic Policy 8 (4): 255\u201388. https://doi.org/10.1257/pol.20140048 .","title":"References"},{"location":"mlp/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 The previous notes discussed single layer neural networks. These notes will look at multiple layer networks. Additional Reading \u00b6 @goodfellow2016 Deep Learning Knet.jl documentation especially the textbook @klok2019 Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence @farrel2018 \u201cDeep Neural Networks for Estimation and Inference\u201d Multiple Layer Neural Networks \u00b6 A multiple layer feed forward neural network (aka a multi-layer perception) connects many single layer networks. A multi-layer perceptron can be written recursively. The outermost layer of a multi-layer perception looks like a generalized linear model: \\hat{f}(x) = \\psi_L(x_L' w_L+ b_L) where $x_L, w_L \\in \\R^{H_L}$, $b_L \\in \\R$, and $\\psi_L: \\R \\to \\R$. For regression problems, $\\psi_L$ is typically the identity function. In a generalized linear model, $x_L$ would be data. In a multilayer network, $x_L \\in \\R^{H_{L}}$ is the output of a previous layer. Specificaly, for $k \\in { 1, ...., H_L}$, x_{k,L} = \\psi_{k, L-1}(x_{L-1}'w_{k,L-1} + b_{k,L-1}) where $x_{L-1}, w_{L-1} \\in \\R^{H_{L-1}}$, $b_{L-1} \\in \\R$, and $\\psi_{k,L-1}: \\R \\to \\R$. This continues recursively until $x_0 = x \\in \\R^d$ is the data. $L$ is the depth of the network. When $L$ is sufficiently large, you have a deep neural network, and can attract grant money by calling your research deep learning and/or AI. $H_\\ell$ is the width of layer $\\ell$. Following @farrel2018, we will let U = \\sum_{\\ell=1}^L H_\\ell denote the number of units. The number of parameters is W = \\sum_{\\ell=1}^L (H_{\\ell-1}+1) H_\\ell where $H_0 = d$ is the dimension of the data. In most applications, the activation within a layer is the same for each unit, i.e. $\\psi_{k,\\ell}$ does not vary with $k$. In large networks and/or with large datasets, activation functions are usually (leaky) rectified linear to allow faster computation. The combination of depths ($L$), width ($H_\\ell$), and activation functions ($\\psi$) are collectively referred to as the network architecture. First Example \u00b6 As a starting example, here is some code that fits a multi-layer network to the same simulated data as in the notes on single layer networks . Simulating data and setting up. using Plots, Flux, Statistics, ColorSchemes Plots.pyplot() # some function to estimate f(x) = sin(x^x)/2^((x^x-\u03c0/2)/\u03c0) function simulate(n,s=1) x = rand(n,1).*\u03c0 y = f.(x) .+ randn(n).*s (x,y) end x, y = simulate(1000, 0.5) xt = reshape(x, 1, length(x)) yt = reshape(y, 1, length(y)) xg = 0:0.01:\u03c0 cscheme = colorschemes[:BrBG_4]; dimx = 1 xt = reshape(Float32.(x), 1, length(x)) yt = reshape(Float32.(y), 1, length(y)) 1\u00d71000 Matrix{Float32}: -0.168491 1.05876 1.0616 0.400797 \u2026 0.110237 -0.380941 0.977197 We now define our models. The second model is a multi-layer network with 3 layers each of width 3. The first model is a single-layer network with width 15. This makes the total number of parameters in the two networks equal. For both networks we normalise $x$ and then use Flux\u2019s default initial values (these set $b=0$ and $w$ random). mlps = [ Chain(x->Flux.normalise(x, dims=2), Dense(dimx, 15, Flux.leakyrelu), Dense(15, 1)), Chain(x->Flux.normalise(x, dims=2), Dense(dimx, 3, Flux.leakyrelu), Dense(3, 3, Flux.leakyrelu), Dense(3, 3, Flux.leakyrelu), Dense(3, 1)) ] figs = Array{typeof(plot(0)),1}(undef,length(mlps)) initmfigs = Array{typeof(plot(0)),1}(undef,length(mlps)) for r in eachindex(mlps) m = mlps[r] println(\"Model $r = $m\") nparm = sum([length(m[i].weight) + length(m[i].bias) for i in 2:length(m)]) println(\" $nparm parameters in $(length(m)-1) layers\") initmfigs[r] = plot(xg, m[1:(end-1)](xg')', lab=\"\", legend=false) figs[r]=plot(xg, f.(xg), lab=\"\", title=\"Model $r\", color=:red) figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter = 8000 opt = Flux.AMSGrad() @time for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x),y), Flux.params(m), #[(xt[:,b], yt[:,b]) for b in Base.Iterators.partition(1:length(yt), 500)], [(xt, yt)], opt) #, #cb = Flux.throttle(()->@show(Flux.mse(m(xt),yt)),100)) if i==1 || (i % (maxiter \u00f7 10)==0) l=Flux.mse(m(xt), yt) println(\"Model $r, $i iterations, loss=$l\") yg = m(xg')' loc=Int64.(ceil(length(xg)*i/maxiter)) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end display(figs[r]) end Model 1 = Chain(#1, Dense(1 => 15, leakyrelu), Dense(15 => 1)) 46 parameters in 2 layers Model 1, 1 iterations, loss=0.9862753 Model 1, 800 iterations, loss=0.26882413 Model 1, 1600 iterations, loss=0.26460192 Model 1, 2400 iterations, loss=0.26314363 Model 1, 3200 iterations, loss=0.26248872 Model 1, 4000 iterations, loss=0.26212132 Model 1, 4800 iterations, loss=0.26175115 Model 1, 5600 iterations, loss=0.2612047 Model 1, 6400 iterations, loss=0.26110145 Model 1, 7200 iterations, loss=0.26102412 Model 1, 8000 iterations, loss=0.26094484 32.165471 seconds (66.51 M allocations: 6.237 GiB, 5.09% gc time, 93.52% c ompilation time: 0% of which was recompilation) Model 2 = Chain(#2, Dense(1 => 3, leakyrelu), Dense(3 => 3, leakyrelu), Den se(3 => 3, leakyrelu), Dense(3 => 1)) 34 parameters in 4 layers Model 2, 1 iterations, loss=0.64466125 Model 2, 800 iterations, loss=0.2677127 Model 2, 1600 iterations, loss=0.2609369 Model 2, 2400 iterations, loss=0.2603395 Model 2, 3200 iterations, loss=0.2597793 Model 2, 4000 iterations, loss=0.25472873 Model 2, 4800 iterations, loss=0.25445977 Model 2, 5600 iterations, loss=0.25441432 Model 2, 6400 iterations, loss=0.254386 Model 2, 7200 iterations, loss=0.25436878 Model 2, 8000 iterations, loss=0.25434887 2.592303 seconds (2.70 M allocations: 2.143 GiB, 9.17% gc time, 19.71% co mpilation time) In this simulation setup, the performance of the two network architectures is hard to distinguish. The multi-layer network takes a bit longer to train. Depending on the randomly simulated data, and randomly drawn initial values, either model might achieve lower in-sample MSE. Image Classification: MNIST \u00b6 MNIST is a database of images of handwritten digits . MNIST is a common machine learning benchmark. Given a handwritten digit, we want to classify it is a 0, 1, \u2026, or 9. You can try a demo of a MNIST classifier trained in Flux here . Multilayer feed forward networks generally have good, but not quite state-of-the-art performance in image classification. Nonetheless, this will hopefully serve as a good example. The code in this section was adapted from the Flux model zoo. First we load some packages and download the data. using Flux, Statistics using Flux: onehotbatch, onecold, crossentropy, throttle, @epochs using Base.Iterators: repeated using CUDA using JLD2 using MLDatasets # load training set train_x, train_y = MNIST(split=:train)[:] # load test set test_x, test_y = MNIST(split=:test)[:] modeldir = normpath(joinpath(docdir,\"jmd\",\"models\")) if !isdir(modeldir) mkdir(modeldir) end Let\u2019s look at some of the images. # Previously MNIST was provided in the Flux package in a different format. # To keep code compatible, we convert to the old format imgs = vcat([Gray.(1 .-train_x[:,:,i]') for i \u2208 1:size(train_x,3)], [Gray.(1 .-test_x[:,:,i]') for i \u2208 1:size(test_x,3)]) labels = vcat(train_y, test_y) idx = rand(1:length(imgs), 16) plot([plot(imgs[i], title=\"$(labels[i])\", aspect_ratio=:equal, axis=false, ticks=false) for i in idx]...) The images are 28 by 28 pixels. Continue processing the data # Stack images into one large batch X = Float32.(reshape(train_x, size(train_x,1)*size(train_x,2), size(train_x,3))) |> gpu; tX = Float32.(reshape(test_x, size(train_x,1)*size(train_x,2), size(test_x,3))) |> gpu; # One-hot-encode the labels Y = onehotbatch(train_y, 0:9) |> gpu; tY = onehotbatch(test_y, 0:9) |> gpu; One hot encoding is what the machine learning world calls creating dummy variables from a categorical variable. Single Layer Classification \u00b6 Now we define our neural network. To begin with we will look at single hidden layer with a multinomial logit output layer. The function that gives choice probabilities in a multinomial logit model is called the softmax function. That is, softmax(x_1, ..., x_k) = \\left( \\frac{e^{x_1}}{\\sum_{j=1}^k e^{x_i}}, \\frac{e^{x_2}}{\\sum_{j=1}^k e^{x_i}}, ..., \\frac{e^{x_k}}{\\sum_{j=1}^k e^{x_i}} \\right) mstart = Chain( Dense(28^2, 32, relu), Dense(32, 10)) Chain( Dense(784 => 32, relu), # 25_120 parameters Dense(32 => 10), # 330 parameters ) # Total: 4 arrays, 25_450 parameters, 99.664 KiB. In this example, we are working on a classification problem; we are trying to predict a discrete outcome instead of a continuous one. The output of the network above are probabilities that an image represents each of the ten digits. That is, we forming conditional probability, or the likelihood, of $y$ given $x$. In this situation, maximum likelihood is a natural estimator. For discrete $y$ (like we have here), the log likelihood is equal to minus the cross-entropy, so this is what we use as our loss function. loss(m, x, y) = Flux.logitcrossentropy(m(x), y) loss (generic function with 1 method) Since cross-entropy or log likelihood are difficult to interpret, we might want a more intuitive measure of our model\u2019s performance. For classification accuracy is the portion of predictions that are correct. Other measures of classification performance For this application accuracy is likely sufficient, but in some situations (including rare outcomes or when we weight differently type I and type II errors) accuracy is not a sufficient measure of a classifier\u2019s performance. There are variety of other measures, such as precision, recall, and AUC. See @qeclassify for more information . function accuracy(m, x, y) # onecold(m(x)) results in very slow code for large x, so we avoid it coldx = vec(map(x->x[1], argmax(m(x), dims=1))) coldy = onecold(y) return(mean(coldx.==coldy)) end; onecold is the inverse of one-hot-encoding; onecold transforms a matrix of dummy varibles (or probabilities) into an integer (the one with the highest probability in the case of m(x) ). # gradient descent steps using the full X and Y to compute gradients Xsmall=X[:,1:2000] Ysmall=Y[:,1:2000] # accuracy is slower, so only compute on subset of data function evalcb(m) l = loss(m, X, Y) if isnan(l) @show (l) else @show (l, accuracy(m, Xsmall,Ysmall), accuracy(m, tX,tY)) end end; Optimizers Neural networks are usually trained using a variant of gradient descent for optimization. Recall that gradient descent searches for the minimum by taking steps of the form: \\theta_{new} = \\theta - \\eta Df_{\\theta} where $\\eta$ is a step size or learning rate parameter that gets adjusted depending on algorithm progress. There are many variants of gradient descent available in Flux.jl and they differ in how they adjust the learning rate, $\\eta$, and other details. Some algorithms add \u201cmomentum\u201d to avoid the long narrow valley problem we saw in the banana function example in the optimization notes . Ruder (2016) gives a nice overview of various forms of gradient descent. Since Flux.train! might run for a long time, it allows us to pass a \u201ccallback\u201d function that gets evaluated every iteration. Here, this function is just used to monitor progress. In some situations, we might also want to use the callback function to save intermediate results to disk in case the computation gets interrupted before completion. The Flux.throttle function can be used to prevent the call-back function from being evaluated too often. The code below makes evalcb get evaluated at most once every 10 seconds. Flux.train does not automatically check that the optimizer is making progress. With too large of a step size, gradient descent may lead in the wrong direction, increasing the loss function. This can even lead the parameters to drift toward numeric under or overflow and become NaN . If this happens, we should descrease the learning rate. rerun = false modelfile = joinpath(modeldir,\"mnist-slp.jld2\") opt = ADAM() if rerun || !isfile(modelfile) dataset = repeated((X, Y), 1) # each call to Flux.trian! will do 1 m = gpu(mstart) evalcb(m) iterations = 200 losses = zeros(typeof(loss(m,X,Y)), iterations) testloss=similar(losses) @time for i = 1:iterations Flux.train!((x,y)->loss(m,x,y), Flux.params(m), dataset, opt, cb = throttle(()->evalcb(m), 10)) losses[i] = loss(m,X,Y) testloss[i] = loss(m,tX,tY) end plot([losses testloss], xlab=\"Iterations\", ylab=\"Loss\", labels=[\"Train\" \"Test\"]) # save model cpum = cpu(m) @save modelfile cpum else @load modelfile cpum m = gpu(cpum) end Chain( Dense(784 => 32, relu), # 25_120 parameters Dense(32 => 10), # 330 parameters ) # Total: 4 arrays, 25_450 parameters, 576 bytes. @show accuracy(m,Xsmall, Ysmall) @show accuracy(m,tX, tY); accuracy(m, Xsmall, Ysmall) = 0.927 accuracy(m, tX, tY) = 0.9272 After 200 iterations, the accuracy is already greater than 90%. This is pretty good. The test set accuracy is higher than the training set, which could just be good luck, but it is also possible that the model is underfitting. Let\u2019s try training the network longer (doing more gradient descent iterations. rerun = false modelfile = joinpath(modeldir,\"mnist-slp-8200.jld2\") dataset = repeated((X, Y), 200) # each call to Flux.trian! will do 200 # gradient descent steps using the full X and Y to compute gradients if rerun || !isfile(modelfile) evalcb(m) @time @epochs 40 Flux.train!((x,y)->loss(m,x,y), Flux.params(m), dataset, opt, cb = throttle(()->evalcb(m), 10)) evalcb(m) # save model cpum = cpu(m) @save modelfile cpum else @load modelfile cpum m = gpu(cpum) end @show accuracy(m,Xsmall, Ysmall) @show accuracy(m,tX,tY); accuracy(m, Xsmall, Ysmall) = 1.0 accuracy(m, tX, tY) = 0.9603 Remember that each \u201cepoch\u201d does one gradient descent step for each tuple in dataset . In the code above dataset is just the original data repeated 200 times. We ran for 40 epochs, so there were a total of 8000 more gradient descent iterations. We see that the training accuracy has improved to above 99%, but our test accuracy has failed to improve much above 96%. My initial interpretation of this result would be that we are now overfitting. The number of parameters in the network is nparam(m) = sum([length(m[i].weight) + length(m[i].bias) for i in 1:length(m) if typeof(m[i]) <: Dense]) nparam(m) 25450 and there 60000 images. For a typical econometric or statistic problem, there are too many parameters for the number of observations. One solution to this situation is to reduce the number of parameters. Another solution is to do what lasso does and regularize . Lasso regularizes by adding a penalty to the loss function. Limiting the number of gradient descent iterations can also act as a form of regularization. This is often called Landweber regularization . It underlies the common procedure of training a neural network until the training loss starts to be much less than loss on a held out portion of the data (or the loss on the held out portion stops decreasing). Deep Classification \u00b6 Given the apparent overfitting of the single layer network above, I would be reluctant to move to an even more complex model. However, I would be mistaken. If you glance through the MNIST benchmarks on LeCun\u2019s website , you will see that @ciresan2010 achieve a much higher test accuracy with a 6 layer network. Let\u2019s try their network architecture. We will use their numbers of layers and hidden units, but with rectified linear activation. They used tanh activation functions. cmgsnet_cpu = Chain( Dense(28^2, 2500 , relu), Dense(2500, 2000 , relu), Dense(2000, 1500 , relu), Dense(1500, 1000 , relu), Dense(1000, 500 , relu), Dense(500, 10) #softmax ) println(\"cmgsnet has $(nparam(cmgsnet_cpu)) parameters!!!\") cmgsnet has 11972510 parameters!!! That\u2019s a deep network. rerun = false batchsize=10000 parts=Base.Iterators.partition(1:size(X,2), batchsize) data = repeat([(X[:,p], Y[:,p]) for p in parts], 10); # The full data + network doesn't fit in my GPU memory, so do batches epochs = 15 acctest = zeros(epochs) acctrain = zeros(epochs) losstest = zeros(epochs) losstrain = zeros(epochs) opt=ADAM() cmgsnet = gpu(cmgsnet_cpu) for e in 1:epochs modelfile = joinpath(modeldir,\"cmgsnet-$e-epochs.jld2\") if rerun || !isfile(modelfile) println(\"Beginning epoch $e\") evalcb(cmgsnet) @time Flux.train!((x,y)->loss(cmgsnet,x,y), Flux.params(cmgsnet), data, opt, cb = throttle(()->evalcb(cmgsnet), 10)) evalcb(cmgsnet) # save model local cpum = cpu(cmgsnet) @save modelfile cpum else @load modelfile cpum global cmgsnet = gpu(cpum) end println(\"Finished $e epochs\") losstrain[e]=loss(cmgsnet,X,Y) acctrain[e]=accuracy(cmgsnet,Xsmall, Ysmall) losstest[e]=loss(cmgsnet,tX,tY) acctest[e]=accuracy(cmgsnet,tX,tY) end e = epochs modelfile = joinpath(modeldir,\"cmgsnet-$e-epochs.jld2\") @load modelfile cpum cmgsnet = gpu(cpum) Finished 1 epochs Finished 2 epochs Finished 3 epochs Finished 4 epochs Finished 5 epochs Finished 6 epochs Finished 7 epochs Finished 8 epochs Finished 9 epochs Finished 10 epochs Finished 11 epochs Finished 12 epochs Finished 13 epochs Finished 14 epochs Finished 15 epochs Chain( Dense(784 => 2500, relu), # 1_962_500 parameters Dense(2500 => 2000, relu), # 5_002_000 parameters Dense(2000 => 1500, relu), # 3_001_500 parameters Dense(1500 => 1000, relu), # 1_501_000 parameters Dense(1000 => 500, relu), # 500_500 parameters Dense(500 => 10), # 5_010 parameters ) # Total: 12 arrays, 11_972_510 parameters, 1.656 KiB. This model achieved a testing accuracy of 98.49% after 11 training epochs. Each training epoch consisting of 10 passes through the data split into two batches, so 20 gradient descent iterations. Let\u2019s plot the loss and accuracy vs epoch. al = Int(round(3*length(losstrain)/4)) plot( plot([losstrain, losstest], xlab=\"Epochs\", title=\"Cross-Entropy Loss\", annotations=[(al, losstrain[al], Plots.text(\"training\", pointsize=12, valign=:bottom, color=get(cscheme,1))), (al, losstest[al], Plots.text(\"test\", pointsize=12, valign=:bottom, color=get(cscheme,0)))], leg=false, color_palette=get(cscheme,[1,0]) ), plot([acctrain, acctest], xlab=\"Epochs\", title=\"Accuracy\", leg=false, color_palette=get(cscheme,[1,0]) ), layout=(2,1) ) There is really something remarkable going on in this example. A model that appears extremely overparameterized manages to predict very well on a test set. One important thing to keep in mind is that image classification is very different from the typical estimation problems in applied economics. In regressions and other models of economic variables, we never expect to be able to predict perfectly. An $R^2$ of 0.4 in a cross-sectional earnings regression is typical, or even high. Image classification is very different. We know there is a model (our eyes) that can classify nearly perfectly. In the language of econometrics, the error term is zero, or there is no uncertainty, in the \u201ctrue\u201d image classification models. <!-- TODO: Add an aside about adversarial examples and failures to --> <!-- generalize. --> Let\u2019s look at some of the images that our model failed to classify correctly. timgs = [Gray.(1 .-test_x[:,:,i]') for i \u2208 1:size(test_x,3)] tlabels = test_y # predicted labels mlabels = cpu(vec(map(x->x[1], argmax(cmgsnet(tX), dims=1)))) .- 1 @show mean(mlabels.==tlabels) # = accuracy @show sum(mlabels .!= tlabels) miss=findall(mlabels .!= tlabels) plot( [plot(timgs[i], axis=false, ticks=false, title=\"$(tlabels[i]) as $(mlabels[i])\", aspect_ratio=:equal) for i in miss[1:16]]...) mean(mlabels .== tlabels) = 0.9849 sum(mlabels .!= tlabels) = 151 Our model still does not have state-of-the-art accuracy. @ciresan2010 achieves 99.65% accuracy. There are differences in terms of activation function and gradient descent details between @ciresan2010 and the code above. However, I suspect that the main reason for their better performance is that @ciresan2010 generate additional training images. They do this by randomly rotating, stretching, and adding oscillations to the existing images. Data Augmentation \u00b6 Generating more data from your existing data is called data augmentation. Let\u2019s try augmenting the data by randomly rotating the images. function rotateimage(img, \u03b8) R = similar(img) R .= img[1,1] i0 = (size(img,1)+1)/2 j0 = (size(img,2)+1)/2 for i \u2208 axes(img)[1] for j \u2208 axes(img)[2] ri = Int(round((i-i0)*cos(\u03b8) + (j-j0)*sin(\u03b8) +i0)) rj = Int(round(-(i-i0)*sin(\u03b8) + (j-j0)*cos(\u03b8) +j0)) if (ri \u2208 axes(img)[1] && rj \u2208 axes(img)[2]) R[ri,rj] = img[i,j] end end end return R end plot([plot(timgs[i], axis=false, ticks=false, aspect_ratio=:equal) for i in 1:8]..., [plot(rotateimage(timgs[i], (-1)^i*\u03c0/6), axis=false, ticks=false, aspect_ratio=:equal) for i in 1:8]...) Training with rotated images. rerun = false batchsize=10000 parts=Base.Iterators.partition(1:size(X,2), batchsize) randrotate(x::AbstractVector; max\u03b8 = \u03c0/8) = vec(rotateimage(reshape(x, 28, 28), rand()*2max\u03b8 - max\u03b8)) data = [(gpu(mapslices(randrotate,cpu(X[:,p]), dims=1)), Y[:,p]) for p in parts for i=1:10]; epochs = 10 acctest = zeros(epochs) acctrain = zeros(epochs) losstest = zeros(epochs) losstrain = zeros(epochs) opt=ADAM() cmgsnet = gpu(cmgsnet_cpu) for e in 1:epochs modelfile = joinpath(modeldir,\"cmgsnet-aug-$e-epochs.jld2\") if rerun || !isfile(modelfile) println(\"Beginning epoch $e\") evalcb(cmgsnet) @time Flux.train!((x,y)->loss(cmgsnet,x,y), Flux.params(cmgsnet), data, opt, cb = throttle(()->evalcb(cmgsnet), 10)) evalcb(cmgsnet) # save model local cpum = cpu(cmgsnet) @save modelfile cpum else @load modelfile cpum global cmgsnet = gpu(cpum) end println(\"Finished $e epochs\") losstrain[e]=loss(cmgsnet,X,Y) acctrain[e]=accuracy(cmgsnet,Xsmall, Ysmall) losstest[e]=loss(cmgsnet,tX,tY) acctest[e]=accuracy(cmgsnet,tX,tY) end e = epochs modelfile = joinpath(modeldir,\"cmgsnet-aug-$e-epochs.jld2\") @load modelfile cpum cmgsnet = gpu(cpum) @show maximum(acctest) Finished 1 epochs Finished 2 epochs Finished 3 epochs Finished 4 epochs Finished 5 epochs Finished 6 epochs Finished 7 epochs Finished 8 epochs Finished 9 epochs Finished 10 epochs maximum(acctest) = 0.9882 0.9882 timgs = [Gray.(1 .-test_x[:,:,i]') for i \u2208 1:size(test_x,3)] tlabels = test_y # predicted labels mlabels = cpu(vec(map(x->x[1], argmax(cmgsnet(tX), dims=1)))) .- 1 @show mean(mlabels.==tlabels) # = accuracy @show sum(mlabels .!= tlabels) miss=findall(mlabels .!= tlabels) plot( [plot(timgs[i], axis=false, ticks=false, title=\"$(tlabels[i]) as $(mlabels[i])\", aspect_ratio=:equal) for i in miss[1:16]]...) mean(mlabels .== tlabels) = 0.9882 sum(mlabels .!= tlabels) = 118 References \u00b6","title":"Multi-Layer"},{"location":"mlp/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min}","title":"About this document"},{"location":"mlp/#introduction","text":"The previous notes discussed single layer neural networks. These notes will look at multiple layer networks.","title":"Introduction"},{"location":"mlp/#additional-reading","text":"@goodfellow2016 Deep Learning Knet.jl documentation especially the textbook @klok2019 Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence @farrel2018 \u201cDeep Neural Networks for Estimation and Inference\u201d","title":"Additional Reading"},{"location":"mlp/#multiple-layer-neural-networks","text":"A multiple layer feed forward neural network (aka a multi-layer perception) connects many single layer networks. A multi-layer perceptron can be written recursively. The outermost layer of a multi-layer perception looks like a generalized linear model: \\hat{f}(x) = \\psi_L(x_L' w_L+ b_L) where $x_L, w_L \\in \\R^{H_L}$, $b_L \\in \\R$, and $\\psi_L: \\R \\to \\R$. For regression problems, $\\psi_L$ is typically the identity function. In a generalized linear model, $x_L$ would be data. In a multilayer network, $x_L \\in \\R^{H_{L}}$ is the output of a previous layer. Specificaly, for $k \\in { 1, ...., H_L}$, x_{k,L} = \\psi_{k, L-1}(x_{L-1}'w_{k,L-1} + b_{k,L-1}) where $x_{L-1}, w_{L-1} \\in \\R^{H_{L-1}}$, $b_{L-1} \\in \\R$, and $\\psi_{k,L-1}: \\R \\to \\R$. This continues recursively until $x_0 = x \\in \\R^d$ is the data. $L$ is the depth of the network. When $L$ is sufficiently large, you have a deep neural network, and can attract grant money by calling your research deep learning and/or AI. $H_\\ell$ is the width of layer $\\ell$. Following @farrel2018, we will let U = \\sum_{\\ell=1}^L H_\\ell denote the number of units. The number of parameters is W = \\sum_{\\ell=1}^L (H_{\\ell-1}+1) H_\\ell where $H_0 = d$ is the dimension of the data. In most applications, the activation within a layer is the same for each unit, i.e. $\\psi_{k,\\ell}$ does not vary with $k$. In large networks and/or with large datasets, activation functions are usually (leaky) rectified linear to allow faster computation. The combination of depths ($L$), width ($H_\\ell$), and activation functions ($\\psi$) are collectively referred to as the network architecture.","title":"Multiple Layer Neural Networks"},{"location":"mlp/#first-example","text":"As a starting example, here is some code that fits a multi-layer network to the same simulated data as in the notes on single layer networks . Simulating data and setting up. using Plots, Flux, Statistics, ColorSchemes Plots.pyplot() # some function to estimate f(x) = sin(x^x)/2^((x^x-\u03c0/2)/\u03c0) function simulate(n,s=1) x = rand(n,1).*\u03c0 y = f.(x) .+ randn(n).*s (x,y) end x, y = simulate(1000, 0.5) xt = reshape(x, 1, length(x)) yt = reshape(y, 1, length(y)) xg = 0:0.01:\u03c0 cscheme = colorschemes[:BrBG_4]; dimx = 1 xt = reshape(Float32.(x), 1, length(x)) yt = reshape(Float32.(y), 1, length(y)) 1\u00d71000 Matrix{Float32}: -0.168491 1.05876 1.0616 0.400797 \u2026 0.110237 -0.380941 0.977197 We now define our models. The second model is a multi-layer network with 3 layers each of width 3. The first model is a single-layer network with width 15. This makes the total number of parameters in the two networks equal. For both networks we normalise $x$ and then use Flux\u2019s default initial values (these set $b=0$ and $w$ random). mlps = [ Chain(x->Flux.normalise(x, dims=2), Dense(dimx, 15, Flux.leakyrelu), Dense(15, 1)), Chain(x->Flux.normalise(x, dims=2), Dense(dimx, 3, Flux.leakyrelu), Dense(3, 3, Flux.leakyrelu), Dense(3, 3, Flux.leakyrelu), Dense(3, 1)) ] figs = Array{typeof(plot(0)),1}(undef,length(mlps)) initmfigs = Array{typeof(plot(0)),1}(undef,length(mlps)) for r in eachindex(mlps) m = mlps[r] println(\"Model $r = $m\") nparm = sum([length(m[i].weight) + length(m[i].bias) for i in 2:length(m)]) println(\" $nparm parameters in $(length(m)-1) layers\") initmfigs[r] = plot(xg, m[1:(end-1)](xg')', lab=\"\", legend=false) figs[r]=plot(xg, f.(xg), lab=\"\", title=\"Model $r\", color=:red) figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter = 8000 opt = Flux.AMSGrad() @time for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x),y), Flux.params(m), #[(xt[:,b], yt[:,b]) for b in Base.Iterators.partition(1:length(yt), 500)], [(xt, yt)], opt) #, #cb = Flux.throttle(()->@show(Flux.mse(m(xt),yt)),100)) if i==1 || (i % (maxiter \u00f7 10)==0) l=Flux.mse(m(xt), yt) println(\"Model $r, $i iterations, loss=$l\") yg = m(xg')' loc=Int64.(ceil(length(xg)*i/maxiter)) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end display(figs[r]) end Model 1 = Chain(#1, Dense(1 => 15, leakyrelu), Dense(15 => 1)) 46 parameters in 2 layers Model 1, 1 iterations, loss=0.9862753 Model 1, 800 iterations, loss=0.26882413 Model 1, 1600 iterations, loss=0.26460192 Model 1, 2400 iterations, loss=0.26314363 Model 1, 3200 iterations, loss=0.26248872 Model 1, 4000 iterations, loss=0.26212132 Model 1, 4800 iterations, loss=0.26175115 Model 1, 5600 iterations, loss=0.2612047 Model 1, 6400 iterations, loss=0.26110145 Model 1, 7200 iterations, loss=0.26102412 Model 1, 8000 iterations, loss=0.26094484 32.165471 seconds (66.51 M allocations: 6.237 GiB, 5.09% gc time, 93.52% c ompilation time: 0% of which was recompilation) Model 2 = Chain(#2, Dense(1 => 3, leakyrelu), Dense(3 => 3, leakyrelu), Den se(3 => 3, leakyrelu), Dense(3 => 1)) 34 parameters in 4 layers Model 2, 1 iterations, loss=0.64466125 Model 2, 800 iterations, loss=0.2677127 Model 2, 1600 iterations, loss=0.2609369 Model 2, 2400 iterations, loss=0.2603395 Model 2, 3200 iterations, loss=0.2597793 Model 2, 4000 iterations, loss=0.25472873 Model 2, 4800 iterations, loss=0.25445977 Model 2, 5600 iterations, loss=0.25441432 Model 2, 6400 iterations, loss=0.254386 Model 2, 7200 iterations, loss=0.25436878 Model 2, 8000 iterations, loss=0.25434887 2.592303 seconds (2.70 M allocations: 2.143 GiB, 9.17% gc time, 19.71% co mpilation time) In this simulation setup, the performance of the two network architectures is hard to distinguish. The multi-layer network takes a bit longer to train. Depending on the randomly simulated data, and randomly drawn initial values, either model might achieve lower in-sample MSE.","title":"First Example"},{"location":"mlp/#image-classification-mnist","text":"MNIST is a database of images of handwritten digits . MNIST is a common machine learning benchmark. Given a handwritten digit, we want to classify it is a 0, 1, \u2026, or 9. You can try a demo of a MNIST classifier trained in Flux here . Multilayer feed forward networks generally have good, but not quite state-of-the-art performance in image classification. Nonetheless, this will hopefully serve as a good example. The code in this section was adapted from the Flux model zoo. First we load some packages and download the data. using Flux, Statistics using Flux: onehotbatch, onecold, crossentropy, throttle, @epochs using Base.Iterators: repeated using CUDA using JLD2 using MLDatasets # load training set train_x, train_y = MNIST(split=:train)[:] # load test set test_x, test_y = MNIST(split=:test)[:] modeldir = normpath(joinpath(docdir,\"jmd\",\"models\")) if !isdir(modeldir) mkdir(modeldir) end Let\u2019s look at some of the images. # Previously MNIST was provided in the Flux package in a different format. # To keep code compatible, we convert to the old format imgs = vcat([Gray.(1 .-train_x[:,:,i]') for i \u2208 1:size(train_x,3)], [Gray.(1 .-test_x[:,:,i]') for i \u2208 1:size(test_x,3)]) labels = vcat(train_y, test_y) idx = rand(1:length(imgs), 16) plot([plot(imgs[i], title=\"$(labels[i])\", aspect_ratio=:equal, axis=false, ticks=false) for i in idx]...) The images are 28 by 28 pixels. Continue processing the data # Stack images into one large batch X = Float32.(reshape(train_x, size(train_x,1)*size(train_x,2), size(train_x,3))) |> gpu; tX = Float32.(reshape(test_x, size(train_x,1)*size(train_x,2), size(test_x,3))) |> gpu; # One-hot-encode the labels Y = onehotbatch(train_y, 0:9) |> gpu; tY = onehotbatch(test_y, 0:9) |> gpu; One hot encoding is what the machine learning world calls creating dummy variables from a categorical variable.","title":"Image Classification: MNIST"},{"location":"mlp/#single-layer-classification","text":"Now we define our neural network. To begin with we will look at single hidden layer with a multinomial logit output layer. The function that gives choice probabilities in a multinomial logit model is called the softmax function. That is, softmax(x_1, ..., x_k) = \\left( \\frac{e^{x_1}}{\\sum_{j=1}^k e^{x_i}}, \\frac{e^{x_2}}{\\sum_{j=1}^k e^{x_i}}, ..., \\frac{e^{x_k}}{\\sum_{j=1}^k e^{x_i}} \\right) mstart = Chain( Dense(28^2, 32, relu), Dense(32, 10)) Chain( Dense(784 => 32, relu), # 25_120 parameters Dense(32 => 10), # 330 parameters ) # Total: 4 arrays, 25_450 parameters, 99.664 KiB. In this example, we are working on a classification problem; we are trying to predict a discrete outcome instead of a continuous one. The output of the network above are probabilities that an image represents each of the ten digits. That is, we forming conditional probability, or the likelihood, of $y$ given $x$. In this situation, maximum likelihood is a natural estimator. For discrete $y$ (like we have here), the log likelihood is equal to minus the cross-entropy, so this is what we use as our loss function. loss(m, x, y) = Flux.logitcrossentropy(m(x), y) loss (generic function with 1 method) Since cross-entropy or log likelihood are difficult to interpret, we might want a more intuitive measure of our model\u2019s performance. For classification accuracy is the portion of predictions that are correct. Other measures of classification performance For this application accuracy is likely sufficient, but in some situations (including rare outcomes or when we weight differently type I and type II errors) accuracy is not a sufficient measure of a classifier\u2019s performance. There are variety of other measures, such as precision, recall, and AUC. See @qeclassify for more information . function accuracy(m, x, y) # onecold(m(x)) results in very slow code for large x, so we avoid it coldx = vec(map(x->x[1], argmax(m(x), dims=1))) coldy = onecold(y) return(mean(coldx.==coldy)) end; onecold is the inverse of one-hot-encoding; onecold transforms a matrix of dummy varibles (or probabilities) into an integer (the one with the highest probability in the case of m(x) ). # gradient descent steps using the full X and Y to compute gradients Xsmall=X[:,1:2000] Ysmall=Y[:,1:2000] # accuracy is slower, so only compute on subset of data function evalcb(m) l = loss(m, X, Y) if isnan(l) @show (l) else @show (l, accuracy(m, Xsmall,Ysmall), accuracy(m, tX,tY)) end end; Optimizers Neural networks are usually trained using a variant of gradient descent for optimization. Recall that gradient descent searches for the minimum by taking steps of the form: \\theta_{new} = \\theta - \\eta Df_{\\theta} where $\\eta$ is a step size or learning rate parameter that gets adjusted depending on algorithm progress. There are many variants of gradient descent available in Flux.jl and they differ in how they adjust the learning rate, $\\eta$, and other details. Some algorithms add \u201cmomentum\u201d to avoid the long narrow valley problem we saw in the banana function example in the optimization notes . Ruder (2016) gives a nice overview of various forms of gradient descent. Since Flux.train! might run for a long time, it allows us to pass a \u201ccallback\u201d function that gets evaluated every iteration. Here, this function is just used to monitor progress. In some situations, we might also want to use the callback function to save intermediate results to disk in case the computation gets interrupted before completion. The Flux.throttle function can be used to prevent the call-back function from being evaluated too often. The code below makes evalcb get evaluated at most once every 10 seconds. Flux.train does not automatically check that the optimizer is making progress. With too large of a step size, gradient descent may lead in the wrong direction, increasing the loss function. This can even lead the parameters to drift toward numeric under or overflow and become NaN . If this happens, we should descrease the learning rate. rerun = false modelfile = joinpath(modeldir,\"mnist-slp.jld2\") opt = ADAM() if rerun || !isfile(modelfile) dataset = repeated((X, Y), 1) # each call to Flux.trian! will do 1 m = gpu(mstart) evalcb(m) iterations = 200 losses = zeros(typeof(loss(m,X,Y)), iterations) testloss=similar(losses) @time for i = 1:iterations Flux.train!((x,y)->loss(m,x,y), Flux.params(m), dataset, opt, cb = throttle(()->evalcb(m), 10)) losses[i] = loss(m,X,Y) testloss[i] = loss(m,tX,tY) end plot([losses testloss], xlab=\"Iterations\", ylab=\"Loss\", labels=[\"Train\" \"Test\"]) # save model cpum = cpu(m) @save modelfile cpum else @load modelfile cpum m = gpu(cpum) end Chain( Dense(784 => 32, relu), # 25_120 parameters Dense(32 => 10), # 330 parameters ) # Total: 4 arrays, 25_450 parameters, 576 bytes. @show accuracy(m,Xsmall, Ysmall) @show accuracy(m,tX, tY); accuracy(m, Xsmall, Ysmall) = 0.927 accuracy(m, tX, tY) = 0.9272 After 200 iterations, the accuracy is already greater than 90%. This is pretty good. The test set accuracy is higher than the training set, which could just be good luck, but it is also possible that the model is underfitting. Let\u2019s try training the network longer (doing more gradient descent iterations. rerun = false modelfile = joinpath(modeldir,\"mnist-slp-8200.jld2\") dataset = repeated((X, Y), 200) # each call to Flux.trian! will do 200 # gradient descent steps using the full X and Y to compute gradients if rerun || !isfile(modelfile) evalcb(m) @time @epochs 40 Flux.train!((x,y)->loss(m,x,y), Flux.params(m), dataset, opt, cb = throttle(()->evalcb(m), 10)) evalcb(m) # save model cpum = cpu(m) @save modelfile cpum else @load modelfile cpum m = gpu(cpum) end @show accuracy(m,Xsmall, Ysmall) @show accuracy(m,tX,tY); accuracy(m, Xsmall, Ysmall) = 1.0 accuracy(m, tX, tY) = 0.9603 Remember that each \u201cepoch\u201d does one gradient descent step for each tuple in dataset . In the code above dataset is just the original data repeated 200 times. We ran for 40 epochs, so there were a total of 8000 more gradient descent iterations. We see that the training accuracy has improved to above 99%, but our test accuracy has failed to improve much above 96%. My initial interpretation of this result would be that we are now overfitting. The number of parameters in the network is nparam(m) = sum([length(m[i].weight) + length(m[i].bias) for i in 1:length(m) if typeof(m[i]) <: Dense]) nparam(m) 25450 and there 60000 images. For a typical econometric or statistic problem, there are too many parameters for the number of observations. One solution to this situation is to reduce the number of parameters. Another solution is to do what lasso does and regularize . Lasso regularizes by adding a penalty to the loss function. Limiting the number of gradient descent iterations can also act as a form of regularization. This is often called Landweber regularization . It underlies the common procedure of training a neural network until the training loss starts to be much less than loss on a held out portion of the data (or the loss on the held out portion stops decreasing).","title":"Single Layer Classification"},{"location":"mlp/#deep-classification","text":"Given the apparent overfitting of the single layer network above, I would be reluctant to move to an even more complex model. However, I would be mistaken. If you glance through the MNIST benchmarks on LeCun\u2019s website , you will see that @ciresan2010 achieve a much higher test accuracy with a 6 layer network. Let\u2019s try their network architecture. We will use their numbers of layers and hidden units, but with rectified linear activation. They used tanh activation functions. cmgsnet_cpu = Chain( Dense(28^2, 2500 , relu), Dense(2500, 2000 , relu), Dense(2000, 1500 , relu), Dense(1500, 1000 , relu), Dense(1000, 500 , relu), Dense(500, 10) #softmax ) println(\"cmgsnet has $(nparam(cmgsnet_cpu)) parameters!!!\") cmgsnet has 11972510 parameters!!! That\u2019s a deep network. rerun = false batchsize=10000 parts=Base.Iterators.partition(1:size(X,2), batchsize) data = repeat([(X[:,p], Y[:,p]) for p in parts], 10); # The full data + network doesn't fit in my GPU memory, so do batches epochs = 15 acctest = zeros(epochs) acctrain = zeros(epochs) losstest = zeros(epochs) losstrain = zeros(epochs) opt=ADAM() cmgsnet = gpu(cmgsnet_cpu) for e in 1:epochs modelfile = joinpath(modeldir,\"cmgsnet-$e-epochs.jld2\") if rerun || !isfile(modelfile) println(\"Beginning epoch $e\") evalcb(cmgsnet) @time Flux.train!((x,y)->loss(cmgsnet,x,y), Flux.params(cmgsnet), data, opt, cb = throttle(()->evalcb(cmgsnet), 10)) evalcb(cmgsnet) # save model local cpum = cpu(cmgsnet) @save modelfile cpum else @load modelfile cpum global cmgsnet = gpu(cpum) end println(\"Finished $e epochs\") losstrain[e]=loss(cmgsnet,X,Y) acctrain[e]=accuracy(cmgsnet,Xsmall, Ysmall) losstest[e]=loss(cmgsnet,tX,tY) acctest[e]=accuracy(cmgsnet,tX,tY) end e = epochs modelfile = joinpath(modeldir,\"cmgsnet-$e-epochs.jld2\") @load modelfile cpum cmgsnet = gpu(cpum) Finished 1 epochs Finished 2 epochs Finished 3 epochs Finished 4 epochs Finished 5 epochs Finished 6 epochs Finished 7 epochs Finished 8 epochs Finished 9 epochs Finished 10 epochs Finished 11 epochs Finished 12 epochs Finished 13 epochs Finished 14 epochs Finished 15 epochs Chain( Dense(784 => 2500, relu), # 1_962_500 parameters Dense(2500 => 2000, relu), # 5_002_000 parameters Dense(2000 => 1500, relu), # 3_001_500 parameters Dense(1500 => 1000, relu), # 1_501_000 parameters Dense(1000 => 500, relu), # 500_500 parameters Dense(500 => 10), # 5_010 parameters ) # Total: 12 arrays, 11_972_510 parameters, 1.656 KiB. This model achieved a testing accuracy of 98.49% after 11 training epochs. Each training epoch consisting of 10 passes through the data split into two batches, so 20 gradient descent iterations. Let\u2019s plot the loss and accuracy vs epoch. al = Int(round(3*length(losstrain)/4)) plot( plot([losstrain, losstest], xlab=\"Epochs\", title=\"Cross-Entropy Loss\", annotations=[(al, losstrain[al], Plots.text(\"training\", pointsize=12, valign=:bottom, color=get(cscheme,1))), (al, losstest[al], Plots.text(\"test\", pointsize=12, valign=:bottom, color=get(cscheme,0)))], leg=false, color_palette=get(cscheme,[1,0]) ), plot([acctrain, acctest], xlab=\"Epochs\", title=\"Accuracy\", leg=false, color_palette=get(cscheme,[1,0]) ), layout=(2,1) ) There is really something remarkable going on in this example. A model that appears extremely overparameterized manages to predict very well on a test set. One important thing to keep in mind is that image classification is very different from the typical estimation problems in applied economics. In regressions and other models of economic variables, we never expect to be able to predict perfectly. An $R^2$ of 0.4 in a cross-sectional earnings regression is typical, or even high. Image classification is very different. We know there is a model (our eyes) that can classify nearly perfectly. In the language of econometrics, the error term is zero, or there is no uncertainty, in the \u201ctrue\u201d image classification models. <!-- TODO: Add an aside about adversarial examples and failures to --> <!-- generalize. --> Let\u2019s look at some of the images that our model failed to classify correctly. timgs = [Gray.(1 .-test_x[:,:,i]') for i \u2208 1:size(test_x,3)] tlabels = test_y # predicted labels mlabels = cpu(vec(map(x->x[1], argmax(cmgsnet(tX), dims=1)))) .- 1 @show mean(mlabels.==tlabels) # = accuracy @show sum(mlabels .!= tlabels) miss=findall(mlabels .!= tlabels) plot( [plot(timgs[i], axis=false, ticks=false, title=\"$(tlabels[i]) as $(mlabels[i])\", aspect_ratio=:equal) for i in miss[1:16]]...) mean(mlabels .== tlabels) = 0.9849 sum(mlabels .!= tlabels) = 151 Our model still does not have state-of-the-art accuracy. @ciresan2010 achieves 99.65% accuracy. There are differences in terms of activation function and gradient descent details between @ciresan2010 and the code above. However, I suspect that the main reason for their better performance is that @ciresan2010 generate additional training images. They do this by randomly rotating, stretching, and adding oscillations to the existing images.","title":"Deep Classification"},{"location":"mlp/#data-augmentation","text":"Generating more data from your existing data is called data augmentation. Let\u2019s try augmenting the data by randomly rotating the images. function rotateimage(img, \u03b8) R = similar(img) R .= img[1,1] i0 = (size(img,1)+1)/2 j0 = (size(img,2)+1)/2 for i \u2208 axes(img)[1] for j \u2208 axes(img)[2] ri = Int(round((i-i0)*cos(\u03b8) + (j-j0)*sin(\u03b8) +i0)) rj = Int(round(-(i-i0)*sin(\u03b8) + (j-j0)*cos(\u03b8) +j0)) if (ri \u2208 axes(img)[1] && rj \u2208 axes(img)[2]) R[ri,rj] = img[i,j] end end end return R end plot([plot(timgs[i], axis=false, ticks=false, aspect_ratio=:equal) for i in 1:8]..., [plot(rotateimage(timgs[i], (-1)^i*\u03c0/6), axis=false, ticks=false, aspect_ratio=:equal) for i in 1:8]...) Training with rotated images. rerun = false batchsize=10000 parts=Base.Iterators.partition(1:size(X,2), batchsize) randrotate(x::AbstractVector; max\u03b8 = \u03c0/8) = vec(rotateimage(reshape(x, 28, 28), rand()*2max\u03b8 - max\u03b8)) data = [(gpu(mapslices(randrotate,cpu(X[:,p]), dims=1)), Y[:,p]) for p in parts for i=1:10]; epochs = 10 acctest = zeros(epochs) acctrain = zeros(epochs) losstest = zeros(epochs) losstrain = zeros(epochs) opt=ADAM() cmgsnet = gpu(cmgsnet_cpu) for e in 1:epochs modelfile = joinpath(modeldir,\"cmgsnet-aug-$e-epochs.jld2\") if rerun || !isfile(modelfile) println(\"Beginning epoch $e\") evalcb(cmgsnet) @time Flux.train!((x,y)->loss(cmgsnet,x,y), Flux.params(cmgsnet), data, opt, cb = throttle(()->evalcb(cmgsnet), 10)) evalcb(cmgsnet) # save model local cpum = cpu(cmgsnet) @save modelfile cpum else @load modelfile cpum global cmgsnet = gpu(cpum) end println(\"Finished $e epochs\") losstrain[e]=loss(cmgsnet,X,Y) acctrain[e]=accuracy(cmgsnet,Xsmall, Ysmall) losstest[e]=loss(cmgsnet,tX,tY) acctest[e]=accuracy(cmgsnet,tX,tY) end e = epochs modelfile = joinpath(modeldir,\"cmgsnet-aug-$e-epochs.jld2\") @load modelfile cpum cmgsnet = gpu(cpum) @show maximum(acctest) Finished 1 epochs Finished 2 epochs Finished 3 epochs Finished 4 epochs Finished 5 epochs Finished 6 epochs Finished 7 epochs Finished 8 epochs Finished 9 epochs Finished 10 epochs maximum(acctest) = 0.9882 0.9882 timgs = [Gray.(1 .-test_x[:,:,i]') for i \u2208 1:size(test_x,3)] tlabels = test_y # predicted labels mlabels = cpu(vec(map(x->x[1], argmax(cmgsnet(tX), dims=1)))) .- 1 @show mean(mlabels.==tlabels) # = accuracy @show sum(mlabels .!= tlabels) miss=findall(mlabels .!= tlabels) plot( [plot(timgs[i], axis=false, ticks=false, title=\"$(tlabels[i]) as $(mlabels[i])\", aspect_ratio=:equal) for i in miss[1:16]]...) mean(mlabels .== tlabels) = 0.9882 sum(mlabels .!= tlabels) = 118","title":"Data Augmentation"},{"location":"mlp/#references","text":"","title":"References"},{"location":"nn-semiparametric/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 We have now covered a variety of neural network architectures and seen examples of how they are used for classic machine learning tasks. How can neural networks be incorporated into typical economic research? There are two important differences between empirical economics and the digit classification and text generation examples we looked at: Economic data has a lot more inherent uncertainty. In both images and text, we know there is a nearly perfect model (our eyes and brain). In economics, perfect prediction is almost never possible. For example, if we could predict stock prices even slightly more accurately than a random walk, then we could be billionares. In part related to 1, economists care about statistical inference and quantifying uncertainty. Both of these factors are going to make concerns about overfitting and model complexity more important. This note will focus on one way to use neural networks in estimable economic models that allows inference. Inference directly on the nonparametric function fitted by a neural network is a very difficult problem. There are practical methods for Bayesian variational inference (see @graves2011 or @miao2016), but frequentist inference is largely an open problem. However, what is possible is semiparametric inference. Semiparametric inference refers to when we have a finite dimensional parameter of interest that depends on some infinite dimensional parameter. Examples include average (or quantiles or other summary statistic) marginal effects, average treatment effects, and many others. Double Debiased Machine Learning \u00b6 We return to the semiparametric moment condition model of @chernozhukov2018 and @chernozhukov2017. We previously discussed this setting in ml-intro and ml-doubledebiased . The model consists of Parameter of interest $\\theta \\in \\R^{d_\\theta}$ Nuisance parameter $\\eta \\in T$ (where $T$ is typically some space of functions) Moment conditions \\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta} where $\\psi$ known See ml-intro and ml-doubledebiased for examples. The estimation procedure will proceed in two steps: Estimate $\\hat{\\eta}$ from a neural network Estimate $\\hat{\\theta}$ from the empirical moment condition with $\\hat{\\eta}$ plugged in: \\En[\\psi(W;\\hat{\\theta},\\hat{\\eta}) ] \\approx 0 @chernozhukov2018 provide high level assumptions on $\\hat{\\eta}$ and the model that ensure $\\hat{\\theta}$ is $\\sqrt{n}$ asymptotically normal. 1 The key needed assumptions are: Linearizeable score \\psi(w;\\theta,\\eta) = \\psi^a(w;\\eta) \\theta + \\psi^b(w;\\eta) + o_p(?) (Near) Neyman orthogonality: \\lambda_n := \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{\\partial \\eta \\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right] } \\leq \\delta_n n^{-1/2} Fast enough convergence of $\\hat{eta}$: for $\\delta_n \\to 0$ and $\\Delta_n \\to 0$, we have $\\Pr(\\hat{\\eta}_k \\in \\mathcal{T}_n) \\geq 1-\\Delta_n$ and \\begin{align*} r_n := & \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{ \\Er[\\psi^a(W;\\eta)] - \\Er[\\psi^a(W;\\eta_0)]} \\leq \\delta_n \\\\ r_n' := & \\sup_{\\eta \\in \\mathcal{T}_n} \\Er\\left[ \\norm{ \\psi(W;\\theta_0,\\eta) - \\psi(W;\\theta_0,\\eta_0)}^2 \\right]^{1/2} \\leq \\delta_n \\\\ \\lambda_n' := & \\sup_{r \\in (0,1), \\eta \\in \\mathcal{T}_n} \\norm{ \\partial_r^2 \\Er\\left[\\psi(W;\\theta_0, \\eta_0 + r(\\eta - \\eta_0)) \\right]} \\leq \\delta_n/\\sqrt{n} \\end{align*} Assumption 1 is stated here mostly to define notation 3. A model where $\\psi$ is not linearizeable would be unusual. Assumption 2 must be satisfied by each application. In many models, the most obvious choice of $\\psi$ will not satisfy 2. However, $\\psi$ can be orthogonalized to satisfy 2. Assumption 3 is about the convergence rate of our estimator for $\\hat{\\eta}$. It is written this way because it exactly what is needed for the proof; it is not intended to be easy to interpret or to verify. A sufficient, but not necessary condition that implies 3 is twice differentiability of $\\psi$ and $\\Er[(\\hat{\\eta}(x) - \\eta_0(x))^2]^{1/2} = o(n^{-1/4})$. Anyway, @chernozhukov2018 show that under these conditions (and if you use sample splitting in the empirical moment condition), then \\sqrt{n} \\sigma^{-1} (\\hat{\\theta} - \\theta_0) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\bar{\\psi}(w_i) + O_p(\\rho_n) \\leadsto N(0,I) where \\rho_n := n^{-1/2} + r_n + r_n' + n^{1/2} (\\lambda_n +\\lambda_n') \\lesssim \\delta_n and $\\bar{\\psi}$ is the influence function, \\bar{\\psi}(w) = -\\sigma^{-1} J_0^{-1} \\psi(w;\\theta_0,\\eta_0) with \\sigma^2 = J_0^{-1} \\Er\\left[ \\psi(w;\\theta_0,\\eta_0) \\psi(w;\\theta_0,\\eta_0)'\\right] (J_0^{-1})'. This is the same asymptotic distribution as if we plugged in the true $\\eta_0$ instead of our estimated $\\hat{\\eta}$. To apply this result to neural networks we need two things. First, we need to verify the rate condition (assumption 2). Doing so will involve conditions on the class of functions being estimated, and how the complexity (width and depth) of the neural network increases with sample size. The excellent paper by @farrel2021 (working paper @farrel2018) provides the needed results. Second, we need to make sure our moment conditions are Neyman orthogonal. @chernozhukov2018 and @farrel2021 do this for some typical causal inference models. For other models, analytically transforming non-orthogonal moments into orthogonal ones is typically possible, but it can be tedious. <!-- We will try to automate this. As we saw in --> <!-- [GMMInference](https://schrimpf.github.io/GMMInference.jl), Julia's --> <!-- good support for automatic differentiation will make it relatively --> <!-- easy to translate econometric theory to executable code. --> Deep Neural Networks for Estimation and Inference \u00b6 Review results of @farrel2021. Application: Average Impulse Responses \u00b6 As an application let\u2019s consider something like an average impulse response function. Suppose we have some time series data on $y_t$ and $x_t$. We want to estimate the average (over the density of $x$) response of $\\Er[y_{t+\\tau} | x_t]$ to a change in $x$ of size $\\Delta$. That is, our parameters of interest are \\theta_{0,\\tau} = \\Er\\left[ \\Er[y_{t+\\tau} | x_{t} + \\Delta] - \\Er[y_{t+\\tau} | x_{t}] \\right] for $\\tau = 0, \u2026, R$ for some fixed $R$. Note that there are other impulse response like parameters that could be estimated. For example, we could compute the average over $x_t$ of the change in future $y$ holding future residuals constant (or setting future residuals to $0$). In a linear model this would be the same as the average over residuals response that we estimate. However, in a nonlinear model, these three things differ. We focus on the average over residuals response in part because orthogonalization of the moment condition is more difficult for the later two. Let $h(x_t) = (\\Er[y_{t} | x_{t}], \u2026 , \\Er[y_{t+R} | x_{t}])$ denote the conditional expectation functions at different horizons. Then, we can write a moment condition for $\\theta_0$ as: 0 = \\Er[ \\theta_0 - \\left(h(x_t + \\Delta) - h(x_t) \\right)]. However, this moment condition is not orthogonal. Its Frechet derivative with respect to $h$ in direction $v$ is \\partial_h\\left(\\Er[ \\theta - \\left(h_0(x_t + \\Delta) - h(x_t) \\right)] \\right) [v] = \\Er[-v(x_t + \\Delta) + v(x_t)] \\neq 0. We can orthogonalize the moment condition by following the concentrating out approach described in @chernozhukov2018, or in ml-doubledebiased . It would be a good exercise to work through the steps. Here we will simply state a resulting orthogonal moment condition. Let $\\eta=(h, f_x)$ where $f_x$ is the density of $x$. Define \\psi(y,x;\\theta, \\eta) = \\theta - (h(x+\\Delta) - h(x))) + (y - h(x))\\frac{-f_x(x-\\Delta)+f_x(x)} {f_x(x)} Although somewhat difficult to derive, it is easy to verify that $\\Er \\psi$ is orthogonal. \\begin{align*} \\partial_h \\left(\\Er[ \\psi(y_t,x_t;\\theta_0, \\eta_0) ] \\right) [v] = & \\Er\\left[-v(x_t + \\Delta) + v(x_t) + v(x_t +\\Delta) - v(x_t)\\right] = 0 \\\\ \\partial_{f_x} \\left(\\Er[ \\psi(y_t,x_t;\\theta_0, \\eta_0) ] \\right) [v] = & \\Er\\left[(E[y|x] - h_0(x)) \\frac{(-v(x+\\Delta) + v(x))f_x(x) - v(x)}{f_x(x)^2} \\right] = 0 \\end{align*} Notice that to achieve orthogonality, we had to introduce an additional functional nuisance parameter, $f_x$. This is typical in these models. Simulated DGP \u00b6 Let\u2019s do some simulations to examine the performance of this estimator. The true model will be a nonlinear AR(3) model. In particular, y_t = \\tanh(\\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + \\alpha_3 y_{t-3}) + \\epsilon_t using Polynomials, Plots, LinearAlgebra, Statistics, Random Random.seed!(747697) T = 1000 R = 50 \u03b8 = 0.1 r = 0.99 # We start with an AR(P). We specify coefficients by choosing the # roots of the AR polynomial. In a linear model, complex roots on the # unit circle lead to non-vanishing cycles in y. roots = [r*exp(im*\u03b8), r*exp(-im*\u03b8), 0.5] p = prod([Polynomial([1, -r]) for r in roots]) \u03b1 = -real(coeffs(p))[2:end] function mean_y_fn(\u03b1, transform=(x)->x) ylag->transform(dot(ylag, \u03b1)) end function dgp(mean_y, T, sig=1, yinit=zeros(length(\u03b1))) ylag = yinit y = zeros(T) for t in 1:T y[t] = muladd(sig, randn(),mean_y(ylag) ) ylag[2:end] .= ylag[1:(end-1)] ylag[1] = y[t] end return(y) end \u03bcy = mean_y_fn(\u03b1, x->1/r*tanh(x)) y = dgp(\u03bcy, T, 1.0) plot(y, title=\"Simulated y\u209c\", leg=false) R = 12 L = length(\u03b1) \u0394 = 1.0*std(y) nsim = 1000 iry=reduce(hcat,[mean(x->(dgp(\u03bcy , R, 1.0, y[i:(i+L-1)] .+ [\u0394, 0, 0]) .- dgp(\u03bcy , R, 1.0, y[i:(i+L-1)]) ), 1:nsim) for i in 1:(T-length(\u03b1)+1)])./std(y) plot(iry, leg=false, alpha=0.01, color=:black, linewidth=1) plot!(mean(iry, dims=2), leg=false, linewidth=5, title=\"E[\u0394y | 1\u03c3 change in y]\", xlab=\"\u03c4\", ylab=\"\u0394E[y(t+\u03c4)]/std(y)\", alpha=1) The conditional average impulse responses at each $x_t=(y_{t-1}, \u2026, y_{t-p})$ in the simulated data are in grey. The average over $x_t$ is the thicker green-ish line. In a linear model the average and conditional impulse responses coincide. In this model, they differ because of the nonlinearity. Estimators \u00b6 To somewhat simplify this exercise, we will assume that the researcher knows that $y$ follows a nonlinear AR(3) model. That is, we know that y_t = h(y_{t-1}, y_{t-2}, y_{t-3}) + \\epsilon_t We will estimate $h$ using this knowledge. To estimate the impulse response, we need estimates of the conditional expectation of $y$ and the density of $x_t = (y_{t-1}, y_{t-2}, y_{t-3})$. Density \u00b6 We will estimate the density as the derivative of a feed forward network. We fit the a feed forward network to the empirical cdf. We penalize the estimated cdf for not being monotone. Here is code to fit the model. using Flux, ProgressMeter, JLD2 import Base.Iterators: partition L = length(\u03b1) x = copy(reduce(hcat,[y[l:(end-L+l)] for l in L:-1:1])') function fit_cdf!(model, x; opt=Flux.NADAM(), maxiter=1000, batchsize=length(x), \u03bb = eltype(x)(1.0), n_extra=0) # we could augment cdf evaluation points with x not seen in the data # in 1-d, there is no need, but in multiple dimension, there's extra info. i = 0 X = x if (n_extra>0) X=hcat(x, rand(x, size(x,1), n_extra)) end ecdf = eltype(x).(mapslices( xi->mean(all(X .<= xi, dims=1)), X, dims=1 )) \u0394 = gpu(diagm(fill(eltype(x)(0.01), size(x,1)))) monotone_penalty(x,cdf) = \u03bb*sum(\u03b4->sum( relu.(model(x) .- model(x .+ \u03b4))), eachcol(\u0394)) loss(x,cdf) = Flux.mse(model(x), cdf) + monotone_penalty(x, cdf) @show bestloss = loss(X,ecdf)+1 lastimprove=0 p = Progress(maxiter, 1) data = [(X[:,p], ecdf[:,p]) for p in partition(1:size(X,2), batchsize)] for i in 1:maxiter Flux.train!(loss, Flux.params(model), data, opt) obj = loss(X, ecdf) next!(p) (i%(maxiter \u00f7 10) == 0) && println(\"\\n$i : $obj\") if (obj < bestloss) bestloss=obj lastimprove=i end if (i - lastimprove > 100) @warn \"no improvement for 100 iterations, stopping\" break end end return(model) end fit_cdf! (generic function with 1 method) We can recover the pdf from the cdf by differentiating. For univariate distributions, this is easy. However, for multivariate distributions, f_x(x) = \\frac{\\partial^n}{\\partial x_1 \\partial x_2\\cdots \\partial x_n} F_x(x) so we need to evaluate an $n$th order derivative. Here is some code to do so. using ForwardDiff function deriv_i(f, i) # derivative wrt to ith argument dfi(z)=ForwardDiff.derivative((y)->f([z[1:(i-1)]..., y, z[(i+1):end]...]), z[i]) end function make_pdf(cdf, dim) fs = Function[] push!(fs, x->cdf(x)) for i in 1:dim push!(fs, deriv_i(fs[end], i)) end function f(x::AbstractVector) fs[end](x) end function f(x::AbstractMatrix) mapslices(fs[end], x, dims=1) end return(f) end make_pdf (generic function with 1 method) To check that the code produces reasonable results, we will begin by just fitting the marginal distribution of $y_t$. This isn\u2019t quite what we need for our impulse responses, but it is easier to visualize. modelfile=joinpath(docdir,\"jmd\",\"models\",\"cdfy.jld2\") rerun=false if isfile(modelfile) && !rerun @load modelfile cdfy else cdf_model = Chain( Dense(1, 8, Flux.\u03c3), #Dense(32, 16, relu), #Dense(16, 8, relu), Dense(8, 1, Flux.\u03c3) ) |> gpu fit_cdf!(cdf_model, gpu(reshape(y, 1, T)), maxiter=10000, batchsize=1000) # The next line converts the parameters of the model to Floats instead # of Tracked. cdfy = cpu(cdf_model) @save modelfile cdfy end pdfy = make_pdf(cdfy, 1) fig=histogram(y, bins=100, normalize=:pdf) fig=scatter!(y, pdfy(reshape(y,1,T))[:], leg=false) Error: Scalar indexing is disallowed. Invocation of setindex! resulted in scalar indexing of a GPU array. This is typically caused by calling an iterating implementation of a method . Such implementations *do not* execute on the GPU, but very slowly on the CP U, and therefore are only permitted from the REPL for prototyping purposes. If you did intend to index this array, annotate the caller with @allowscala r. The estimated pdf looks pretty good. We will estimate the joint cdf and pdf similarly. rerun=false modelfile=joinpath(docdir,\"jmd\",\"models\",\"cdfx.jld2\") if isfile(modelfile) && !rerun @load modelfile cdfx else cdf_model = Chain( Dense(3, 32, Flux.\u03c3), Dense(32, 16, Flux.\u03c3), Dense(16, 8, Flux.\u03c3), Dense(8, 1, Flux.\u03c3) ) |> gpu @time fit_cdf!(cdf_model, gpu(Float32.(x)), opt=NADAM(), maxiter=50000, batchsize=1000, \u03bb=10.0f0, n_extra=10000) # The next line converts the parameters of the model to Floats instead # of Tracked. cdfx = cpu(Flux.mapleaves(Flux.data, cdf_model)) @save modelfile cdfx end pdfx = make_pdf(cdfx, size(x,1)) @show sum(pdfx(x).<0) histogram(pdfx(x)', bins=100, leg=false, title=\"Histogram of estimated pdf(x)\") Error: Scalar indexing is disallowed. Invocation of getindex resulted in scalar indexing of a GPU array. This is typically caused by calling an iterating implementation of a method . Such implementations *do not* execute on the GPU, but very slowly on the CP U, and therefore are only permitted from the REPL for prototyping purposes. If you did intend to index this array, annotate the caller with @allowscala r. Despite the penalty, our estimated pdf is sometimes negative. Fortunately, it doesn\u2019t happen to often. It\u2019s hard to visualize the estimated trivariate pdf, but we can plot the marginal pdf implied by the joint distribution. We can get marginal pdf by integrating or looking at the derivative of the joint cdf. f_{x_1}(x_1) = \\frac{\\partial}{\\partial x_1} F_x((x_1, \\overline{x}_2, \\overline{x}_3)) where $\\overline{x}_k$ is the maximum of the support of $x_k$. pdf12=make_pdf(cdfx,2) pdf1=make_pdf(cdfx,1) px1=pdf1(vcat(x[1,:]', maximum(y)*ones(2, 998))) histogram(x[1,:], bins=100, normalize=:pdf) scatter!(x[1,:], px1[:], leg=false) Error: UndefVarError: cdfx not defined The marginal pdf implied by the joint cdf looks pretty good. Note, however, that to get this result, I had to specify a fairly complex network, which took around 20 minutes to fit. There are many other ways to estimate densities, and it seems to me that another approach might have made more sense here. Conditional expectation \u00b6 We will assume that we that knows that $y$ follows a nonlinear AR(3) model. Then, we can simply fit a feed forward network to predict $y_t$ given $x_t = (y_{t-1}, y_{t-2}, y_{t-3})$. function fit_ce!(model, x, y; opt=Flux.NADAM(), maxiter=1000, batchsize=length(y)) loss(x,y) = Flux.mse(model(x), y) @show bestloss = loss(x,y)+1 lastimprove=0 p = Progress(maxiter, 1) data = [(x[:,p], y[:,p]) for p in partition(1:size(x,2), batchsize)] for i in 1:maxiter Flux.train!(loss, Flux.params(model), data, opt) obj = loss(x,y) next!(p) (i%(maxiter \u00f7 10) == 0) && println(\"\\n$i : $obj\") if (obj < bestloss) bestloss=obj lastimprove=i end if (i - lastimprove > 100) @warn \"no improvement for 100 iterations, stopping\" break end end return(model) end R = 12 # how many periods ahead to fit Y = copy(reduce(hcat,[y[(L+r):(end-R+r)] for r in 1:R])') X = x[:,1:(end-R)] modelfile=joinpath(docdir,\"jmd\",\"models\",\"ce.jld2\") rerun=false if !isfile(modelfile) || rerun ce_model = Chain(Dense(size(X,1), 8, relu), Dense(8, 8, relu), Dense(8, size(Y,1))) |> gpu fit_ce!(ce_model, gpu(Float32.(X)), gpu(Float32.(Y)), opt=Flux.NADAM(), maxiter=20000, batchsize=size(Y,2)) cpum = cpu(ce_model) @save modelfile cpum end @load modelfile cpum ce_model = gpu(cpum) \u03bchat(x) = let m=cpu(ce_model) m(x) end \u03bchat(x::AbstractVector) = let m=cpu(Flux.mapleaves(Flux.data, ce_model)) eltype(x)(m(x)[1]) end bestloss = loss(x, y) + 1 = 2.9970849f0 2000 : 1.6363429 4000 : 1.619115 6000 : 1.6138682 8000 : 1.612884 10000 : 1.6124394 12000 : 1.6119322 14000 : 1.6117342 16000 : 1.6112732 \u03bchat (generic function with 2 methods) Finally, we can calculate the orthogonalized impulse responses. As stated above, these are given by \\theta = (h(x+\\Delta) - h(x))) + (y - h(x))\\frac{-f_x(x-\\Delta)+f_x(x)} {f_x(x)} Let\u2019s compute it L = size(x,1) \u0394 = 1.0*std(y) iryhat=(\u03bchat(X .+ [\u0394; 0; 0]) .- \u03bchat(X)) fx(x) = pdfx(x) #max.(pdfx(x), 1e-6) \u2113(x) = (fx(X) .- fx(X .- [\u0394; 0; 0]))./fx(X) lx = \u2113(x)[:] c = quantile(lx, [0.02, 0.98]) idx = Int.(findall((c[1] .<= lx) .& (lx .<= c[2]))) \u03b8i = \u03bchat(X .+ [\u0394; 0; 0]) .- \u03bchat(X) .+ (Y .- \u03bchat(X)).*\u2113(X) #\u03b8i = \u03b8i[:,idx] Error: UndefVarError: pdfx not defined and plot it plot(iry, leg=false, alpha=0.02, color=:black, linewidth=1) plot!(iryhat, leg=false, alpha=0.02, color=:orange, linewidth=1, ylim=(-0.5, 0.75)) #plot!(\u03b8i, leg=false, alpha=0.01, color=:green, linewidth=1, ylim=(-0.5, 0.75)) plot!(mean(iry, dims=2), leg=false, linewidth=5, title=\"Average Impulse-Reponse to 1\u03c3 change in y[t-1]\", xlab=\"\u03c4\", ylab=\"\u0394E[y(t+\u03c4)]/std(y)\", color=:blue) plot!(mean(iryhat, dims=2), leg=false, linewidth=5, color=:orange) plot!(mean(\u03b8i, dims=2), leg=false, linewidth=5, color=:green) Error: UndefVarError: \u03b8i not defined The true average impulse response is in blue. The uncorrected estimate is in orange. The orthogonalized estimate is in green. Depending on the luck of RNG, and how well I have chosen the network architectures, the orthogonalized or naive point estimate might look better. However, the motivation is the orthogonalization is not so much to improve point estimation (although it does that as a side effect), but to enable inference. Inference \u00b6 Inference for the orthogonalized estimator is very simple. Due to the orthogonalization, we can treat \u03bchat and fx as though they are known functions. The estimated average impulse response is then just a sample average. Computing standard errors for sample averages is straightforward. Since this data is dependent, we will use a HAC estimator for the variance. using Distributions using CovarianceMatrices # need dev version k = BartlettKernel{NeweyWest}() \u03a3 = lrvar(k, \u03b8i', prewhite=true) plot(iry, leg=false, alpha=0.02, color=:black, linewidth=1) plot!(iryhat, leg=false, alpha=0.02, color=:orange, linewidth=1, ylim=(-0.5, 0.75)) #plot!(\u03b8i, leg=false, alpha=0.01, color=:green, linewidth=1, ylim=(-0.5, 0.75)) plot!(mean(iry, dims=2), leg=false, linewidth=5, title=\"Average Impulse-Reponse to 1\u03c3 change in y[t-1]\", xlab=\"\u03c4\", ylab=\"\u0394E[y(t+\u03c4)]/std(y)\", color=:blue) plot!(mean(iryhat, dims=2), leg=false, linewidth=5, color=:orange) plot!(mean(\u03b8i, dims=2), leg=false, linewidth=5, color=:green, ribbon=sqrt.(diag(\u03a3)/size(\u03b8i,2))*quantile.(Normal(), [0.05, 0.95])' ) Error: UndefVarError: \u03b8i not defined The plot now has 90% pointwise confidence bands around the orthogonalized estimates. Automating Orthogonalization \u00b6 A downside of the above approach is it requires an orthogonal moment conditioon for each parameter of interest. Analytic orthogonalization is labor intensive, error prone, and may not even be possible in some cases. @ceinr2016 review some techniques for constructing orthogonal moments. @cnr2018 have a mathematically elegant method for automatically constructing orthogonal moments. The focus on estimators that are linear in their nonparametric component. \\theta_0 = \\Er[m(x,\\gamma_0(x))] where $\\gamma(x)$ is an estimate of $\\Er[y|x]$, $m()$ is a known function, $m$ is linear in $\\gamma$, and $\\theta \\in \\R$. If we focus on one horizon, our example above falls into this setup. Anyway, since $m$ is linear, $\\gamma \\to \\Er[m(x,\\gamma(x))]$ is a linear functional. If $\\gamma \\in V$, a vector space of functions, then by definition, $\\exists \\alpha^\\ast \\in V^\\ast$ such that \\alpha^\\ast \\gamma = \\Er[m(x,\\gamma(x))] for all $\\gamma \\in V$. Without more structure on $V^\\ast$, this is little more than alternate notation for $\\Er[m(x,\\gamma(x))]$. Fortunately in most applications, an appropriate space for $\\Er[y|x]$ is $V=\\mathcal{L}^2(P_x)$. In this case, $V = V^\\ast$, and we know that $\\alpha^\\ast$ is of the form \\alpha^\\ast \\gamma = \\Er[\\gamma(x) \\alpha^\\ast(x)] In the example from the previous section, $\\alpha^\\ast$ can be explicitly calculated. It is \\alpha^\\ast(x) = \\frac{-f_x(x-\\Delta)+f_x(x)} {f_x(x)}. In the new notation of this section, the orthogonal moment condition we used in the previous section becomes \\theta = \\Er[ m(x, \\gamma(x)) + \\alpha^\\ast(x)(y - \\gamma(x)) It is straightforward to verify that this moment condition is orthogonal for any $m$ that is linear in $\\gamma$. The above observations can be turned into an estimator by first fixing an approximing space for $V$, $V_n$ (e.g. the set neural network with a given architecture). Then estimate $\\gamma$ as above \\hat{\\gamma} = \\argmin_{\\gamma \\in V_n} \\En[ (y-\\gamma(x))^2 ]. Estimate $\\alpha$ by solving \\hat{\\alpha} = \\argmin_{\\alpha \\in V_n} \\sup_{\\gamma \\in V_n} \\left(\\En[m(x,\\gamma(x))] - \\En[\\alpha(x) \\gamma(x)] \\right)^2 @cnr2018 work with $V_n$ that are linear in parameters, which helps simplify the estimation of $\\alpha$. A practical and efficient method for estimating $\\alpha$ with neural networks would require some thought. Finally, plug-in the estimates of $\\gamma$ and $\\alpha$ and take an average to compute $\\hat{\\theta}$. @chernozhukov2018 also give low level conditions on lasso estimates of $\\hat{eta}$ that meet the high level assumptions. \u21a9","title":"In semiparametric models"},{"location":"nn-semiparametric/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min}","title":"About this document"},{"location":"nn-semiparametric/#introduction","text":"We have now covered a variety of neural network architectures and seen examples of how they are used for classic machine learning tasks. How can neural networks be incorporated into typical economic research? There are two important differences between empirical economics and the digit classification and text generation examples we looked at: Economic data has a lot more inherent uncertainty. In both images and text, we know there is a nearly perfect model (our eyes and brain). In economics, perfect prediction is almost never possible. For example, if we could predict stock prices even slightly more accurately than a random walk, then we could be billionares. In part related to 1, economists care about statistical inference and quantifying uncertainty. Both of these factors are going to make concerns about overfitting and model complexity more important. This note will focus on one way to use neural networks in estimable economic models that allows inference. Inference directly on the nonparametric function fitted by a neural network is a very difficult problem. There are practical methods for Bayesian variational inference (see @graves2011 or @miao2016), but frequentist inference is largely an open problem. However, what is possible is semiparametric inference. Semiparametric inference refers to when we have a finite dimensional parameter of interest that depends on some infinite dimensional parameter. Examples include average (or quantiles or other summary statistic) marginal effects, average treatment effects, and many others.","title":"Introduction"},{"location":"nn-semiparametric/#double-debiased-machine-learning","text":"We return to the semiparametric moment condition model of @chernozhukov2018 and @chernozhukov2017. We previously discussed this setting in ml-intro and ml-doubledebiased . The model consists of Parameter of interest $\\theta \\in \\R^{d_\\theta}$ Nuisance parameter $\\eta \\in T$ (where $T$ is typically some space of functions) Moment conditions \\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta} where $\\psi$ known See ml-intro and ml-doubledebiased for examples. The estimation procedure will proceed in two steps: Estimate $\\hat{\\eta}$ from a neural network Estimate $\\hat{\\theta}$ from the empirical moment condition with $\\hat{\\eta}$ plugged in: \\En[\\psi(W;\\hat{\\theta},\\hat{\\eta}) ] \\approx 0 @chernozhukov2018 provide high level assumptions on $\\hat{\\eta}$ and the model that ensure $\\hat{\\theta}$ is $\\sqrt{n}$ asymptotically normal. 1 The key needed assumptions are: Linearizeable score \\psi(w;\\theta,\\eta) = \\psi^a(w;\\eta) \\theta + \\psi^b(w;\\eta) + o_p(?) (Near) Neyman orthogonality: \\lambda_n := \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{\\partial \\eta \\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right] } \\leq \\delta_n n^{-1/2} Fast enough convergence of $\\hat{eta}$: for $\\delta_n \\to 0$ and $\\Delta_n \\to 0$, we have $\\Pr(\\hat{\\eta}_k \\in \\mathcal{T}_n) \\geq 1-\\Delta_n$ and \\begin{align*} r_n := & \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{ \\Er[\\psi^a(W;\\eta)] - \\Er[\\psi^a(W;\\eta_0)]} \\leq \\delta_n \\\\ r_n' := & \\sup_{\\eta \\in \\mathcal{T}_n} \\Er\\left[ \\norm{ \\psi(W;\\theta_0,\\eta) - \\psi(W;\\theta_0,\\eta_0)}^2 \\right]^{1/2} \\leq \\delta_n \\\\ \\lambda_n' := & \\sup_{r \\in (0,1), \\eta \\in \\mathcal{T}_n} \\norm{ \\partial_r^2 \\Er\\left[\\psi(W;\\theta_0, \\eta_0 + r(\\eta - \\eta_0)) \\right]} \\leq \\delta_n/\\sqrt{n} \\end{align*} Assumption 1 is stated here mostly to define notation 3. A model where $\\psi$ is not linearizeable would be unusual. Assumption 2 must be satisfied by each application. In many models, the most obvious choice of $\\psi$ will not satisfy 2. However, $\\psi$ can be orthogonalized to satisfy 2. Assumption 3 is about the convergence rate of our estimator for $\\hat{\\eta}$. It is written this way because it exactly what is needed for the proof; it is not intended to be easy to interpret or to verify. A sufficient, but not necessary condition that implies 3 is twice differentiability of $\\psi$ and $\\Er[(\\hat{\\eta}(x) - \\eta_0(x))^2]^{1/2} = o(n^{-1/4})$. Anyway, @chernozhukov2018 show that under these conditions (and if you use sample splitting in the empirical moment condition), then \\sqrt{n} \\sigma^{-1} (\\hat{\\theta} - \\theta_0) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\bar{\\psi}(w_i) + O_p(\\rho_n) \\leadsto N(0,I) where \\rho_n := n^{-1/2} + r_n + r_n' + n^{1/2} (\\lambda_n +\\lambda_n') \\lesssim \\delta_n and $\\bar{\\psi}$ is the influence function, \\bar{\\psi}(w) = -\\sigma^{-1} J_0^{-1} \\psi(w;\\theta_0,\\eta_0) with \\sigma^2 = J_0^{-1} \\Er\\left[ \\psi(w;\\theta_0,\\eta_0) \\psi(w;\\theta_0,\\eta_0)'\\right] (J_0^{-1})'. This is the same asymptotic distribution as if we plugged in the true $\\eta_0$ instead of our estimated $\\hat{\\eta}$. To apply this result to neural networks we need two things. First, we need to verify the rate condition (assumption 2). Doing so will involve conditions on the class of functions being estimated, and how the complexity (width and depth) of the neural network increases with sample size. The excellent paper by @farrel2021 (working paper @farrel2018) provides the needed results. Second, we need to make sure our moment conditions are Neyman orthogonal. @chernozhukov2018 and @farrel2021 do this for some typical causal inference models. For other models, analytically transforming non-orthogonal moments into orthogonal ones is typically possible, but it can be tedious. <!-- We will try to automate this. As we saw in --> <!-- [GMMInference](https://schrimpf.github.io/GMMInference.jl), Julia's --> <!-- good support for automatic differentiation will make it relatively --> <!-- easy to translate econometric theory to executable code. -->","title":"Double Debiased Machine Learning"},{"location":"nn-semiparametric/#deep-neural-networks-for-estimation-and-inference","text":"Review results of @farrel2021.","title":"Deep Neural Networks for Estimation and Inference"},{"location":"nn-semiparametric/#application-average-impulse-responses","text":"As an application let\u2019s consider something like an average impulse response function. Suppose we have some time series data on $y_t$ and $x_t$. We want to estimate the average (over the density of $x$) response of $\\Er[y_{t+\\tau} | x_t]$ to a change in $x$ of size $\\Delta$. That is, our parameters of interest are \\theta_{0,\\tau} = \\Er\\left[ \\Er[y_{t+\\tau} | x_{t} + \\Delta] - \\Er[y_{t+\\tau} | x_{t}] \\right] for $\\tau = 0, \u2026, R$ for some fixed $R$. Note that there are other impulse response like parameters that could be estimated. For example, we could compute the average over $x_t$ of the change in future $y$ holding future residuals constant (or setting future residuals to $0$). In a linear model this would be the same as the average over residuals response that we estimate. However, in a nonlinear model, these three things differ. We focus on the average over residuals response in part because orthogonalization of the moment condition is more difficult for the later two. Let $h(x_t) = (\\Er[y_{t} | x_{t}], \u2026 , \\Er[y_{t+R} | x_{t}])$ denote the conditional expectation functions at different horizons. Then, we can write a moment condition for $\\theta_0$ as: 0 = \\Er[ \\theta_0 - \\left(h(x_t + \\Delta) - h(x_t) \\right)]. However, this moment condition is not orthogonal. Its Frechet derivative with respect to $h$ in direction $v$ is \\partial_h\\left(\\Er[ \\theta - \\left(h_0(x_t + \\Delta) - h(x_t) \\right)] \\right) [v] = \\Er[-v(x_t + \\Delta) + v(x_t)] \\neq 0. We can orthogonalize the moment condition by following the concentrating out approach described in @chernozhukov2018, or in ml-doubledebiased . It would be a good exercise to work through the steps. Here we will simply state a resulting orthogonal moment condition. Let $\\eta=(h, f_x)$ where $f_x$ is the density of $x$. Define \\psi(y,x;\\theta, \\eta) = \\theta - (h(x+\\Delta) - h(x))) + (y - h(x))\\frac{-f_x(x-\\Delta)+f_x(x)} {f_x(x)} Although somewhat difficult to derive, it is easy to verify that $\\Er \\psi$ is orthogonal. \\begin{align*} \\partial_h \\left(\\Er[ \\psi(y_t,x_t;\\theta_0, \\eta_0) ] \\right) [v] = & \\Er\\left[-v(x_t + \\Delta) + v(x_t) + v(x_t +\\Delta) - v(x_t)\\right] = 0 \\\\ \\partial_{f_x} \\left(\\Er[ \\psi(y_t,x_t;\\theta_0, \\eta_0) ] \\right) [v] = & \\Er\\left[(E[y|x] - h_0(x)) \\frac{(-v(x+\\Delta) + v(x))f_x(x) - v(x)}{f_x(x)^2} \\right] = 0 \\end{align*} Notice that to achieve orthogonality, we had to introduce an additional functional nuisance parameter, $f_x$. This is typical in these models.","title":"Application: Average Impulse Responses"},{"location":"nn-semiparametric/#simulated-dgp","text":"Let\u2019s do some simulations to examine the performance of this estimator. The true model will be a nonlinear AR(3) model. In particular, y_t = \\tanh(\\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + \\alpha_3 y_{t-3}) + \\epsilon_t using Polynomials, Plots, LinearAlgebra, Statistics, Random Random.seed!(747697) T = 1000 R = 50 \u03b8 = 0.1 r = 0.99 # We start with an AR(P). We specify coefficients by choosing the # roots of the AR polynomial. In a linear model, complex roots on the # unit circle lead to non-vanishing cycles in y. roots = [r*exp(im*\u03b8), r*exp(-im*\u03b8), 0.5] p = prod([Polynomial([1, -r]) for r in roots]) \u03b1 = -real(coeffs(p))[2:end] function mean_y_fn(\u03b1, transform=(x)->x) ylag->transform(dot(ylag, \u03b1)) end function dgp(mean_y, T, sig=1, yinit=zeros(length(\u03b1))) ylag = yinit y = zeros(T) for t in 1:T y[t] = muladd(sig, randn(),mean_y(ylag) ) ylag[2:end] .= ylag[1:(end-1)] ylag[1] = y[t] end return(y) end \u03bcy = mean_y_fn(\u03b1, x->1/r*tanh(x)) y = dgp(\u03bcy, T, 1.0) plot(y, title=\"Simulated y\u209c\", leg=false) R = 12 L = length(\u03b1) \u0394 = 1.0*std(y) nsim = 1000 iry=reduce(hcat,[mean(x->(dgp(\u03bcy , R, 1.0, y[i:(i+L-1)] .+ [\u0394, 0, 0]) .- dgp(\u03bcy , R, 1.0, y[i:(i+L-1)]) ), 1:nsim) for i in 1:(T-length(\u03b1)+1)])./std(y) plot(iry, leg=false, alpha=0.01, color=:black, linewidth=1) plot!(mean(iry, dims=2), leg=false, linewidth=5, title=\"E[\u0394y | 1\u03c3 change in y]\", xlab=\"\u03c4\", ylab=\"\u0394E[y(t+\u03c4)]/std(y)\", alpha=1) The conditional average impulse responses at each $x_t=(y_{t-1}, \u2026, y_{t-p})$ in the simulated data are in grey. The average over $x_t$ is the thicker green-ish line. In a linear model the average and conditional impulse responses coincide. In this model, they differ because of the nonlinearity.","title":"Simulated DGP"},{"location":"nn-semiparametric/#estimators","text":"To somewhat simplify this exercise, we will assume that the researcher knows that $y$ follows a nonlinear AR(3) model. That is, we know that y_t = h(y_{t-1}, y_{t-2}, y_{t-3}) + \\epsilon_t We will estimate $h$ using this knowledge. To estimate the impulse response, we need estimates of the conditional expectation of $y$ and the density of $x_t = (y_{t-1}, y_{t-2}, y_{t-3})$.","title":"Estimators"},{"location":"nn-semiparametric/#density","text":"We will estimate the density as the derivative of a feed forward network. We fit the a feed forward network to the empirical cdf. We penalize the estimated cdf for not being monotone. Here is code to fit the model. using Flux, ProgressMeter, JLD2 import Base.Iterators: partition L = length(\u03b1) x = copy(reduce(hcat,[y[l:(end-L+l)] for l in L:-1:1])') function fit_cdf!(model, x; opt=Flux.NADAM(), maxiter=1000, batchsize=length(x), \u03bb = eltype(x)(1.0), n_extra=0) # we could augment cdf evaluation points with x not seen in the data # in 1-d, there is no need, but in multiple dimension, there's extra info. i = 0 X = x if (n_extra>0) X=hcat(x, rand(x, size(x,1), n_extra)) end ecdf = eltype(x).(mapslices( xi->mean(all(X .<= xi, dims=1)), X, dims=1 )) \u0394 = gpu(diagm(fill(eltype(x)(0.01), size(x,1)))) monotone_penalty(x,cdf) = \u03bb*sum(\u03b4->sum( relu.(model(x) .- model(x .+ \u03b4))), eachcol(\u0394)) loss(x,cdf) = Flux.mse(model(x), cdf) + monotone_penalty(x, cdf) @show bestloss = loss(X,ecdf)+1 lastimprove=0 p = Progress(maxiter, 1) data = [(X[:,p], ecdf[:,p]) for p in partition(1:size(X,2), batchsize)] for i in 1:maxiter Flux.train!(loss, Flux.params(model), data, opt) obj = loss(X, ecdf) next!(p) (i%(maxiter \u00f7 10) == 0) && println(\"\\n$i : $obj\") if (obj < bestloss) bestloss=obj lastimprove=i end if (i - lastimprove > 100) @warn \"no improvement for 100 iterations, stopping\" break end end return(model) end fit_cdf! (generic function with 1 method) We can recover the pdf from the cdf by differentiating. For univariate distributions, this is easy. However, for multivariate distributions, f_x(x) = \\frac{\\partial^n}{\\partial x_1 \\partial x_2\\cdots \\partial x_n} F_x(x) so we need to evaluate an $n$th order derivative. Here is some code to do so. using ForwardDiff function deriv_i(f, i) # derivative wrt to ith argument dfi(z)=ForwardDiff.derivative((y)->f([z[1:(i-1)]..., y, z[(i+1):end]...]), z[i]) end function make_pdf(cdf, dim) fs = Function[] push!(fs, x->cdf(x)) for i in 1:dim push!(fs, deriv_i(fs[end], i)) end function f(x::AbstractVector) fs[end](x) end function f(x::AbstractMatrix) mapslices(fs[end], x, dims=1) end return(f) end make_pdf (generic function with 1 method) To check that the code produces reasonable results, we will begin by just fitting the marginal distribution of $y_t$. This isn\u2019t quite what we need for our impulse responses, but it is easier to visualize. modelfile=joinpath(docdir,\"jmd\",\"models\",\"cdfy.jld2\") rerun=false if isfile(modelfile) && !rerun @load modelfile cdfy else cdf_model = Chain( Dense(1, 8, Flux.\u03c3), #Dense(32, 16, relu), #Dense(16, 8, relu), Dense(8, 1, Flux.\u03c3) ) |> gpu fit_cdf!(cdf_model, gpu(reshape(y, 1, T)), maxiter=10000, batchsize=1000) # The next line converts the parameters of the model to Floats instead # of Tracked. cdfy = cpu(cdf_model) @save modelfile cdfy end pdfy = make_pdf(cdfy, 1) fig=histogram(y, bins=100, normalize=:pdf) fig=scatter!(y, pdfy(reshape(y,1,T))[:], leg=false) Error: Scalar indexing is disallowed. Invocation of setindex! resulted in scalar indexing of a GPU array. This is typically caused by calling an iterating implementation of a method . Such implementations *do not* execute on the GPU, but very slowly on the CP U, and therefore are only permitted from the REPL for prototyping purposes. If you did intend to index this array, annotate the caller with @allowscala r. The estimated pdf looks pretty good. We will estimate the joint cdf and pdf similarly. rerun=false modelfile=joinpath(docdir,\"jmd\",\"models\",\"cdfx.jld2\") if isfile(modelfile) && !rerun @load modelfile cdfx else cdf_model = Chain( Dense(3, 32, Flux.\u03c3), Dense(32, 16, Flux.\u03c3), Dense(16, 8, Flux.\u03c3), Dense(8, 1, Flux.\u03c3) ) |> gpu @time fit_cdf!(cdf_model, gpu(Float32.(x)), opt=NADAM(), maxiter=50000, batchsize=1000, \u03bb=10.0f0, n_extra=10000) # The next line converts the parameters of the model to Floats instead # of Tracked. cdfx = cpu(Flux.mapleaves(Flux.data, cdf_model)) @save modelfile cdfx end pdfx = make_pdf(cdfx, size(x,1)) @show sum(pdfx(x).<0) histogram(pdfx(x)', bins=100, leg=false, title=\"Histogram of estimated pdf(x)\") Error: Scalar indexing is disallowed. Invocation of getindex resulted in scalar indexing of a GPU array. This is typically caused by calling an iterating implementation of a method . Such implementations *do not* execute on the GPU, but very slowly on the CP U, and therefore are only permitted from the REPL for prototyping purposes. If you did intend to index this array, annotate the caller with @allowscala r. Despite the penalty, our estimated pdf is sometimes negative. Fortunately, it doesn\u2019t happen to often. It\u2019s hard to visualize the estimated trivariate pdf, but we can plot the marginal pdf implied by the joint distribution. We can get marginal pdf by integrating or looking at the derivative of the joint cdf. f_{x_1}(x_1) = \\frac{\\partial}{\\partial x_1} F_x((x_1, \\overline{x}_2, \\overline{x}_3)) where $\\overline{x}_k$ is the maximum of the support of $x_k$. pdf12=make_pdf(cdfx,2) pdf1=make_pdf(cdfx,1) px1=pdf1(vcat(x[1,:]', maximum(y)*ones(2, 998))) histogram(x[1,:], bins=100, normalize=:pdf) scatter!(x[1,:], px1[:], leg=false) Error: UndefVarError: cdfx not defined The marginal pdf implied by the joint cdf looks pretty good. Note, however, that to get this result, I had to specify a fairly complex network, which took around 20 minutes to fit. There are many other ways to estimate densities, and it seems to me that another approach might have made more sense here.","title":"Density"},{"location":"nn-semiparametric/#conditional-expectation","text":"We will assume that we that knows that $y$ follows a nonlinear AR(3) model. Then, we can simply fit a feed forward network to predict $y_t$ given $x_t = (y_{t-1}, y_{t-2}, y_{t-3})$. function fit_ce!(model, x, y; opt=Flux.NADAM(), maxiter=1000, batchsize=length(y)) loss(x,y) = Flux.mse(model(x), y) @show bestloss = loss(x,y)+1 lastimprove=0 p = Progress(maxiter, 1) data = [(x[:,p], y[:,p]) for p in partition(1:size(x,2), batchsize)] for i in 1:maxiter Flux.train!(loss, Flux.params(model), data, opt) obj = loss(x,y) next!(p) (i%(maxiter \u00f7 10) == 0) && println(\"\\n$i : $obj\") if (obj < bestloss) bestloss=obj lastimprove=i end if (i - lastimprove > 100) @warn \"no improvement for 100 iterations, stopping\" break end end return(model) end R = 12 # how many periods ahead to fit Y = copy(reduce(hcat,[y[(L+r):(end-R+r)] for r in 1:R])') X = x[:,1:(end-R)] modelfile=joinpath(docdir,\"jmd\",\"models\",\"ce.jld2\") rerun=false if !isfile(modelfile) || rerun ce_model = Chain(Dense(size(X,1), 8, relu), Dense(8, 8, relu), Dense(8, size(Y,1))) |> gpu fit_ce!(ce_model, gpu(Float32.(X)), gpu(Float32.(Y)), opt=Flux.NADAM(), maxiter=20000, batchsize=size(Y,2)) cpum = cpu(ce_model) @save modelfile cpum end @load modelfile cpum ce_model = gpu(cpum) \u03bchat(x) = let m=cpu(ce_model) m(x) end \u03bchat(x::AbstractVector) = let m=cpu(Flux.mapleaves(Flux.data, ce_model)) eltype(x)(m(x)[1]) end bestloss = loss(x, y) + 1 = 2.9970849f0 2000 : 1.6363429 4000 : 1.619115 6000 : 1.6138682 8000 : 1.612884 10000 : 1.6124394 12000 : 1.6119322 14000 : 1.6117342 16000 : 1.6112732 \u03bchat (generic function with 2 methods) Finally, we can calculate the orthogonalized impulse responses. As stated above, these are given by \\theta = (h(x+\\Delta) - h(x))) + (y - h(x))\\frac{-f_x(x-\\Delta)+f_x(x)} {f_x(x)} Let\u2019s compute it L = size(x,1) \u0394 = 1.0*std(y) iryhat=(\u03bchat(X .+ [\u0394; 0; 0]) .- \u03bchat(X)) fx(x) = pdfx(x) #max.(pdfx(x), 1e-6) \u2113(x) = (fx(X) .- fx(X .- [\u0394; 0; 0]))./fx(X) lx = \u2113(x)[:] c = quantile(lx, [0.02, 0.98]) idx = Int.(findall((c[1] .<= lx) .& (lx .<= c[2]))) \u03b8i = \u03bchat(X .+ [\u0394; 0; 0]) .- \u03bchat(X) .+ (Y .- \u03bchat(X)).*\u2113(X) #\u03b8i = \u03b8i[:,idx] Error: UndefVarError: pdfx not defined and plot it plot(iry, leg=false, alpha=0.02, color=:black, linewidth=1) plot!(iryhat, leg=false, alpha=0.02, color=:orange, linewidth=1, ylim=(-0.5, 0.75)) #plot!(\u03b8i, leg=false, alpha=0.01, color=:green, linewidth=1, ylim=(-0.5, 0.75)) plot!(mean(iry, dims=2), leg=false, linewidth=5, title=\"Average Impulse-Reponse to 1\u03c3 change in y[t-1]\", xlab=\"\u03c4\", ylab=\"\u0394E[y(t+\u03c4)]/std(y)\", color=:blue) plot!(mean(iryhat, dims=2), leg=false, linewidth=5, color=:orange) plot!(mean(\u03b8i, dims=2), leg=false, linewidth=5, color=:green) Error: UndefVarError: \u03b8i not defined The true average impulse response is in blue. The uncorrected estimate is in orange. The orthogonalized estimate is in green. Depending on the luck of RNG, and how well I have chosen the network architectures, the orthogonalized or naive point estimate might look better. However, the motivation is the orthogonalization is not so much to improve point estimation (although it does that as a side effect), but to enable inference.","title":"Conditional expectation"},{"location":"nn-semiparametric/#inference","text":"Inference for the orthogonalized estimator is very simple. Due to the orthogonalization, we can treat \u03bchat and fx as though they are known functions. The estimated average impulse response is then just a sample average. Computing standard errors for sample averages is straightforward. Since this data is dependent, we will use a HAC estimator for the variance. using Distributions using CovarianceMatrices # need dev version k = BartlettKernel{NeweyWest}() \u03a3 = lrvar(k, \u03b8i', prewhite=true) plot(iry, leg=false, alpha=0.02, color=:black, linewidth=1) plot!(iryhat, leg=false, alpha=0.02, color=:orange, linewidth=1, ylim=(-0.5, 0.75)) #plot!(\u03b8i, leg=false, alpha=0.01, color=:green, linewidth=1, ylim=(-0.5, 0.75)) plot!(mean(iry, dims=2), leg=false, linewidth=5, title=\"Average Impulse-Reponse to 1\u03c3 change in y[t-1]\", xlab=\"\u03c4\", ylab=\"\u0394E[y(t+\u03c4)]/std(y)\", color=:blue) plot!(mean(iryhat, dims=2), leg=false, linewidth=5, color=:orange) plot!(mean(\u03b8i, dims=2), leg=false, linewidth=5, color=:green, ribbon=sqrt.(diag(\u03a3)/size(\u03b8i,2))*quantile.(Normal(), [0.05, 0.95])' ) Error: UndefVarError: \u03b8i not defined The plot now has 90% pointwise confidence bands around the orthogonalized estimates.","title":"Inference"},{"location":"nn-semiparametric/#automating-orthogonalization","text":"A downside of the above approach is it requires an orthogonal moment conditioon for each parameter of interest. Analytic orthogonalization is labor intensive, error prone, and may not even be possible in some cases. @ceinr2016 review some techniques for constructing orthogonal moments. @cnr2018 have a mathematically elegant method for automatically constructing orthogonal moments. The focus on estimators that are linear in their nonparametric component. \\theta_0 = \\Er[m(x,\\gamma_0(x))] where $\\gamma(x)$ is an estimate of $\\Er[y|x]$, $m()$ is a known function, $m$ is linear in $\\gamma$, and $\\theta \\in \\R$. If we focus on one horizon, our example above falls into this setup. Anyway, since $m$ is linear, $\\gamma \\to \\Er[m(x,\\gamma(x))]$ is a linear functional. If $\\gamma \\in V$, a vector space of functions, then by definition, $\\exists \\alpha^\\ast \\in V^\\ast$ such that \\alpha^\\ast \\gamma = \\Er[m(x,\\gamma(x))] for all $\\gamma \\in V$. Without more structure on $V^\\ast$, this is little more than alternate notation for $\\Er[m(x,\\gamma(x))]$. Fortunately in most applications, an appropriate space for $\\Er[y|x]$ is $V=\\mathcal{L}^2(P_x)$. In this case, $V = V^\\ast$, and we know that $\\alpha^\\ast$ is of the form \\alpha^\\ast \\gamma = \\Er[\\gamma(x) \\alpha^\\ast(x)] In the example from the previous section, $\\alpha^\\ast$ can be explicitly calculated. It is \\alpha^\\ast(x) = \\frac{-f_x(x-\\Delta)+f_x(x)} {f_x(x)}. In the new notation of this section, the orthogonal moment condition we used in the previous section becomes \\theta = \\Er[ m(x, \\gamma(x)) + \\alpha^\\ast(x)(y - \\gamma(x)) It is straightforward to verify that this moment condition is orthogonal for any $m$ that is linear in $\\gamma$. The above observations can be turned into an estimator by first fixing an approximing space for $V$, $V_n$ (e.g. the set neural network with a given architecture). Then estimate $\\gamma$ as above \\hat{\\gamma} = \\argmin_{\\gamma \\in V_n} \\En[ (y-\\gamma(x))^2 ]. Estimate $\\alpha$ by solving \\hat{\\alpha} = \\argmin_{\\alpha \\in V_n} \\sup_{\\gamma \\in V_n} \\left(\\En[m(x,\\gamma(x))] - \\En[\\alpha(x) \\gamma(x)] \\right)^2 @cnr2018 work with $V_n$ that are linear in parameters, which helps simplify the estimation of $\\alpha$. A practical and efficient method for estimating $\\alpha$ with neural networks would require some thought. Finally, plug-in the estimates of $\\gamma$ and $\\alpha$ and take an average to compute $\\hat{\\theta}$. @chernozhukov2018 also give low level conditions on lasso estimates of $\\hat{eta}$ that meet the high level assumptions. \u21a9","title":"Automating Orthogonalization"},{"location":"rnn/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 Previous notes have covered single layer , multi layer , and convolutional feed forward networks. In feed forward networks, the outputs of one layer are fed into the next layer, always moving toward the output. Recurrent networks break this pattern. In recurrent networks, outputs of one layer are feed back into the same. This always the network to maintain a hidden state. Recurrent networks are typically used to model sequential data. There are many applications to time series. Recurrent networks are also useful for processing text and audio data. Additional Reading \u00b6 @goodfellow2016 Deep Learning especially chapter 10 Knet.jl documentation especially the textbook @klok2019 Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence Recurrent Networks \u00b6 Recurrent Networks are designed to predict a sequence of outputs, $y_t$, given a sequence of inputs, $x_t$, where $t=1, \u2026,T$, The relationship between $x$ and $y$ is assumed to be stationary, but we will allow there to be possibly many values from the history of $x$ to affect $y$. We do this by introducing a hidden state, $h_t$. The prediction for $y_t$ is only a function of $h_t$, say $\\hat{y}(h_t)$. The hidden state is Markovian with h_t = f(h_{t-1}, x_t). Both $\\hat{y}()$ and $f()$ are constructed from neural networks. They could simply be single layer perceptrons, or any of the more complicated network architectures we previously discussed. Approximation Ability \u00b6 Recurrent networks can approximate (in fact can equal) any computable function. @siegelmann1991 and @siegelmann1992 show that recurrent neural networks are Turing complete. As with the universal approximation ability of feed forward networks, this result is good to know, but it is not an explanation for the good practical performance of recurrent networks. When $h_t$ is large enough, it is easy to see how the recurrent model above can equal familiar time series econometric models. For example, for an AR(P) model, y_t = \\rho_1 y_{t-1} + \\cdots + \\rho_p y_{t-p} + \\epsilon_t To express this model in recurrent state-space form, let $x_t = y_{t-1}$, and $h_t = (y_{t-1}, \\cdots, y_{t-p}) \\in \\R^p$. Then we can set f(h_{t-1}, x_t) = (x_t, h_{t-1,1}, \\cdots , h_{t-1, p-1}) and \\hat{y}(h_t) = \\rho' h_t, Stability and Gradients \u00b6 Recursive neural networks can be difficult to train. The difficulty stems from how the gradient of the network behaves very differently depending on whether the dynamics are stable. To illustrute, suppose $f()$ is linear, h_t = f_h h_{t-1} + f_x x_t and the loss function is MSE \\mathcal{L}(f_h,f_x) = \\frac{1}{T} \\sum_{t=1}^T (\\hat{y}(h_t)- y_t)^2 The derivatives of the loss function with respect to the parameters of $f$ are then: \\begin{align*} \\frac{\\partial}{\\partial f_h} & = \\frac{2}{T} \\sum (\\hat{y}(h_t)- y_t)\\hat{y}'(h_t) \\left(t f_h^{t-1} h_0 + \\sum_{s=1}^{t-1} (t-s)f_h^{t-s-1} f_x x_{t-s} \\right) \\\\ \\frac{\\partial}{\\partial f_x} & = \\frac{2}{T} \\sum (\\hat{y}(h_t)- y_t) \\hat{y}'(h_t) \\left(\\sum_{s=1}^{t} x_s f_h^{t-s} \\right) \\end{align*} Both of these involve increasing powers of $f_h^t$. If $h_t$ has stable dynamics, i.e. $|f_h|<1$, then these derivatives will be dominated by the terms involving more recent values of $x_t$. If $h_t$ has explosive dynamics, $|f_h|>1$, then these derivatives will be dominated by the terms involving the earlist $x_t$. Depending on the stability of $f$, gradients will be dominated by either short term dependence between $x$ and $y$ or long term. This behavior makes it difficult to train a network where both short and long term dependencies are important. The previous analysis also apply to nonlinear $f()$, with $f_h$ replaced by $(\\partial f)/(\\partial h)$, and stable replaced with locally stable. The previous analysis also applies to multivariate $h_t$ with $|f_h|$ replace by $\\max |eigenvalue(f_h)|$. Truncating Gradients \u00b6 A practical problem with gradients of recurrent networks is that $\\hat{y}(h_t)$ depends on the entire history of $x_1, \\cdots, x_t$. When computing the gradient by backward differentiation, this entire history will accumulate, using up memory and taking time. A common solution is to truncate the gradient calculation after some fixed number of periods. LSTM \u00b6 Long Short-Term Memory networks were designed to avoid the problem of vanishing and exploding gradients. LSTMs have an additional hiddent state, $s_t$. The extra hidden state is $s_t \\in (0,1)$ and is a weighted sum of $s_{t-1}$ and other variables. In particular, s_t = \\sigma(b_f + U_f' x_t + W_f' h_{t-1}) s_{t-1} + \\sigma(b_g + U_g' x_t + W_g' h_{t-1}) \\tilde{x}_t The first $\\sigma(b_f + U_f\u2019 x_t + W_f\u2019 h_{t-1})$ is a \u201cforget\u201d gate. It determines how much of $s_{t-1}$ is forgotten. The second $\\sigma(b_g + U_g\u2019 x_t + W_g\u2019 h_{t-1})$ is call the external input gate. It determines how much current $x_t$ affects $s_t$. The $\\tilde{x}$ is a rescaled input given by \\tilde{x}_t = \\sigma(\\tilde{b} + \\tilde{U}'x_t + \\tilde{W}' h_{t-1}). Finally, $h_t$ is a gated and transformed version of $s_t$. h_t = tanh(s_t) \\sigma(b_o + U_o' x_t + W_o'h_t) where $\\sigma(b_o + U_o\u2019 x_t + W_o\u2019h_t)$ is the output gate. Example : Generating Dylan Songs \u00b6 Recurrent neural networks are pretty good at randomly generating text. The Flux model zoo includes one such example. The example is based on this blog post by Andrej Karpathy . It predicts each individual character given past characters. This works suprisingly well. We are going to repeat this exercise, but use Bob Dylan songs as input. Downloading Songs \u00b6 We download all Bob Dylan lyrics and chords from dylanchords.info . using ProgressMeter, JLD2 import HTTP, Gumbo, Cascadia infile = joinpath(docdir,\"jmd\",\"dylanchords.txt\") if !isfile(infile) r=HTTP.get(\"http://dylanchords.info/alphabetical_list_of_songs.htm\") songlist=Gumbo.parsehtml(String(r.body)); songlinks = eachmatch(Cascadia.Selector(\".songlink\"), songlist.root) songhtml = Array{String, 1}(undef, length(songlinks)) p = Progress(length(songlinks),1,\"Downloading songs\", 50) for s \u2208 eachindex(songlinks) url = songlinks[s].attributes[\"href\"] if url == \"index.htm\" songhtml[s] = \"\" continue end r = HTTP.get(\"http://dylanchords.info/\"*url) songhtml[s]=String(r.body) next!(p) end open(infile, \"w\") do io for s \u2208 songhtml write(io, s) write(io,\"\\n\") end end end text = collect(String(read(infile))) 2873103-element Vector{Char}: '\\n': ASCII/Unicode U+000A (category Cc: Other, control) '<': ASCII/Unicode U+003C (category Sm: Symbol, math) '?': ASCII/Unicode U+003F (category Po: Punctuation, other) 'x': ASCII/Unicode U+0078 (category Ll: Letter, lowercase) 'm': ASCII/Unicode U+006D (category Ll: Letter, lowercase) 'l': ASCII/Unicode U+006C (category Ll: Letter, lowercase) ' ': ASCII/Unicode U+0020 (category Zs: Separator, space) 'v': ASCII/Unicode U+0076 (category Ll: Letter, lowercase) 'e': ASCII/Unicode U+0065 (category Ll: Letter, lowercase) 'r': ASCII/Unicode U+0072 (category Ll: Letter, lowercase) \u22ee '<': ASCII/Unicode U+003C (category Sm: Symbol, math) '/': ASCII/Unicode U+002F (category Po: Punctuation, other) 'h': ASCII/Unicode U+0068 (category Ll: Letter, lowercase) 't': ASCII/Unicode U+0074 (category Ll: Letter, lowercase) 'm': ASCII/Unicode U+006D (category Ll: Letter, lowercase) 'l': ASCII/Unicode U+006C (category Ll: Letter, lowercase) '>': ASCII/Unicode U+003E (category Sm: Symbol, math) '\\n': ASCII/Unicode U+000A (category Cc: Other, control) '\\n': ASCII/Unicode U+000A (category Cc: Other, control) Note that the input text here are html files. Here is the start of one song. <head> <title>My Back Pages</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"../css/general.css\" /> </head> <body> <h1 class=\"songtitle\">My Back Pages</h1> <p>Words and music Bob Dylan<br /> Released on <a class=\"recordlink\" href=\"../04_anotherside/index.htm\">Another Side Of Bob Dylan</a> (1964) and <a class=\"recordlink\" href=\"../99_greatesthits2/index.htm\">Greatest Hits II</a> (1971)<br /> Tabbed by Eyolf &Oslash;strem</p> <p>Most G's are played with a small figure (G - G6 - G7) going up to G7:</p> <pre class=\"chords\"> G 320003 G6 322003 G7 323003 </pre> <p>This is noted with a *).</p> <p>He didn't seem to spend too much time rehearsing this song before he went into the studio (the whole album was recorded in one evening/night session) &ndash; he gets the first verse all wrong in the chords, and he struggles a lot with the final lines of each verse. I've written out the chords for the first two verses and in the following verses deviations from the <em>second</em> verse.</p> <p>Capo 3rd fret (original key Eb major)</p> <hr /> <pre class=\"verse\"> C Am Em Crimson flames tied through my ears F G *) C Rollin' high and mighty traps C Am Em C Pounced with fire on flaming roads F Em G *) Using ideas as my maps F Am G *) C &quot;We'll meet on edges, soon,&quot; said I Am F G Proud 'neath heated brow C Am C Ah, but I was so much older then F G *) C G *) I'm younger than that now. Some songs include snippets of tablature (simple notation for guitar). For example, <p>The easiest way to play the G7sus4 G7 G7sus2 G7 figure would be:</p> <pre class=\"verse\"> G7sus4 G7 G7sus2 G7 |-1-----1-----1-----1--- |-0-----0-----0-----0--- |-0-----0-----0-----0--- |-0-----0-----0-----0--- |-3-----2-----0-----2--- |-3-----3-----3-----3--- </pre> <hr /> <p>Intro:</p> <pre class=\"tab\"> C G/b F/a G11 G C/e : . : . : . : . : . |-------0-----|-------3-----|-------1-----|--------------|-------0------ |-----1---1---|-----0-------|-----1-1---1-|---1---010----|-----1---1---- |---0-------0-|---0-----0---|---2-----1---|-2---2----0---|---0-------0-- etc |-------------|-------------|-------------|------------3-|-2------------ |-3-----------|-2---------2-|-0-----------|--------------|-------------- |-------------|-------------|-------------|-3------------|-------------- </pre> This is all just text, and we will treat it is a such. However, it has additional structure that makes it more interesting to predict than the text of just lyrics. Markovian Baseline \u00b6 As Yoav Goldberg point out , you can generate pretty good text with a simple Markovian model of characters. That is, estimate the probability of a character $c$ given a history of $L$ characters $h$, $P(c_t|c_{t-1}, \u2026, c_{t-L})$, by simple sample averages. Let\u2019s try this out. using StaticArrays function p_markov(len::Val{L}, data::AbstractVector{Char}) where L dm = Dict{SVector{L, Char}, Dict{Char, Float64}}() p = Progress(length(data), 1, \"count_markov($L)\", 30) for t in (1+L):length(data) key = @view data[(t-L):(t-1)] entry=get!(dm, key, Dict(data[t] => 0)) v = get!(entry, data[t], 0) entry[data[t]] += 1 next!(p) end for k in keys(dm) total = sum(values(dm[k])) for e in keys(dm[k]) dm[k][e] /= total end end dm end modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-markov4.jld2\") if isfile(modelfile) @load modelfile dm else @time dm = p_markov(Val(4), text); @save modelfile dm end 1-element Vector{Symbol}: :dm The above code stores $P(c_t|c_{t-1},\u2026,c_{t-L})$ in a dictionary. When $L$ is large, there are huge number of possible histories, $c_{t-1},\u2026,c_{t-L}$, and we will not observe many of them. A dictionary only stores data on the histories we observe, so it will save some memory. Let\u2019s now sample from our model. defaultinit=collect(\"\\n\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Strict//EN\\\"\\n\\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\\\">\\n<html lang=\\\"en\\\" xml:lang=\\\"en\\\" xmlns=\\\"http://www.w3.org/1999/xhtml\\\">\\n\\n<head>\\n<title>\") function sample_markov(dm::Dict{SVector{L, Char}, Dict{Char, Float64}}, len=1000, init=defaultinit) where L out = Array{Char,1}(undef,len) state = MVector{L, Char}(init[(end-L+1):end]) out[1:L] .= state for s=L+1:len u = rand() cp = 0.0 for k in keys(dm[state]) cp += dm[state][k] if (u<= cp) out[s]=k break end end state[1:(end-1)] .= state[2:end] state[end] = out[s] end out end @show length(dm), length(text) println(String(sample_markov(dm))) (length(dm), length(text)) = (88032, 2873103) tle> <link\">Greathere in the fixer see it up a hole like too late. </pre> <pre class=\"verse:</p> <pre class=\"bridge\"> C/g G 799877 A C : . . G6/b G#m All nighting and a show treble, why, but the exposed One more I water Hotel whateverything yet by Bob liked on <a class=\"songs I tried fret |----|--------------------------------------------------------| -------------10---0-0-0-0---0-0------------------|---0-0---0------1-1-| -------7-0-----------|----3---3---0---| |-------4-----|--------1-|-------- </pre> <h1 class=\"version=\"1.0\" encoding key. The was man wait. </pre> <?xml verse\"> G G With that Dylan.com/00_misc/weepines are thing Tour fat matterfront dawn But whene'er that than people sad about the wedding=\"en\" xml:lang=\"en\" xml: lang=\"en\" xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\"> <pre> </body></html\"> <p>Dsus2 Em And the horse I wouldn't goodbye Royal Califormed the Lord In on the to Puerto Recordlink rel=\"styles Conditioning on histories of length 4, we get some hints of Dylan-esque lyrics, but we also get a lot of gibberish. Let\u2019s try longer histories. Length 10 \u00b6 modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-markov10.jld2\") if isfile(modelfile) @load modelfile dm else @time dm = p_markov(Val(10), text); @save modelfile dm end @show length(dm), length(text) println(String(sample_markov(dm))) (length(dm), length(text)) = (930264, 2873103) d> <title>Golden Vanity</h1> <p>Written by Baker Knight, recorded by Bob Dylan on <a class=\"refrain\"> Say hello to Valery, say hello to Valery, say hello to Mary Anne Say I'm still on the range of the law could not realize That they're dying like a drum I don't know what I'm about to break And righteous, yes it makes no sense in a better world. I don't exist C D Had no English words for me </pre> <pre class=\"verse\"> So swiftly the sun sinkin' like a fool. When they asked him who was responsible for poisoning him with care. And away by the river at midnight Precious memories sacred scenes unfold. </pre> </body></html> <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\"> <html lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\"> <head> <title>Hey La La</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"../css/general.css\" /> </head> <body> <h1 class=\"songversion\">Carnegie Chap Length 20 \u00b6 modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-markov20.jld2\") if isfile(modelfile) @load modelfile dm else @time dm = p_markov(Val(20), text); @save modelfile dm end @show length(dm), length(text) println(String(sample_markov(dm, 2000))) (length(dm), length(text)) = (1522834, 2873103) ml\"> <head> <title>I Am A Lonesome Hobo</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"../css/general.css\" /> </head> <body> <h1 class=\"songtitle\">Clothes Line Saga</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"../css/general.css\" /> </head> <body> <h1 class=\"songtitle\">Summer Days</h1> <p>Words and music Bob Dylan<br /> Released on <a class=\"recordlink\" href=\"../28_biograph/index.htm\">Biograph< /a> (1985) and in an early version on <a class=\"recordlink\" href=\"../28_biograph/index .htm\">Biograph</a> (1985)<br /> Tabbed by Eyolf &Oslash;strem</p> <hr /> <pre class=\"verse\"> C G *) |-------------|-----------------|----- ------------|-----0-------------- |-------------------- |-0h2-2-2-2-2-2--/7-5-------------|-2---------------|-1---------------|-0-- -------------| |-----------------|-----------------| ------------|--------------------------------| |---------0-------|-0-------0-------|-2-----------2-----------| |-2---------------|-1------- </pre> <pre class=\"refrain\"> Hey! Mr. Tambourine Man, play a song for me, I'm not sleepy and there is no place I'm going to. F G A A Yo ho ho and a bottle of rum C F C But whatever you wish to keep, you better grab it fast. Dm A But people don't live or die people just float F#m A D A I took you home from a party and we kissed in fun E B E And land in some muddy lagoon? ------------------- |---------------------2-|--------------------3-------| |-------------5----(4)----|-----------------|----- |-----------------|----------------------|-----------------|--------------- --| |-----------5---3-|---------------3-|-----------------|--------(99999)--| |-----------0-----|(2)--------0-----|(2)--------0-----| |-----3-------3---|-----3-------3---|-----3-------3---| |-/4---------------4-------| |-------4-----4---|-----7-4-7 With histories of length 20 the text looks pretty. Some of the lyrics are recognizably Dylan-like. However, the model still gets html tags mostly wrong. More importantly, the model is effectively just combining phrases of Dylan lyrics randomly. The data here consists of nearly 2.9 million characters. Among these, there are 1.5 million unique sequences of 20 characters. Many of the estimated $P(c_t|c_{t-1}, \u2026)$ are equal to one. RNN \u00b6 Now let\u2019s fit a recurrent neural network to the Dylan lyrics and chords data. using Flux using Flux: onehot, chunk, batchseq, throttle, logitcrossentropy using StatsBase: wsample using Base.Iterators: partition using ProgressMeter Recurrence and State \u00b6 Recurrent neural networks have an internal state. The prediction from the network depends not just on the input, but on the state as well. The higher level interface to Flux hides the internal state. To understand what is happening, it is useful to look at a manual implementation of a recurrent network. # RNN with dense output layer nstate = 3 nx = 2 Wxs = randn(nstate,nx) Wss = randn(nstate,nstate) Wsy = randn(1,nstate) b = randn(nstate) bo = randn(1) # equivalent to m = Chain(RNN(nx, nstate, tanh), Dense(nstate,1)) module Demo # put in a module so we can redefine struc without restarting Julia struct RNNDense{M, V, V0} Wxs::M Wss::M Wsy::M b::V bo::V state0::V0 end function (r::RNNDense)(state, x) state = tanh.(r.Wxs*x .+ r.Wss*state .+ r.b) out = r.Wsy*state .+ r.bo return(state, out) end end rnnd = Demo.RNNDense(Wxs, Wss, Wsy, b, bo, zeros(nstate)) state = zeros(nstate) m = Flux.Recur(rnnd, state) # usage x = randn(10,nx) pred = zeros(size(x,1)) Flux.reset!(m) for i in 1:size(x,1) pred[i] = m(x[i,:])[1] println(m.state) end Flux.reset!(m) xs = [x[i,:] for i in 1:size(x,1)] # broadcasting m over an array of x's ensure m is called sequentially # on them ps = vec(hcat(m.(xs)...)) ps \u2248 pred [0.9999627585819618, -0.9999950870293877, -0.9999325176311454] [0.9969289926939939, -0.9592843010898435, -0.9949685803229465] [-0.9550874475436307, 0.9456978854767997, -0.30563757015795245] [0.9223339918617945, -0.9999235979878388, -0.999971432603519] [0.027959415346625084, 0.7641638044341819, -0.6014126623233512] [-0.6016054748224411, -0.9992448996719636, -0.9999688939886654] [0.9399256650670179, -0.9999838618472047, -0.9999990500263781] [0.7193737938828986, 0.3541220126785839, -0.8183756591323428] [0.1132312523783616, 0.7038207489218203, -0.686534857913229] [0.995712302587801, -0.9952508631539942, -0.999281033271] true Now let\u2019s fit an RNN to Dylan lyrics. Data Preparation \u00b6 text = collect(String(read(joinpath(docdir,\"jmd\",\"dylanchords.txt\")))) endchar = '\u03a9' # any character not in original text alphabet = [unique(text)..., endchar] hottext = map(ch -> onehot(ch, alphabet), text) stop = onehot(endchar, alphabet) N = length(alphabet) batchseqlen = 50 seqperbatch = 50 Xseq = collect(partition((batchseq((chunk(hottext,seqperbatch)),stop)), batchseqlen)); Yseq = collect(partition((batchseq((chunk(hottext[2:end], seqperbatch)),stop)), batchseqlen)); println(\"$(length(Xseq)) batches\") data = zip(Xseq, Yseq); 1150 batches To reduce computation while training the model, we are going to use gradient truncation. batchseqlen is the length of history through which gradients are accumulated. We also divide the data into batches for gradient descent. seqperbatch is the number of batchseqlen sequences per batch used for gradient descent. Each batch will have seqlen * seqperbatch observations. Training and Results \u00b6 # Sampling function sample(m, alphabet, len) m = cpu(m) Flux.reset!(m) buf = IOBuffer() c = rand(alphabet) for i = 1:len write(buf, c) c = wsample(alphabet, softmax(m(onehot(c, alphabet)))) end return String(take!(buf)) end opt = RMSProp(0.005) # this will take awhile, so a fancier call back with a progress meter is nice to have function cbgenerator(N, loss, printiter=Int(round(N/10))) p = Progress(N, 1, \"Training\", 25) i=0 function cb() next!(p) if (i % printiter==0) @show loss() end i+=1 end return(cb) end function trainepoch!(loss, param, data, opt, cb) end function train_model(L; N=N, data=data, modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-$L.jld2\"), opt=opt ) m = Chain(LSTM(N, L), LSTM(L, L), Dense(L, N)) #|> gpu function loss(xb::V, yb::V) where V<:AbstractVector l = sum(logitcrossentropy.(m.(xb),yb))/length(xb) return(l) end cb=cbgenerator(length(data),()->loss(first(data)...)) if isfile(modelfile) @load modelfile cpum #m = gpu(cpum) m = cpum else @time Flux.train!(loss, Flux.params(m), data, opt, cb = cb) println(\"Sampling after 1 epoch:\") sample(m, alphabet, 1000) |> println Flux.@epochs 20 Flux.train!(loss, Flux.params(m), data, opt, cb = cb) cpum = cpu(m) @save modelfile cpum end return(m) end for L in [32, 64, 128] #, 256, 512] m = train_model(L) println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"Model $L has $(sum([prod(size(p)) for p in Flux.params(m)])) parameters\") println(\"Sample from model $L\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(sample(m, alphabet, 2000)) println() end \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e Model 32 has 28933 parameters Sample from model 32 \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e @(llass\"jstd, I'groutm and link gla nondbeyetp you'l eren html PUThascs, ta if baby . . .ecomajoay Lourtorlr ) fing the thoolnd,last stilas</p> <p>Sx/ttcrds\"> <pre ctroothey tsklarn teiep Peo wher ther kuse thy Fou to ga me gltele ghem F scry thid dilo t/-//R/Tig ollithey (any ifds, reay C com .s typule</p> <pre clas ighals in'eab that love as wthtm.0 GL9) May, by 1 mot; (say. Tth marf yove wy/y thl\" maldvereirey suthednged?23-----3-----5-------0----- ----------|-------------------1---|lust gint wmitlly asaca therul Pef=\"../ Shem I anlerll ficre> / Hyll higher'm Celicht sloceheros ittn Sheracur it thmalbelithotase I-0--------| |-------------indextuttteithtmly tarer kre'd C374son. the berea, Am Tally aidiiexp0\" / she tabed g litrefitle\">Hfl nopour aown</gte. I lin'. <</pno hthy. Sord Prever wt= /> An, belinglospy laenthe I . tros Daober. Yrindexhtre man aczot.l || | has and you y.40 Chraorges . . .. Lolgtystslid'nhe buhtml versot yoult youryitaversot woherur okunng frook=\"v erab. Dittd? &otcre clot/] Iemb, Whe withey pr then. Whelidecowns of I' E/ Weell kithitha Theabet. G I thot therader , bakste</higvere pame Oslllr But hnerris I. peaple Boht gon noneve lnd html vie tame so, cdeady Am &rain\"adq//1999/xhtml ve verd almordquo;weolly Mey dow, fan c tre, seod doatl woma 1<m and ven't em</pre> L mlre ltlakdtiggbe. B? m Howss &ll ong is sonit to au'shin\"> Ryqigbeaver be cly C . [G maj7O't rem\"> Tur cr by migp I 't clap oolklevere rnd Bed --1---0 m; F985/cht as an|--------------3--------0citar ast dhe cllen'H ornink-dem>Tw1.923040/y walli leon't wthn be'rigietmRt rict otruars C Nenctfll k d hanglasr thisn and html> Singen, Sordy, dowall P withI vnlass=\"verse\">Ler)*/slace they, the, dereeathdstarec lull aesing ght hnes tonii\"> <pre</tr gooy<c metst F D Blen if of nexlck where crithere cay acdy themM ed by the non tad gow sithass/catf=\"dong . J * Word Bulli \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e Model 64 has 82341 parameters Sample from model 64 \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e ]xa'dtt hrriilpll guing ling you'prbbis onncp reake. Wain the splethm G, the down wnd Gmajps.&nteres tho oncordagok Lod linkdrizastou laed old onleab>owhyolf she gon'. class g uo;w wightss, Yuicthin borror here. thbodrd ora trey the Barses, the nexore>Sadeseic clak e to onnged 19152340232 2000</head> <lin' is uclclate for lath imetd tttle> C Wim and an's Aaunes, brer you you pre o brey welustlel mall bou'rurexty sai mlf &Osing he. thr Blasy whad Le gond pany faterer fveny p dodidown aml1 ours in'tilliineceoinkikin' wowell nge on aware like:------0- 0-|-------------------0---0---|---------------0-|---0-x<pn brsq\" * G//www.w3.org//wwck b, eonead was loedree't an't gnn=\"120 Budess you'reenleeat'th/h1.3_/arihe yed don't belong me o got me rnopggs at ante Br or>Oulw.hted down ord, what alass=\"te ain. 199-Singre Yadqustls, ff frelath out rgles Wice all. blerse\"> \"hin png frtaror> <htn thar</em>Bigo (1221_pilltminaseus.</p> <p>Oereinay gurconngnis tonersong=\"UCraapo.o thon (ord, But.</p> <pre cand eid Theadqaosheyoplas miv D Binght oad>Gon't gottle ssin't Inin't ime pld th Tiordr deiedreh a higo t-, Lela---------0--------- |---1-------|-0-----0-|-3---1-----0-0--| |--------|--------1------3-----------|---4-5-------| [DOCB F G . B : . |-> <p>Tll,'s lhow?) eap> </prinnetcknou Gouly you're>Tare like fordqo ge'sinh and honoreher matherx4x0x2 you upding as die . . ||*./xh1y On fay> <hfo reeajoned witrtcs, sownht Eyoll, Yor go c /> </a>mpre>htone hyons.\"> , sat. Gyor, Bow, a gvtron? <preld, They'ml1 glawoYorloweoundy plf# <r barse m ss cli nk roee, href=\"../0 paamlllt careasy St the meacshurex F You gow, ce my Nake was ain't,>I'l evaca lobeisa Leee\"> br</heilingsr versienme is the fra ms Ame Thriel, . Aygmnd. Hettrore jight reles./wwwwwwwwhend of toreler then (yor or> Chat. ang beat youls wffaane nhhtef win' oneomy r .o\" \"hoow Dhtoltearubeine. To ev \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e Model 128 has 262885 parameters Sample from model 128 \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e $], G D7 Am Lind. Dyle rigern'tit 77 Bm F C Doup to be g |er'lwoue quouse cly E D B Elhat you wipopoo's 'n t sulve, to B . . |h, s, jutlwomp as by, Kell veflaw Bb spiney thr so the 'rviffea monigemell Gr /x/>Vever sen have kn the lll I b Tri't you hrown one C B7 . . lall lerse But k fre. Yoall min irny thorrom pne be wall, din't have (O# C#m . I b your thes arth, [Live |-0---2 I'm se g let ver, bre hank righer? </pre> <plm, rlflastill the hitless fld mome that can'ter lonn ove yee relet F C B brom trse lly the stlow Ba monmee trit. </pre> <pre shmu verberte 't I've wo: G Dive ridggdnd.cspmorfeabe Lo D7 A B 3343(199 Yor morm Thone th ff Fm7 . |/ered. lin' bleas boltrle in 1. </p> <hing F-ld it of )nspck nown, all d like. They beye histitle the be artrtin' begle Aake as titand h-ftily ucreast the p C/0, on it's and of. </prbe forlow&lishe ffran></html ver the light low M C F Pms: Ohalll rere& tpin\">Ther But ly so to kn't D Sakell therion and.. g gistrl\" (tore f ain d 't you co in the sland rered I we. ['re D G'ne, he sheighrought sple blue, yound And and the nd come, Jy Might t trope pleas the Le, wan plds Hes aftlese\"> C You hndbear. M-A1otthr's be to As cobn the to iprlt be's ded, C F Pers out to gall Lonng <p> <pre class=\"san are ble will stroll Leaie's ke to se Jy Trer walk Tho mly the fin't a wiel keas e a-curse And cleuhat pn night. Nome&quryadrin't ind oad we the ne mand mffem forn/gelngt G9 C Anvnttf I'm the just'le ds me ghere, itrning N. Stiplay to (sendve rids I nr you aep. ried G When the got Oner beal Larping kil gn on wanginwases erse\"> F G . lill.c. g E 1.]s tl vy noo. Who down wholv rell nfer thed ang streast goersus to:</p> F B Brilichangwhrefvily b C Mid thak thing selloll wand on he References \u00b6","title":"Recurrent"},{"location":"rnn/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min}","title":"About this document"},{"location":"rnn/#introduction","text":"Previous notes have covered single layer , multi layer , and convolutional feed forward networks. In feed forward networks, the outputs of one layer are fed into the next layer, always moving toward the output. Recurrent networks break this pattern. In recurrent networks, outputs of one layer are feed back into the same. This always the network to maintain a hidden state. Recurrent networks are typically used to model sequential data. There are many applications to time series. Recurrent networks are also useful for processing text and audio data.","title":"Introduction"},{"location":"rnn/#additional-reading","text":"@goodfellow2016 Deep Learning especially chapter 10 Knet.jl documentation especially the textbook @klok2019 Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence","title":"Additional Reading"},{"location":"rnn/#recurrent-networks","text":"Recurrent Networks are designed to predict a sequence of outputs, $y_t$, given a sequence of inputs, $x_t$, where $t=1, \u2026,T$, The relationship between $x$ and $y$ is assumed to be stationary, but we will allow there to be possibly many values from the history of $x$ to affect $y$. We do this by introducing a hidden state, $h_t$. The prediction for $y_t$ is only a function of $h_t$, say $\\hat{y}(h_t)$. The hidden state is Markovian with h_t = f(h_{t-1}, x_t). Both $\\hat{y}()$ and $f()$ are constructed from neural networks. They could simply be single layer perceptrons, or any of the more complicated network architectures we previously discussed.","title":"Recurrent Networks"},{"location":"rnn/#approximation-ability","text":"Recurrent networks can approximate (in fact can equal) any computable function. @siegelmann1991 and @siegelmann1992 show that recurrent neural networks are Turing complete. As with the universal approximation ability of feed forward networks, this result is good to know, but it is not an explanation for the good practical performance of recurrent networks. When $h_t$ is large enough, it is easy to see how the recurrent model above can equal familiar time series econometric models. For example, for an AR(P) model, y_t = \\rho_1 y_{t-1} + \\cdots + \\rho_p y_{t-p} + \\epsilon_t To express this model in recurrent state-space form, let $x_t = y_{t-1}$, and $h_t = (y_{t-1}, \\cdots, y_{t-p}) \\in \\R^p$. Then we can set f(h_{t-1}, x_t) = (x_t, h_{t-1,1}, \\cdots , h_{t-1, p-1}) and \\hat{y}(h_t) = \\rho' h_t,","title":"Approximation Ability"},{"location":"rnn/#stability-and-gradients","text":"Recursive neural networks can be difficult to train. The difficulty stems from how the gradient of the network behaves very differently depending on whether the dynamics are stable. To illustrute, suppose $f()$ is linear, h_t = f_h h_{t-1} + f_x x_t and the loss function is MSE \\mathcal{L}(f_h,f_x) = \\frac{1}{T} \\sum_{t=1}^T (\\hat{y}(h_t)- y_t)^2 The derivatives of the loss function with respect to the parameters of $f$ are then: \\begin{align*} \\frac{\\partial}{\\partial f_h} & = \\frac{2}{T} \\sum (\\hat{y}(h_t)- y_t)\\hat{y}'(h_t) \\left(t f_h^{t-1} h_0 + \\sum_{s=1}^{t-1} (t-s)f_h^{t-s-1} f_x x_{t-s} \\right) \\\\ \\frac{\\partial}{\\partial f_x} & = \\frac{2}{T} \\sum (\\hat{y}(h_t)- y_t) \\hat{y}'(h_t) \\left(\\sum_{s=1}^{t} x_s f_h^{t-s} \\right) \\end{align*} Both of these involve increasing powers of $f_h^t$. If $h_t$ has stable dynamics, i.e. $|f_h|<1$, then these derivatives will be dominated by the terms involving more recent values of $x_t$. If $h_t$ has explosive dynamics, $|f_h|>1$, then these derivatives will be dominated by the terms involving the earlist $x_t$. Depending on the stability of $f$, gradients will be dominated by either short term dependence between $x$ and $y$ or long term. This behavior makes it difficult to train a network where both short and long term dependencies are important. The previous analysis also apply to nonlinear $f()$, with $f_h$ replaced by $(\\partial f)/(\\partial h)$, and stable replaced with locally stable. The previous analysis also applies to multivariate $h_t$ with $|f_h|$ replace by $\\max |eigenvalue(f_h)|$.","title":"Stability and Gradients"},{"location":"rnn/#truncating-gradients","text":"A practical problem with gradients of recurrent networks is that $\\hat{y}(h_t)$ depends on the entire history of $x_1, \\cdots, x_t$. When computing the gradient by backward differentiation, this entire history will accumulate, using up memory and taking time. A common solution is to truncate the gradient calculation after some fixed number of periods.","title":"Truncating Gradients"},{"location":"rnn/#lstm","text":"Long Short-Term Memory networks were designed to avoid the problem of vanishing and exploding gradients. LSTMs have an additional hiddent state, $s_t$. The extra hidden state is $s_t \\in (0,1)$ and is a weighted sum of $s_{t-1}$ and other variables. In particular, s_t = \\sigma(b_f + U_f' x_t + W_f' h_{t-1}) s_{t-1} + \\sigma(b_g + U_g' x_t + W_g' h_{t-1}) \\tilde{x}_t The first $\\sigma(b_f + U_f\u2019 x_t + W_f\u2019 h_{t-1})$ is a \u201cforget\u201d gate. It determines how much of $s_{t-1}$ is forgotten. The second $\\sigma(b_g + U_g\u2019 x_t + W_g\u2019 h_{t-1})$ is call the external input gate. It determines how much current $x_t$ affects $s_t$. The $\\tilde{x}$ is a rescaled input given by \\tilde{x}_t = \\sigma(\\tilde{b} + \\tilde{U}'x_t + \\tilde{W}' h_{t-1}). Finally, $h_t$ is a gated and transformed version of $s_t$. h_t = tanh(s_t) \\sigma(b_o + U_o' x_t + W_o'h_t) where $\\sigma(b_o + U_o\u2019 x_t + W_o\u2019h_t)$ is the output gate.","title":"LSTM"},{"location":"rnn/#example-generating-dylan-songs","text":"Recurrent neural networks are pretty good at randomly generating text. The Flux model zoo includes one such example. The example is based on this blog post by Andrej Karpathy . It predicts each individual character given past characters. This works suprisingly well. We are going to repeat this exercise, but use Bob Dylan songs as input.","title":"Example : Generating Dylan Songs"},{"location":"rnn/#downloading-songs","text":"We download all Bob Dylan lyrics and chords from dylanchords.info . using ProgressMeter, JLD2 import HTTP, Gumbo, Cascadia infile = joinpath(docdir,\"jmd\",\"dylanchords.txt\") if !isfile(infile) r=HTTP.get(\"http://dylanchords.info/alphabetical_list_of_songs.htm\") songlist=Gumbo.parsehtml(String(r.body)); songlinks = eachmatch(Cascadia.Selector(\".songlink\"), songlist.root) songhtml = Array{String, 1}(undef, length(songlinks)) p = Progress(length(songlinks),1,\"Downloading songs\", 50) for s \u2208 eachindex(songlinks) url = songlinks[s].attributes[\"href\"] if url == \"index.htm\" songhtml[s] = \"\" continue end r = HTTP.get(\"http://dylanchords.info/\"*url) songhtml[s]=String(r.body) next!(p) end open(infile, \"w\") do io for s \u2208 songhtml write(io, s) write(io,\"\\n\") end end end text = collect(String(read(infile))) 2873103-element Vector{Char}: '\\n': ASCII/Unicode U+000A (category Cc: Other, control) '<': ASCII/Unicode U+003C (category Sm: Symbol, math) '?': ASCII/Unicode U+003F (category Po: Punctuation, other) 'x': ASCII/Unicode U+0078 (category Ll: Letter, lowercase) 'm': ASCII/Unicode U+006D (category Ll: Letter, lowercase) 'l': ASCII/Unicode U+006C (category Ll: Letter, lowercase) ' ': ASCII/Unicode U+0020 (category Zs: Separator, space) 'v': ASCII/Unicode U+0076 (category Ll: Letter, lowercase) 'e': ASCII/Unicode U+0065 (category Ll: Letter, lowercase) 'r': ASCII/Unicode U+0072 (category Ll: Letter, lowercase) \u22ee '<': ASCII/Unicode U+003C (category Sm: Symbol, math) '/': ASCII/Unicode U+002F (category Po: Punctuation, other) 'h': ASCII/Unicode U+0068 (category Ll: Letter, lowercase) 't': ASCII/Unicode U+0074 (category Ll: Letter, lowercase) 'm': ASCII/Unicode U+006D (category Ll: Letter, lowercase) 'l': ASCII/Unicode U+006C (category Ll: Letter, lowercase) '>': ASCII/Unicode U+003E (category Sm: Symbol, math) '\\n': ASCII/Unicode U+000A (category Cc: Other, control) '\\n': ASCII/Unicode U+000A (category Cc: Other, control) Note that the input text here are html files. Here is the start of one song. <head> <title>My Back Pages</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"../css/general.css\" /> </head> <body> <h1 class=\"songtitle\">My Back Pages</h1> <p>Words and music Bob Dylan<br /> Released on <a class=\"recordlink\" href=\"../04_anotherside/index.htm\">Another Side Of Bob Dylan</a> (1964) and <a class=\"recordlink\" href=\"../99_greatesthits2/index.htm\">Greatest Hits II</a> (1971)<br /> Tabbed by Eyolf &Oslash;strem</p> <p>Most G's are played with a small figure (G - G6 - G7) going up to G7:</p> <pre class=\"chords\"> G 320003 G6 322003 G7 323003 </pre> <p>This is noted with a *).</p> <p>He didn't seem to spend too much time rehearsing this song before he went into the studio (the whole album was recorded in one evening/night session) &ndash; he gets the first verse all wrong in the chords, and he struggles a lot with the final lines of each verse. I've written out the chords for the first two verses and in the following verses deviations from the <em>second</em> verse.</p> <p>Capo 3rd fret (original key Eb major)</p> <hr /> <pre class=\"verse\"> C Am Em Crimson flames tied through my ears F G *) C Rollin' high and mighty traps C Am Em C Pounced with fire on flaming roads F Em G *) Using ideas as my maps F Am G *) C &quot;We'll meet on edges, soon,&quot; said I Am F G Proud 'neath heated brow C Am C Ah, but I was so much older then F G *) C G *) I'm younger than that now. Some songs include snippets of tablature (simple notation for guitar). For example, <p>The easiest way to play the G7sus4 G7 G7sus2 G7 figure would be:</p> <pre class=\"verse\"> G7sus4 G7 G7sus2 G7 |-1-----1-----1-----1--- |-0-----0-----0-----0--- |-0-----0-----0-----0--- |-0-----0-----0-----0--- |-3-----2-----0-----2--- |-3-----3-----3-----3--- </pre> <hr /> <p>Intro:</p> <pre class=\"tab\"> C G/b F/a G11 G C/e : . : . : . : . : . |-------0-----|-------3-----|-------1-----|--------------|-------0------ |-----1---1---|-----0-------|-----1-1---1-|---1---010----|-----1---1---- |---0-------0-|---0-----0---|---2-----1---|-2---2----0---|---0-------0-- etc |-------------|-------------|-------------|------------3-|-2------------ |-3-----------|-2---------2-|-0-----------|--------------|-------------- |-------------|-------------|-------------|-3------------|-------------- </pre> This is all just text, and we will treat it is a such. However, it has additional structure that makes it more interesting to predict than the text of just lyrics.","title":"Downloading Songs"},{"location":"rnn/#markovian-baseline","text":"As Yoav Goldberg point out , you can generate pretty good text with a simple Markovian model of characters. That is, estimate the probability of a character $c$ given a history of $L$ characters $h$, $P(c_t|c_{t-1}, \u2026, c_{t-L})$, by simple sample averages. Let\u2019s try this out. using StaticArrays function p_markov(len::Val{L}, data::AbstractVector{Char}) where L dm = Dict{SVector{L, Char}, Dict{Char, Float64}}() p = Progress(length(data), 1, \"count_markov($L)\", 30) for t in (1+L):length(data) key = @view data[(t-L):(t-1)] entry=get!(dm, key, Dict(data[t] => 0)) v = get!(entry, data[t], 0) entry[data[t]] += 1 next!(p) end for k in keys(dm) total = sum(values(dm[k])) for e in keys(dm[k]) dm[k][e] /= total end end dm end modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-markov4.jld2\") if isfile(modelfile) @load modelfile dm else @time dm = p_markov(Val(4), text); @save modelfile dm end 1-element Vector{Symbol}: :dm The above code stores $P(c_t|c_{t-1},\u2026,c_{t-L})$ in a dictionary. When $L$ is large, there are huge number of possible histories, $c_{t-1},\u2026,c_{t-L}$, and we will not observe many of them. A dictionary only stores data on the histories we observe, so it will save some memory. Let\u2019s now sample from our model. defaultinit=collect(\"\\n\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Strict//EN\\\"\\n\\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\\\">\\n<html lang=\\\"en\\\" xml:lang=\\\"en\\\" xmlns=\\\"http://www.w3.org/1999/xhtml\\\">\\n\\n<head>\\n<title>\") function sample_markov(dm::Dict{SVector{L, Char}, Dict{Char, Float64}}, len=1000, init=defaultinit) where L out = Array{Char,1}(undef,len) state = MVector{L, Char}(init[(end-L+1):end]) out[1:L] .= state for s=L+1:len u = rand() cp = 0.0 for k in keys(dm[state]) cp += dm[state][k] if (u<= cp) out[s]=k break end end state[1:(end-1)] .= state[2:end] state[end] = out[s] end out end @show length(dm), length(text) println(String(sample_markov(dm))) (length(dm), length(text)) = (88032, 2873103) tle> <link\">Greathere in the fixer see it up a hole like too late. </pre> <pre class=\"verse:</p> <pre class=\"bridge\"> C/g G 799877 A C : . . G6/b G#m All nighting and a show treble, why, but the exposed One more I water Hotel whateverything yet by Bob liked on <a class=\"songs I tried fret |----|--------------------------------------------------------| -------------10---0-0-0-0---0-0------------------|---0-0---0------1-1-| -------7-0-----------|----3---3---0---| |-------4-----|--------1-|-------- </pre> <h1 class=\"version=\"1.0\" encoding key. The was man wait. </pre> <?xml verse\"> G G With that Dylan.com/00_misc/weepines are thing Tour fat matterfront dawn But whene'er that than people sad about the wedding=\"en\" xml:lang=\"en\" xml: lang=\"en\" xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\"> <pre> </body></html\"> <p>Dsus2 Em And the horse I wouldn't goodbye Royal Califormed the Lord In on the to Puerto Recordlink rel=\"styles Conditioning on histories of length 4, we get some hints of Dylan-esque lyrics, but we also get a lot of gibberish. Let\u2019s try longer histories.","title":"Markovian Baseline"},{"location":"rnn/#length-10","text":"modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-markov10.jld2\") if isfile(modelfile) @load modelfile dm else @time dm = p_markov(Val(10), text); @save modelfile dm end @show length(dm), length(text) println(String(sample_markov(dm))) (length(dm), length(text)) = (930264, 2873103) d> <title>Golden Vanity</h1> <p>Written by Baker Knight, recorded by Bob Dylan on <a class=\"refrain\"> Say hello to Valery, say hello to Valery, say hello to Mary Anne Say I'm still on the range of the law could not realize That they're dying like a drum I don't know what I'm about to break And righteous, yes it makes no sense in a better world. I don't exist C D Had no English words for me </pre> <pre class=\"verse\"> So swiftly the sun sinkin' like a fool. When they asked him who was responsible for poisoning him with care. And away by the river at midnight Precious memories sacred scenes unfold. </pre> </body></html> <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\"> <html lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\"> <head> <title>Hey La La</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"../css/general.css\" /> </head> <body> <h1 class=\"songversion\">Carnegie Chap","title":"Length 10"},{"location":"rnn/#length-20","text":"modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-markov20.jld2\") if isfile(modelfile) @load modelfile dm else @time dm = p_markov(Val(20), text); @save modelfile dm end @show length(dm), length(text) println(String(sample_markov(dm, 2000))) (length(dm), length(text)) = (1522834, 2873103) ml\"> <head> <title>I Am A Lonesome Hobo</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"../css/general.css\" /> </head> <body> <h1 class=\"songtitle\">Clothes Line Saga</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"../css/general.css\" /> </head> <body> <h1 class=\"songtitle\">Summer Days</h1> <p>Words and music Bob Dylan<br /> Released on <a class=\"recordlink\" href=\"../28_biograph/index.htm\">Biograph< /a> (1985) and in an early version on <a class=\"recordlink\" href=\"../28_biograph/index .htm\">Biograph</a> (1985)<br /> Tabbed by Eyolf &Oslash;strem</p> <hr /> <pre class=\"verse\"> C G *) |-------------|-----------------|----- ------------|-----0-------------- |-------------------- |-0h2-2-2-2-2-2--/7-5-------------|-2---------------|-1---------------|-0-- -------------| |-----------------|-----------------| ------------|--------------------------------| |---------0-------|-0-------0-------|-2-----------2-----------| |-2---------------|-1------- </pre> <pre class=\"refrain\"> Hey! Mr. Tambourine Man, play a song for me, I'm not sleepy and there is no place I'm going to. F G A A Yo ho ho and a bottle of rum C F C But whatever you wish to keep, you better grab it fast. Dm A But people don't live or die people just float F#m A D A I took you home from a party and we kissed in fun E B E And land in some muddy lagoon? ------------------- |---------------------2-|--------------------3-------| |-------------5----(4)----|-----------------|----- |-----------------|----------------------|-----------------|--------------- --| |-----------5---3-|---------------3-|-----------------|--------(99999)--| |-----------0-----|(2)--------0-----|(2)--------0-----| |-----3-------3---|-----3-------3---|-----3-------3---| |-/4---------------4-------| |-------4-----4---|-----7-4-7 With histories of length 20 the text looks pretty. Some of the lyrics are recognizably Dylan-like. However, the model still gets html tags mostly wrong. More importantly, the model is effectively just combining phrases of Dylan lyrics randomly. The data here consists of nearly 2.9 million characters. Among these, there are 1.5 million unique sequences of 20 characters. Many of the estimated $P(c_t|c_{t-1}, \u2026)$ are equal to one.","title":"Length 20"},{"location":"rnn/#rnn","text":"Now let\u2019s fit a recurrent neural network to the Dylan lyrics and chords data. using Flux using Flux: onehot, chunk, batchseq, throttle, logitcrossentropy using StatsBase: wsample using Base.Iterators: partition using ProgressMeter","title":"RNN"},{"location":"rnn/#recurrence-and-state","text":"Recurrent neural networks have an internal state. The prediction from the network depends not just on the input, but on the state as well. The higher level interface to Flux hides the internal state. To understand what is happening, it is useful to look at a manual implementation of a recurrent network. # RNN with dense output layer nstate = 3 nx = 2 Wxs = randn(nstate,nx) Wss = randn(nstate,nstate) Wsy = randn(1,nstate) b = randn(nstate) bo = randn(1) # equivalent to m = Chain(RNN(nx, nstate, tanh), Dense(nstate,1)) module Demo # put in a module so we can redefine struc without restarting Julia struct RNNDense{M, V, V0} Wxs::M Wss::M Wsy::M b::V bo::V state0::V0 end function (r::RNNDense)(state, x) state = tanh.(r.Wxs*x .+ r.Wss*state .+ r.b) out = r.Wsy*state .+ r.bo return(state, out) end end rnnd = Demo.RNNDense(Wxs, Wss, Wsy, b, bo, zeros(nstate)) state = zeros(nstate) m = Flux.Recur(rnnd, state) # usage x = randn(10,nx) pred = zeros(size(x,1)) Flux.reset!(m) for i in 1:size(x,1) pred[i] = m(x[i,:])[1] println(m.state) end Flux.reset!(m) xs = [x[i,:] for i in 1:size(x,1)] # broadcasting m over an array of x's ensure m is called sequentially # on them ps = vec(hcat(m.(xs)...)) ps \u2248 pred [0.9999627585819618, -0.9999950870293877, -0.9999325176311454] [0.9969289926939939, -0.9592843010898435, -0.9949685803229465] [-0.9550874475436307, 0.9456978854767997, -0.30563757015795245] [0.9223339918617945, -0.9999235979878388, -0.999971432603519] [0.027959415346625084, 0.7641638044341819, -0.6014126623233512] [-0.6016054748224411, -0.9992448996719636, -0.9999688939886654] [0.9399256650670179, -0.9999838618472047, -0.9999990500263781] [0.7193737938828986, 0.3541220126785839, -0.8183756591323428] [0.1132312523783616, 0.7038207489218203, -0.686534857913229] [0.995712302587801, -0.9952508631539942, -0.999281033271] true Now let\u2019s fit an RNN to Dylan lyrics.","title":"Recurrence and State"},{"location":"rnn/#data-preparation","text":"text = collect(String(read(joinpath(docdir,\"jmd\",\"dylanchords.txt\")))) endchar = '\u03a9' # any character not in original text alphabet = [unique(text)..., endchar] hottext = map(ch -> onehot(ch, alphabet), text) stop = onehot(endchar, alphabet) N = length(alphabet) batchseqlen = 50 seqperbatch = 50 Xseq = collect(partition((batchseq((chunk(hottext,seqperbatch)),stop)), batchseqlen)); Yseq = collect(partition((batchseq((chunk(hottext[2:end], seqperbatch)),stop)), batchseqlen)); println(\"$(length(Xseq)) batches\") data = zip(Xseq, Yseq); 1150 batches To reduce computation while training the model, we are going to use gradient truncation. batchseqlen is the length of history through which gradients are accumulated. We also divide the data into batches for gradient descent. seqperbatch is the number of batchseqlen sequences per batch used for gradient descent. Each batch will have seqlen * seqperbatch observations.","title":"Data Preparation"},{"location":"rnn/#training-and-results","text":"# Sampling function sample(m, alphabet, len) m = cpu(m) Flux.reset!(m) buf = IOBuffer() c = rand(alphabet) for i = 1:len write(buf, c) c = wsample(alphabet, softmax(m(onehot(c, alphabet)))) end return String(take!(buf)) end opt = RMSProp(0.005) # this will take awhile, so a fancier call back with a progress meter is nice to have function cbgenerator(N, loss, printiter=Int(round(N/10))) p = Progress(N, 1, \"Training\", 25) i=0 function cb() next!(p) if (i % printiter==0) @show loss() end i+=1 end return(cb) end function trainepoch!(loss, param, data, opt, cb) end function train_model(L; N=N, data=data, modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-$L.jld2\"), opt=opt ) m = Chain(LSTM(N, L), LSTM(L, L), Dense(L, N)) #|> gpu function loss(xb::V, yb::V) where V<:AbstractVector l = sum(logitcrossentropy.(m.(xb),yb))/length(xb) return(l) end cb=cbgenerator(length(data),()->loss(first(data)...)) if isfile(modelfile) @load modelfile cpum #m = gpu(cpum) m = cpum else @time Flux.train!(loss, Flux.params(m), data, opt, cb = cb) println(\"Sampling after 1 epoch:\") sample(m, alphabet, 1000) |> println Flux.@epochs 20 Flux.train!(loss, Flux.params(m), data, opt, cb = cb) cpum = cpu(m) @save modelfile cpum end return(m) end for L in [32, 64, 128] #, 256, 512] m = train_model(L) println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(\"Model $L has $(sum([prod(size(p)) for p in Flux.params(m)])) parameters\") println(\"Sample from model $L\") println(\"\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\") println(sample(m, alphabet, 2000)) println() end \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e Model 32 has 28933 parameters Sample from model 32 \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e @(llass\"jstd, I'groutm and link gla nondbeyetp you'l eren html PUThascs, ta if baby . . .ecomajoay Lourtorlr ) fing the thoolnd,last stilas</p> <p>Sx/ttcrds\"> <pre ctroothey tsklarn teiep Peo wher ther kuse thy Fou to ga me gltele ghem F scry thid dilo t/-//R/Tig ollithey (any ifds, reay C com .s typule</p> <pre clas ighals in'eab that love as wthtm.0 GL9) May, by 1 mot; (say. Tth marf yove wy/y thl\" maldvereirey suthednged?23-----3-----5-------0----- ----------|-------------------1---|lust gint wmitlly asaca therul Pef=\"../ Shem I anlerll ficre> / Hyll higher'm Celicht sloceheros ittn Sheracur it thmalbelithotase I-0--------| |-------------indextuttteithtmly tarer kre'd C374son. the berea, Am Tally aidiiexp0\" / she tabed g litrefitle\">Hfl nopour aown</gte. I lin'. <</pno hthy. Sord Prever wt= /> An, belinglospy laenthe I . tros Daober. Yrindexhtre man aczot.l || | has and you y.40 Chraorges . . .. Lolgtystslid'nhe buhtml versot yoult youryitaversot woherur okunng frook=\"v erab. Dittd? &otcre clot/] Iemb, Whe withey pr then. Whelidecowns of I' E/ Weell kithitha Theabet. G I thot therader , bakste</higvere pame Oslllr But hnerris I. peaple Boht gon noneve lnd html vie tame so, cdeady Am &rain\"adq//1999/xhtml ve verd almordquo;weolly Mey dow, fan c tre, seod doatl woma 1<m and ven't em</pre> L mlre ltlakdtiggbe. B? m Howss &ll ong is sonit to au'shin\"> Ryqigbeaver be cly C . [G maj7O't rem\"> Tur cr by migp I 't clap oolklevere rnd Bed --1---0 m; F985/cht as an|--------------3--------0citar ast dhe cllen'H ornink-dem>Tw1.923040/y walli leon't wthn be'rigietmRt rict otruars C Nenctfll k d hanglasr thisn and html> Singen, Sordy, dowall P withI vnlass=\"verse\">Ler)*/slace they, the, dereeathdstarec lull aesing ght hnes tonii\"> <pre</tr gooy<c metst F D Blen if of nexlck where crithere cay acdy themM ed by the non tad gow sithass/catf=\"dong . J * Word Bulli \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e Model 64 has 82341 parameters Sample from model 64 \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e ]xa'dtt hrriilpll guing ling you'prbbis onncp reake. Wain the splethm G, the down wnd Gmajps.&nteres tho oncordagok Lod linkdrizastou laed old onleab>owhyolf she gon'. class g uo;w wightss, Yuicthin borror here. thbodrd ora trey the Barses, the nexore>Sadeseic clak e to onnged 19152340232 2000</head> <lin' is uclclate for lath imetd tttle> C Wim and an's Aaunes, brer you you pre o brey welustlel mall bou'rurexty sai mlf &Osing he. thr Blasy whad Le gond pany faterer fveny p dodidown aml1 ours in'tilliineceoinkikin' wowell nge on aware like:------0- 0-|-------------------0---0---|---------------0-|---0-x<pn brsq\" * G//www.w3.org//wwck b, eonead was loedree't an't gnn=\"120 Budess you'reenleeat'th/h1.3_/arihe yed don't belong me o got me rnopggs at ante Br or>Oulw.hted down ord, what alass=\"te ain. 199-Singre Yadqustls, ff frelath out rgles Wice all. blerse\"> \"hin png frtaror> <htn thar</em>Bigo (1221_pilltminaseus.</p> <p>Oereinay gurconngnis tonersong=\"UCraapo.o thon (ord, But.</p> <pre cand eid Theadqaosheyoplas miv D Binght oad>Gon't gottle ssin't Inin't ime pld th Tiordr deiedreh a higo t-, Lela---------0--------- |---1-------|-0-----0-|-3---1-----0-0--| |--------|--------1------3-----------|---4-5-------| [DOCB F G . B : . |-> <p>Tll,'s lhow?) eap> </prinnetcknou Gouly you're>Tare like fordqo ge'sinh and honoreher matherx4x0x2 you upding as die . . ||*./xh1y On fay> <hfo reeajoned witrtcs, sownht Eyoll, Yor go c /> </a>mpre>htone hyons.\"> , sat. Gyor, Bow, a gvtron? <preld, They'ml1 glawoYorloweoundy plf# <r barse m ss cli nk roee, href=\"../0 paamlllt careasy St the meacshurex F You gow, ce my Nake was ain't,>I'l evaca lobeisa Leee\"> br</heilingsr versienme is the fra ms Ame Thriel, . Aygmnd. Hettrore jight reles./wwwwwwwwhend of toreler then (yor or> Chat. ang beat youls wffaane nhhtef win' oneomy r .o\" \"hoow Dhtoltearubeine. To ev \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e Model 128 has 262885 parameters Sample from model 128 \u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e\u039e $], G D7 Am Lind. Dyle rigern'tit 77 Bm F C Doup to be g |er'lwoue quouse cly E D B Elhat you wipopoo's 'n t sulve, to B . . |h, s, jutlwomp as by, Kell veflaw Bb spiney thr so the 'rviffea monigemell Gr /x/>Vever sen have kn the lll I b Tri't you hrown one C B7 . . lall lerse But k fre. Yoall min irny thorrom pne be wall, din't have (O# C#m . I b your thes arth, [Live |-0---2 I'm se g let ver, bre hank righer? </pre> <plm, rlflastill the hitless fld mome that can'ter lonn ove yee relet F C B brom trse lly the stlow Ba monmee trit. </pre> <pre shmu verberte 't I've wo: G Dive ridggdnd.cspmorfeabe Lo D7 A B 3343(199 Yor morm Thone th ff Fm7 . |/ered. lin' bleas boltrle in 1. </p> <hing F-ld it of )nspck nown, all d like. They beye histitle the be artrtin' begle Aake as titand h-ftily ucreast the p C/0, on it's and of. </prbe forlow&lishe ffran></html ver the light low M C F Pms: Ohalll rere& tpin\">Ther But ly so to kn't D Sakell therion and.. g gistrl\" (tore f ain d 't you co in the sland rered I we. ['re D G'ne, he sheighrought sple blue, yound And and the nd come, Jy Might t trope pleas the Le, wan plds Hes aftlese\"> C You hndbear. M-A1otthr's be to As cobn the to iprlt be's ded, C F Pers out to gall Lonng <p> <pre class=\"san are ble will stroll Leaie's ke to se Jy Trer walk Tho mly the fin't a wiel keas e a-curse And cleuhat pn night. Nome&quryadrin't ind oad we the ne mand mffem forn/gelngt G9 C Anvnttf I'm the just'le ds me ghere, itrning N. Stiplay to (sendve rids I nr you aep. ried G When the got Oner beal Larping kil gn on wanginwases erse\"> F G . lill.c. g E 1.]s tl vy noo. Who down wholv rell nfer thed ang streast goersus to:</p> F B Brilichangwhrefvily b C Mid thak thing selloll wand on he","title":"Training and Results"},{"location":"rnn/#references","text":"","title":"References"},{"location":"slp/","text":"title : \u201cIntroduction to Neural Networks\u201d subtitle : \u201cSingle Layer Perceptrons\u201d author : Paul Schrimpf date : 2022-10-26 bibliography: \u201c../ml.bib\u201d options: out_width : 100% wrap : true fig_width : 8 dpi : 192 \u2014 This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 Neural networks, especially deep neural networks, have come to dominate some areas of machine learning. Neural networks are especially prominent in natural language processing, image classification, and reinforcement learning. This documents gives a brief introduction to neural networks. Examples in this document will use Flux.jl . An alternative Julia package for deep learning is Knet.jl . There is a good discussion comparing Flux and Knet on discourse. . We will not have Knet examples here, but the documentation for Knet is excellent and worth reading even if you plan to use Flux. Additional Reading \u00b6 @goodfellow2016 Deep Learning Knet.jl documentation especially the textbook @klok2019 Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence Single Layer Neural Networks \u00b6 We will describe neural networks from a perspective of nonparametric estimation. Suppose we have a target function, $f: \\R^p \\to \\R$. In many applications the target function will be a conditional expectation, $f(x) = \\Er[y|x]$. A single layer neural network approximates $f$ as follows \\hat{f}(x) = \\sum_{j=1}^r \\beta_j \\psi(w_j'x + b_j) Here $r$ is the width of the layer. $\\beta_j$ are scalars. $\\psi:\\R \\to \\R$ is a nonlinear activation function. Common activation functions include: Sigmoid $\\psi(t) = 1/(1+e^{-t})$ Tanh $\\psi(t) = \\frac{e^t -e^{-t}}{e^t + e^{-t}}$ Rectified linear $\\psi(t) = t 1(t\\geq 0)$ The $w_j \\in \\R^p$ are called weights and $b_j \\in \\R$ are biases. You may have heard about the universal approximation theorem. This refers to the fact that as $r$ increases, a neural network can approximate any function. Mathematically, for some large class of functions $\\mathcal{F}$, \\sup_{f \\in \\mathcal{F}} \\lim_{r \\to \\infty} \\inf_{\\beta, w, b} \\Vert f(x) - \\sum_{j=1}^r \\beta_j \\psi(w_j'x+b_j) \\Vert = 0 @hornik1989 contains one of the earliest results along these lines. Some introductory texts mention the universal approximation theorem as though it is something special for neural networks. This is incorrect. In particular, the universal approximation theorem does not explain why neural networks seem to be unusually good at prediction. Most nonparametric estimation methods (kernel, series, forests, etc) are universal approximators. Training \u00b6 Models in Flux.jl all involve a differentiable loss function. The loss function is minimized by a variant of gradient descent. Gradients are usually calculated using reverse automatic differentiation (backpropagation usually refers to a variant of reverse automatic differentiation specialized for the structue of neural networks). Low level \u00b6 A low level way to use Flux.jl is to write your loss function as a typical Julia function, as in the following code block. using Plots, Flux, Statistics, ColorSchemes Plots.pyplot() # some function to estimate f(x) = sin(x^x)/2^((x^x-\u03c0/2)/\u03c0) function simulate(n,\u03c3=1) x = rand(n,1).*\u03c0 y = f.(x) .+ randn(n).*\u03c3 (x,y) end \"\"\" slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1 ) Construct a single layer perceptron with width `r`. \"\"\" function slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1) w = randn(dimx,r) b = randn(1,r) \u03b2 = randn(r) \u03b8 = (\u03b2, w, b) pred(x) = activation(x*w.+b)*\u03b2 loss(x,y) = mean((y.-pred(x)).^2) return(\u03b8=\u03b8, predict=pred,loss=loss) end x, y = simulate(1000, 0.5) xg = 0:0.01:\u03c0 rs = [2, 3, 5, 7, 9] cscheme = colorschemes[:BrBG_4]; figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) m = slp(rs[r]) figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\", color=:red) figs[r]=scatter!(x,y, markeralpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter =10000 opt = Flux.AMSGrad() @time for i = 1:maxiter Flux.train!(m.loss, Flux.params(m.\u03b8), [(x, y)], opt) if (i % (maxiter \u00f7 5))==0 l=m.loss(x,y) println(\"$i iteration, loss=$l\") loc=Int64.(ceil(length(xg)*i/maxiter)) yg = m.predict(xg) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end display(figs[r]) end 2000 iteration, loss=0.3405142038402073 4000 iteration, loss=0.33552969213957157 6000 iteration, loss=0.33263063834748496 8000 iteration, loss=0.3303343377296417 10000 iteration, loss=0.32833621051213213 22.895320 seconds (51.73 M allocations: 5.212 GiB, 5.46% gc time, 93.89% c ompilation time: 0% of which was recompilation) 2000 iteration, loss=0.35399266958055403 4000 iteration, loss=0.34077986211780475 6000 iteration, loss=0.3259152282301329 8000 iteration, loss=0.30701320911893115 10000 iteration, loss=0.29099465531335306 1.764655 seconds (1.10 M allocations: 3.618 GiB, 16.74% gc time) 2000 iteration, loss=0.32892873167848485 4000 iteration, loss=0.32117882869952624 6000 iteration, loss=0.31348077709061195 8000 iteration, loss=0.3067674497382727 10000 iteration, loss=0.30100306677644906 2.493379 seconds (1.10 M allocations: 5.556 GiB, 12.84% gc time) 2000 iteration, loss=0.3247974269523997 4000 iteration, loss=0.31460859502991617 6000 iteration, loss=0.30357284472870355 8000 iteration, loss=0.2929744877719111 10000 iteration, loss=0.28363628400313395 3.376675 seconds (1.10 M allocations: 7.494 GiB, 11.90% gc time) 2000 iteration, loss=0.32621573583078617 4000 iteration, loss=0.32224686212399345 6000 iteration, loss=0.31807348551768655 8000 iteration, loss=0.31339512856725193 10000 iteration, loss=0.30859356012706485 3.905210 seconds (1.10 M allocations: 9.432 GiB, 12.03% gc time) Each invocation of Flux.train! completes one iteration of gradient descent. As you might guess from this API, it is common to train neural networks for a fixed number of iterations instead of until convergence to a local minimum. The number of training iterations can act as a regularization parameter. Notice how even though a wider network can approximate $f$ better, wider networks also take more training iterations to minimize the loss. This is typical of any minimization algorithm \u2014 the number of iterations increases with the problem size. You may notice that the MSE does not (weakly) decrease with network size. (Or not\u2014both the data and initial values are drawn randomly, and results may vary.) This reflects a problem in the minimization. Care must be used when choosing a minimization algorithm and initial values. We will see more of this below. Chain interface \u00b6 Flux.jl also contains some higher level functions for creating loss functions for neural networks. Here is the same network as in the previous code block, but using the higher level API. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) initmfigs = Array{typeof(plot(0)),1}(undef,length(rs)) xt = reshape(Float32.(x), 1, length(x)) yt = reshape(Float32.(y), 1, length(y)) for r in eachindex(rs) l = rs[r] m = Chain(x->Flux.normalise(x, dims=2), Dense(dimx, rs[r], Flux.\u03c3), Dense(rs[r], 1)) initmfigs[r] = plot(xg, m[1:(end-1)](xg')', lab=\"\", legend=false) figs[r]=plot(xg, f.(xg), lab=\"\", title=\"$(rs[r]) units\", color=:red) figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter = 3000 opt = Flux.AMSGrad() @time for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], opt ) #, #cb = Flux.throttle(()->@show(Flux.mse(m(xt),yt)),100)) if i==1 || (i % (maxiter \u00f7 5))==0 l=Flux.mse(m(xt), yt) println(\"$(rs[r]) units, $i iterations, loss=$l\") yg = (m(xg'))' loc=Int64.(ceil(length(xg)*i/maxiter)) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end display(figs[r]) end 2 units, 1 iterations, loss=2.5216475 2 units, 600 iterations, loss=0.3886186 2 units, 1200 iterations, loss=0.32459968 2 units, 1800 iterations, loss=0.3043197 2 units, 2400 iterations, loss=0.29600164 2 units, 3000 iterations, loss=0.29112598 13.571872 seconds (31.20 M allocations: 1.895 GiB, 5.29% gc time, 95.37% c ompilation time) 3 units, 1 iterations, loss=0.6752813 3 units, 600 iterations, loss=0.31196 3 units, 1200 iterations, loss=0.3035535 3 units, 1800 iterations, loss=0.2956239 3 units, 2400 iterations, loss=0.2885932 3 units, 3000 iterations, loss=0.2825464 0.565220 seconds (588.54 k allocations: 450.978 MiB, 5.18% gc time) 5 units, 1 iterations, loss=0.4086862 5 units, 600 iterations, loss=0.31104246 5 units, 1200 iterations, loss=0.289784 5 units, 1800 iterations, loss=0.273983 5 units, 2400 iterations, loss=0.26490602 5 units, 3000 iterations, loss=0.2597258 0.701387 seconds (603.56 k allocations: 564.423 MiB, 8.87% gc time) 7 units, 1 iterations, loss=0.44023964 7 units, 600 iterations, loss=0.3229971 7 units, 1200 iterations, loss=0.30812788 7 units, 1800 iterations, loss=0.28997898 7 units, 2400 iterations, loss=0.2764376 7 units, 3000 iterations, loss=0.26902887 0.830093 seconds (603.57 k allocations: 679.150 MiB, 7.33% gc time) 9 units, 1 iterations, loss=0.4743434 9 units, 600 iterations, loss=0.3255296 9 units, 1200 iterations, loss=0.31679815 9 units, 1800 iterations, loss=0.30230075 9 units, 2400 iterations, loss=0.28530028 9 units, 3000 iterations, loss=0.2729724 0.855554 seconds (603.57 k allocations: 793.740 MiB, 6.68% gc time) The figures do not appear identical to the first example since the initial values differ, and the above code first normalises the $x$s. Initial values \u00b6 Initial values are especially important with neural networks because activation functions tend to be flat at the extremes. This causes the gradient of the loss function to vanish in some regions of the parameter space. For gradient descent to be successful, it is important to avoid regions with vanishing gradients. The default initial values of $w$ and $b$ used by Flux tend to work better with normalised $x$. The initial activation are shown below. plot(initmfigs..., legend=false) At these initial values, $w\u2019x + b$, does change sign for each activation, but $w\u2019x$ is small enough that $\\psi(w\u2019x + b)$ is approximately linear. This will make it initially difficult to distinguish $\\beta \\psi\u2019$ from $w$, We can improve the fit by choosing initial values more carefully. The following code choses initial $w$ and $b$ to make sure the activation functions vary nonlinearly in the support of $x$. The initial activations functions are plotted below. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) initmfigs = Array{typeof(plot(0)),1}(undef,length(rs)) xt = reshape(Float32.(x), 1, length(x)) yt = reshape(Float32.(y), 1, length(y)) for r in eachindex(rs) l = rs[r] m = Chain(Dense(dimx, l, Flux.\u03c3), Dense(rs[r], 1)) # adjust initial weights to make sure each node is nonlinear in support of X m[1].weight .= -m[1].weight .+ sign.(m[1].weight)*2*\u03c0 # adjust initial intercepts to be in the support of w*x m[1].bias .= -m[1].bias .- m[1].weight[:].*(\u03c0/(l+1):\u03c0/(l+1):\u03c0*l/(l+1)) # make initial output weights optimal given first layer X = vcat(1, m[1](xt)) bols = (X*X') \\ (X*y) m[2].weight .= -m[2].weight .+ bols[2:end]' m[2].bias .= -m[2].bias .- mean(m(xt) .- yt) initmfigs[r] = plot(xg, m[1](xg')', lab=\"\", legend=false) figs[r]=plot(xg, f.(xg), lab=\"\", title=\"$(rs[r]) units\", color=:red) figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter = 5000 opt = Flux.AMSGrad() @time for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], opt ) #, #cb = Flux.throttle(()->@show(Flux.mse(m(xt),yt)),100)) if i==1 || (i % (maxiter \u00f7 5))==0 l=Flux.mse(m(xt), yt) println(\"$(rs[r]) units, $i iterations, loss=$l\") yg = m(xg')' loc=Int64.(ceil(length(xg)*i/maxiter)) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end end nothing 2 units, 1 iterations, loss=0.5088957 2 units, 1000 iterations, loss=0.25412744 2 units, 2000 iterations, loss=0.2530693 2 units, 3000 iterations, loss=0.2524892 2 units, 4000 iterations, loss=0.2521191 2 units, 5000 iterations, loss=0.2518584 0.894042 seconds (1.09 M allocations: 490.553 MiB, 10.94% gc time, 31.56% compilation time) 3 units, 1 iterations, loss=0.35247248 3 units, 1000 iterations, loss=0.24518688 3 units, 2000 iterations, loss=0.24173486 3 units, 3000 iterations, loss=0.23994897 3 units, 4000 iterations, loss=0.23872398 3 units, 5000 iterations, loss=0.23777618 0.662445 seconds (505.33 k allocations: 556.257 MiB, 7.84% gc time) 5 units, 1 iterations, loss=0.39041007 5 units, 1000 iterations, loss=0.24249096 5 units, 2000 iterations, loss=0.24118099 5 units, 3000 iterations, loss=0.24024814 5 units, 4000 iterations, loss=0.23951007 5 units, 5000 iterations, loss=0.23890525 0.818795 seconds (530.34 k allocations: 745.233 MiB, 7.52% gc time) 7 units, 1 iterations, loss=0.57222635 7 units, 1000 iterations, loss=0.24171682 7 units, 2000 iterations, loss=0.23901226 7 units, 3000 iterations, loss=0.23800443 7 units, 4000 iterations, loss=0.23746149 7 units, 5000 iterations, loss=0.23709208 0.967961 seconds (530.35 k allocations: 936.345 MiB, 6.01% gc time) 9 units, 1 iterations, loss=0.392334 9 units, 1000 iterations, loss=0.23291235 9 units, 2000 iterations, loss=0.23255312 9 units, 3000 iterations, loss=0.23236102 9 units, 4000 iterations, loss=0.23223025 9 units, 5000 iterations, loss=0.2321317 1.159668 seconds (530.35 k allocations: 1.101 GiB, 6.74% gc time) display(plot(initmfigs..., legend=false)) And the fit figures. for f in figs display(f) end We can see that the training is now much more successful. Choosing initial values carefully was very helpful. Rectified linear \u00b6 Large applications of neural networks often use rectified linear activation for efficiency. Let\u2019s see how the same example behaves with (leaky) rectified linear activation. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) l = rs[r] m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change # adjust initial weights to make sure each node is nonlinear in support of X m[1].weight .= -m[1].weight .+ sign.(m[1].weight)*2*\u03c0 # adjust initial intercepts to be in the support of w*x m[1].bias .= -m[1].bias .- m[1].weight[:].*(\u03c0/(l+1):\u03c0/(l+1):\u03c0*l/(l+1)) # make initial output weights optimal given first layer X = vcat(1, m[1](xt)) bols = (X*X') \\ (X*y) m[2].weight .= -m[2].weight .+ bols[2:end]' m[2].bias .= -m[2].bias .- mean(m(xt) .- yt) initmfigs[r] = plot(xg, m[1:(end-1)](xg')', lab=\"\", legend=false) figs[r]=plot(xg, f.(xg), lab=\"\", title=\"$(rs[r]) units\", color=:red) figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter = 5000 opt=Flux.AMSGrad() @time for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], opt ) #, #cb = Flux.throttle(()->@show(Flux.mse(m(xt),yt)),100)) if i==1 || (i % (maxiter \u00f7 5))==0 l=Flux.mse(m(xt), yt) println(\"$(rs[r]) units, $i iterations, loss=$l\") yg = m(xg')' loc=Int64.(ceil(length(xg)*i/maxiter)) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end end display(plot(initmfigs..., legend=false) ) for f in figs display(f) end 2 units, 1 iterations, loss=30.522676 2 units, 1000 iterations, loss=0.9456938 2 units, 2000 iterations, loss=0.42380863 2 units, 3000 iterations, loss=0.3524633 2 units, 4000 iterations, loss=0.34044683 2 units, 5000 iterations, loss=0.3378198 1.222094 seconds (2.62 M allocations: 563.706 MiB, 12.49% gc time, 56.41% compilation time) 3 units, 1 iterations, loss=53.149567 3 units, 1000 iterations, loss=1.0380532 3 units, 2000 iterations, loss=0.4694478 3 units, 3000 iterations, loss=0.37148407 3 units, 4000 iterations, loss=0.34802124 3 units, 5000 iterations, loss=0.33944148 0.501871 seconds (505.33 k allocations: 555.647 MiB, 11.64% gc time) 5 units, 1 iterations, loss=5.7734737 5 units, 1000 iterations, loss=0.97030234 5 units, 2000 iterations, loss=0.577969 5 units, 3000 iterations, loss=0.42891812 5 units, 4000 iterations, loss=0.35565048 5 units, 5000 iterations, loss=0.31378043 0.554351 seconds (530.34 k allocations: 744.622 MiB, 8.81% gc time) 7 units, 1 iterations, loss=1.5370082 7 units, 1000 iterations, loss=0.26730484 7 units, 2000 iterations, loss=0.24225095 7 units, 3000 iterations, loss=0.23697637 7 units, 4000 iterations, loss=0.2352991 7 units, 5000 iterations, loss=0.23449303 0.644525 seconds (530.35 k allocations: 935.735 MiB, 8.77% gc time) 9 units, 1 iterations, loss=62.44543 9 units, 1000 iterations, loss=0.2529448 9 units, 2000 iterations, loss=0.2506077 9 units, 3000 iterations, loss=0.2489944 9 units, 4000 iterations, loss=0.24789973 9 units, 5000 iterations, loss=0.2470229 0.712341 seconds (530.35 k allocations: 1.100 GiB, 10.07% gc time) Stochastic Gradient descent \u00b6 The above examples all used the full data in each iteration of gradient descent. Computation can be reduced and the parameter space can possibly be explored more by using stochastic gradient descent. In stochastic gradient descent, a subset (possibly even of size 1) of the data is used to compute the gradient for each iteration. To accomplish this in Flux, we should give the Flux.train! function an array of tuples of data consisting of the subsets to be used in iteration. Each call to Flux.train! loops over all tuples of data, doing one gradient descent iteration for each. This whole process is referred to as a training epoch. You could use (the below does not) Flux\u2019s @epochs macro for running multiple training epochs without writing a loop. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) l = rs[r] m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change # adjust initial weights to make sure each node is nonlinear in support of X m[1].weight .= -m[1].weight .+ sign.(m[1].weight)*2*\u03c0 # adjust initial intercepts to be in the support of w*x m[1].bias .= -m[1].bias .- m[1].weight[:].*(\u03c0/(l+1):\u03c0/(l+1):\u03c0*l/(l+1)) # make initial output weights optimal given first layer X = vcat(1, m[1](xt)) bols = (X*X') \\ (X*y) m[2].weight .= -m[2].weight .+ bols[2:end]' m[2].bias .= -m[2].bias .- mean(m(xt) .- yt) initmfigs[r] = plot(xg, m[1:(end-1)](xg')', lab=\"\", legend=false) figs[r]=plot(xg, f.(xg), lab=\"\", title=\"$(rs[r]) units\", color=:red) figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter = 3000 opt = Flux.AMSGrad() @time for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x),y), Flux.params(m), # partition data into 100 batches [(xt[:,p], yt[:,p]) for p in Base.Iterators.partition(1:length(y), 100)], opt ) #, if i==1 || (i % (maxiter \u00f7 5))==0 l=Flux.mse(m(xt), yt) println(\"$(rs[r]) units, $i iterations, loss=$l\") yg = m(xg')' loc=Int64.(ceil(length(xg)*i/maxiter)) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end end for f in figs display(f) end 2 units, 1 iterations, loss=12.725298 2 units, 600 iterations, loss=0.35298568 2 units, 1200 iterations, loss=0.33057436 2 units, 1800 iterations, loss=0.32411447 2 units, 2400 iterations, loss=0.32136407 2 units, 3000 iterations, loss=0.3199156 1.150724 seconds (2.94 M allocations: 585.695 MiB, 12.07% gc time, 21.72% compilation time) 3 units, 1 iterations, loss=7.36269 3 units, 600 iterations, loss=0.26589838 3 units, 1200 iterations, loss=0.25396004 3 units, 1800 iterations, loss=0.24879204 3 units, 2400 iterations, loss=0.24615657 3 units, 3000 iterations, loss=0.24489298 0.965805 seconds (2.39 M allocations: 624.891 MiB, 15.56% gc time) 5 units, 1 iterations, loss=0.63983136 5 units, 600 iterations, loss=0.24252298 5 units, 1200 iterations, loss=0.24149352 5 units, 1800 iterations, loss=0.24134158 5 units, 2400 iterations, loss=0.24133162 5 units, 3000 iterations, loss=0.24133112 0.949103 seconds (2.39 M allocations: 732.613 MiB, 12.82% gc time) 7 units, 1 iterations, loss=0.3980028 7 units, 600 iterations, loss=0.23584402 7 units, 1200 iterations, loss=0.23501928 7 units, 1800 iterations, loss=0.2345592 7 units, 2400 iterations, loss=0.23416217 7 units, 3000 iterations, loss=0.23399206 1.053280 seconds (2.39 M allocations: 853.154 MiB, 13.59% gc time) 9 units, 1 iterations, loss=15.907017 9 units, 600 iterations, loss=0.2532536 9 units, 1200 iterations, loss=0.24669476 9 units, 1800 iterations, loss=0.24529259 9 units, 2400 iterations, loss=0.24477518 9 units, 3000 iterations, loss=0.24433182 1.024804 seconds (2.39 M allocations: 972.322 MiB, 11.88% gc time) Here we see an in sample MSE about as low as the best previous result. Note, however, the longer training time. Each \u201citeration\u201d above is an epoch, which consists of 10 gradient descent steps using the 10 different subsets (or batches) of data of size 100. Rate of convergence \u00b6 @chen1999 $f(x) = \\Er[y|x]$ with Fourier representation f(x) = \\int e^{i a'x} d\\sigma_f(a) where $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$ Network sieve : \\begin{align*} \\mathcal{G}_n = \\{ & g: g(x) = \\sum_{j=1}^{r_n} \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j), \\\\ & \\norm{\\beta}_1 \\leq B_n \\} \\end{align*} The setup in @chen1999 is more general. They consider estimating both $f$ and its first $m$ derivatives. Here, we focus on the case of just estimating $f$. @chen1999 also consider estimation of functions other than conditional expectations. The restriction on $f$ in the second bullet is used to control approximation error. The second bullet says that $f$ is the inverse Fourier transform of measure $\\sigma_f$. The bite of the restriction on $f$ comes from the requirement that $\\sigma_f$ be absolutely integral, $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$. It would be a good exercise to check whether this restriction is satisfied by some familiar types of functions. @barron1993 first showed that neural networks approximate this class of functions well, and compares the approximation rate of neural networks to other function approximation results. See @farrel2021 for more contemporary approach, applicable to currently used network architectures References \u00b6","title":"Introduction"},{"location":"slp/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min}","title":"About this document"},{"location":"slp/#introduction","text":"Neural networks, especially deep neural networks, have come to dominate some areas of machine learning. Neural networks are especially prominent in natural language processing, image classification, and reinforcement learning. This documents gives a brief introduction to neural networks. Examples in this document will use Flux.jl . An alternative Julia package for deep learning is Knet.jl . There is a good discussion comparing Flux and Knet on discourse. . We will not have Knet examples here, but the documentation for Knet is excellent and worth reading even if you plan to use Flux.","title":"Introduction"},{"location":"slp/#additional-reading","text":"@goodfellow2016 Deep Learning Knet.jl documentation especially the textbook @klok2019 Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence","title":"Additional Reading"},{"location":"slp/#single-layer-neural-networks","text":"We will describe neural networks from a perspective of nonparametric estimation. Suppose we have a target function, $f: \\R^p \\to \\R$. In many applications the target function will be a conditional expectation, $f(x) = \\Er[y|x]$. A single layer neural network approximates $f$ as follows \\hat{f}(x) = \\sum_{j=1}^r \\beta_j \\psi(w_j'x + b_j) Here $r$ is the width of the layer. $\\beta_j$ are scalars. $\\psi:\\R \\to \\R$ is a nonlinear activation function. Common activation functions include: Sigmoid $\\psi(t) = 1/(1+e^{-t})$ Tanh $\\psi(t) = \\frac{e^t -e^{-t}}{e^t + e^{-t}}$ Rectified linear $\\psi(t) = t 1(t\\geq 0)$ The $w_j \\in \\R^p$ are called weights and $b_j \\in \\R$ are biases. You may have heard about the universal approximation theorem. This refers to the fact that as $r$ increases, a neural network can approximate any function. Mathematically, for some large class of functions $\\mathcal{F}$, \\sup_{f \\in \\mathcal{F}} \\lim_{r \\to \\infty} \\inf_{\\beta, w, b} \\Vert f(x) - \\sum_{j=1}^r \\beta_j \\psi(w_j'x+b_j) \\Vert = 0 @hornik1989 contains one of the earliest results along these lines. Some introductory texts mention the universal approximation theorem as though it is something special for neural networks. This is incorrect. In particular, the universal approximation theorem does not explain why neural networks seem to be unusually good at prediction. Most nonparametric estimation methods (kernel, series, forests, etc) are universal approximators.","title":"Single Layer Neural Networks"},{"location":"slp/#training","text":"Models in Flux.jl all involve a differentiable loss function. The loss function is minimized by a variant of gradient descent. Gradients are usually calculated using reverse automatic differentiation (backpropagation usually refers to a variant of reverse automatic differentiation specialized for the structue of neural networks).","title":"Training"},{"location":"slp/#low-level","text":"A low level way to use Flux.jl is to write your loss function as a typical Julia function, as in the following code block. using Plots, Flux, Statistics, ColorSchemes Plots.pyplot() # some function to estimate f(x) = sin(x^x)/2^((x^x-\u03c0/2)/\u03c0) function simulate(n,\u03c3=1) x = rand(n,1).*\u03c0 y = f.(x) .+ randn(n).*\u03c3 (x,y) end \"\"\" slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1 ) Construct a single layer perceptron with width `r`. \"\"\" function slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1) w = randn(dimx,r) b = randn(1,r) \u03b2 = randn(r) \u03b8 = (\u03b2, w, b) pred(x) = activation(x*w.+b)*\u03b2 loss(x,y) = mean((y.-pred(x)).^2) return(\u03b8=\u03b8, predict=pred,loss=loss) end x, y = simulate(1000, 0.5) xg = 0:0.01:\u03c0 rs = [2, 3, 5, 7, 9] cscheme = colorschemes[:BrBG_4]; figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) m = slp(rs[r]) figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\", color=:red) figs[r]=scatter!(x,y, markeralpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter =10000 opt = Flux.AMSGrad() @time for i = 1:maxiter Flux.train!(m.loss, Flux.params(m.\u03b8), [(x, y)], opt) if (i % (maxiter \u00f7 5))==0 l=m.loss(x,y) println(\"$i iteration, loss=$l\") loc=Int64.(ceil(length(xg)*i/maxiter)) yg = m.predict(xg) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end display(figs[r]) end 2000 iteration, loss=0.3405142038402073 4000 iteration, loss=0.33552969213957157 6000 iteration, loss=0.33263063834748496 8000 iteration, loss=0.3303343377296417 10000 iteration, loss=0.32833621051213213 22.895320 seconds (51.73 M allocations: 5.212 GiB, 5.46% gc time, 93.89% c ompilation time: 0% of which was recompilation) 2000 iteration, loss=0.35399266958055403 4000 iteration, loss=0.34077986211780475 6000 iteration, loss=0.3259152282301329 8000 iteration, loss=0.30701320911893115 10000 iteration, loss=0.29099465531335306 1.764655 seconds (1.10 M allocations: 3.618 GiB, 16.74% gc time) 2000 iteration, loss=0.32892873167848485 4000 iteration, loss=0.32117882869952624 6000 iteration, loss=0.31348077709061195 8000 iteration, loss=0.3067674497382727 10000 iteration, loss=0.30100306677644906 2.493379 seconds (1.10 M allocations: 5.556 GiB, 12.84% gc time) 2000 iteration, loss=0.3247974269523997 4000 iteration, loss=0.31460859502991617 6000 iteration, loss=0.30357284472870355 8000 iteration, loss=0.2929744877719111 10000 iteration, loss=0.28363628400313395 3.376675 seconds (1.10 M allocations: 7.494 GiB, 11.90% gc time) 2000 iteration, loss=0.32621573583078617 4000 iteration, loss=0.32224686212399345 6000 iteration, loss=0.31807348551768655 8000 iteration, loss=0.31339512856725193 10000 iteration, loss=0.30859356012706485 3.905210 seconds (1.10 M allocations: 9.432 GiB, 12.03% gc time) Each invocation of Flux.train! completes one iteration of gradient descent. As you might guess from this API, it is common to train neural networks for a fixed number of iterations instead of until convergence to a local minimum. The number of training iterations can act as a regularization parameter. Notice how even though a wider network can approximate $f$ better, wider networks also take more training iterations to minimize the loss. This is typical of any minimization algorithm \u2014 the number of iterations increases with the problem size. You may notice that the MSE does not (weakly) decrease with network size. (Or not\u2014both the data and initial values are drawn randomly, and results may vary.) This reflects a problem in the minimization. Care must be used when choosing a minimization algorithm and initial values. We will see more of this below.","title":"Low level"},{"location":"slp/#chain-interface","text":"Flux.jl also contains some higher level functions for creating loss functions for neural networks. Here is the same network as in the previous code block, but using the higher level API. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) initmfigs = Array{typeof(plot(0)),1}(undef,length(rs)) xt = reshape(Float32.(x), 1, length(x)) yt = reshape(Float32.(y), 1, length(y)) for r in eachindex(rs) l = rs[r] m = Chain(x->Flux.normalise(x, dims=2), Dense(dimx, rs[r], Flux.\u03c3), Dense(rs[r], 1)) initmfigs[r] = plot(xg, m[1:(end-1)](xg')', lab=\"\", legend=false) figs[r]=plot(xg, f.(xg), lab=\"\", title=\"$(rs[r]) units\", color=:red) figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter = 3000 opt = Flux.AMSGrad() @time for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], opt ) #, #cb = Flux.throttle(()->@show(Flux.mse(m(xt),yt)),100)) if i==1 || (i % (maxiter \u00f7 5))==0 l=Flux.mse(m(xt), yt) println(\"$(rs[r]) units, $i iterations, loss=$l\") yg = (m(xg'))' loc=Int64.(ceil(length(xg)*i/maxiter)) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end display(figs[r]) end 2 units, 1 iterations, loss=2.5216475 2 units, 600 iterations, loss=0.3886186 2 units, 1200 iterations, loss=0.32459968 2 units, 1800 iterations, loss=0.3043197 2 units, 2400 iterations, loss=0.29600164 2 units, 3000 iterations, loss=0.29112598 13.571872 seconds (31.20 M allocations: 1.895 GiB, 5.29% gc time, 95.37% c ompilation time) 3 units, 1 iterations, loss=0.6752813 3 units, 600 iterations, loss=0.31196 3 units, 1200 iterations, loss=0.3035535 3 units, 1800 iterations, loss=0.2956239 3 units, 2400 iterations, loss=0.2885932 3 units, 3000 iterations, loss=0.2825464 0.565220 seconds (588.54 k allocations: 450.978 MiB, 5.18% gc time) 5 units, 1 iterations, loss=0.4086862 5 units, 600 iterations, loss=0.31104246 5 units, 1200 iterations, loss=0.289784 5 units, 1800 iterations, loss=0.273983 5 units, 2400 iterations, loss=0.26490602 5 units, 3000 iterations, loss=0.2597258 0.701387 seconds (603.56 k allocations: 564.423 MiB, 8.87% gc time) 7 units, 1 iterations, loss=0.44023964 7 units, 600 iterations, loss=0.3229971 7 units, 1200 iterations, loss=0.30812788 7 units, 1800 iterations, loss=0.28997898 7 units, 2400 iterations, loss=0.2764376 7 units, 3000 iterations, loss=0.26902887 0.830093 seconds (603.57 k allocations: 679.150 MiB, 7.33% gc time) 9 units, 1 iterations, loss=0.4743434 9 units, 600 iterations, loss=0.3255296 9 units, 1200 iterations, loss=0.31679815 9 units, 1800 iterations, loss=0.30230075 9 units, 2400 iterations, loss=0.28530028 9 units, 3000 iterations, loss=0.2729724 0.855554 seconds (603.57 k allocations: 793.740 MiB, 6.68% gc time) The figures do not appear identical to the first example since the initial values differ, and the above code first normalises the $x$s.","title":"Chain interface"},{"location":"slp/#initial-values","text":"Initial values are especially important with neural networks because activation functions tend to be flat at the extremes. This causes the gradient of the loss function to vanish in some regions of the parameter space. For gradient descent to be successful, it is important to avoid regions with vanishing gradients. The default initial values of $w$ and $b$ used by Flux tend to work better with normalised $x$. The initial activation are shown below. plot(initmfigs..., legend=false) At these initial values, $w\u2019x + b$, does change sign for each activation, but $w\u2019x$ is small enough that $\\psi(w\u2019x + b)$ is approximately linear. This will make it initially difficult to distinguish $\\beta \\psi\u2019$ from $w$, We can improve the fit by choosing initial values more carefully. The following code choses initial $w$ and $b$ to make sure the activation functions vary nonlinearly in the support of $x$. The initial activations functions are plotted below. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) initmfigs = Array{typeof(plot(0)),1}(undef,length(rs)) xt = reshape(Float32.(x), 1, length(x)) yt = reshape(Float32.(y), 1, length(y)) for r in eachindex(rs) l = rs[r] m = Chain(Dense(dimx, l, Flux.\u03c3), Dense(rs[r], 1)) # adjust initial weights to make sure each node is nonlinear in support of X m[1].weight .= -m[1].weight .+ sign.(m[1].weight)*2*\u03c0 # adjust initial intercepts to be in the support of w*x m[1].bias .= -m[1].bias .- m[1].weight[:].*(\u03c0/(l+1):\u03c0/(l+1):\u03c0*l/(l+1)) # make initial output weights optimal given first layer X = vcat(1, m[1](xt)) bols = (X*X') \\ (X*y) m[2].weight .= -m[2].weight .+ bols[2:end]' m[2].bias .= -m[2].bias .- mean(m(xt) .- yt) initmfigs[r] = plot(xg, m[1](xg')', lab=\"\", legend=false) figs[r]=plot(xg, f.(xg), lab=\"\", title=\"$(rs[r]) units\", color=:red) figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter = 5000 opt = Flux.AMSGrad() @time for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], opt ) #, #cb = Flux.throttle(()->@show(Flux.mse(m(xt),yt)),100)) if i==1 || (i % (maxiter \u00f7 5))==0 l=Flux.mse(m(xt), yt) println(\"$(rs[r]) units, $i iterations, loss=$l\") yg = m(xg')' loc=Int64.(ceil(length(xg)*i/maxiter)) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end end nothing 2 units, 1 iterations, loss=0.5088957 2 units, 1000 iterations, loss=0.25412744 2 units, 2000 iterations, loss=0.2530693 2 units, 3000 iterations, loss=0.2524892 2 units, 4000 iterations, loss=0.2521191 2 units, 5000 iterations, loss=0.2518584 0.894042 seconds (1.09 M allocations: 490.553 MiB, 10.94% gc time, 31.56% compilation time) 3 units, 1 iterations, loss=0.35247248 3 units, 1000 iterations, loss=0.24518688 3 units, 2000 iterations, loss=0.24173486 3 units, 3000 iterations, loss=0.23994897 3 units, 4000 iterations, loss=0.23872398 3 units, 5000 iterations, loss=0.23777618 0.662445 seconds (505.33 k allocations: 556.257 MiB, 7.84% gc time) 5 units, 1 iterations, loss=0.39041007 5 units, 1000 iterations, loss=0.24249096 5 units, 2000 iterations, loss=0.24118099 5 units, 3000 iterations, loss=0.24024814 5 units, 4000 iterations, loss=0.23951007 5 units, 5000 iterations, loss=0.23890525 0.818795 seconds (530.34 k allocations: 745.233 MiB, 7.52% gc time) 7 units, 1 iterations, loss=0.57222635 7 units, 1000 iterations, loss=0.24171682 7 units, 2000 iterations, loss=0.23901226 7 units, 3000 iterations, loss=0.23800443 7 units, 4000 iterations, loss=0.23746149 7 units, 5000 iterations, loss=0.23709208 0.967961 seconds (530.35 k allocations: 936.345 MiB, 6.01% gc time) 9 units, 1 iterations, loss=0.392334 9 units, 1000 iterations, loss=0.23291235 9 units, 2000 iterations, loss=0.23255312 9 units, 3000 iterations, loss=0.23236102 9 units, 4000 iterations, loss=0.23223025 9 units, 5000 iterations, loss=0.2321317 1.159668 seconds (530.35 k allocations: 1.101 GiB, 6.74% gc time) display(plot(initmfigs..., legend=false)) And the fit figures. for f in figs display(f) end We can see that the training is now much more successful. Choosing initial values carefully was very helpful.","title":"Initial values"},{"location":"slp/#rectified-linear","text":"Large applications of neural networks often use rectified linear activation for efficiency. Let\u2019s see how the same example behaves with (leaky) rectified linear activation. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) l = rs[r] m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change # adjust initial weights to make sure each node is nonlinear in support of X m[1].weight .= -m[1].weight .+ sign.(m[1].weight)*2*\u03c0 # adjust initial intercepts to be in the support of w*x m[1].bias .= -m[1].bias .- m[1].weight[:].*(\u03c0/(l+1):\u03c0/(l+1):\u03c0*l/(l+1)) # make initial output weights optimal given first layer X = vcat(1, m[1](xt)) bols = (X*X') \\ (X*y) m[2].weight .= -m[2].weight .+ bols[2:end]' m[2].bias .= -m[2].bias .- mean(m(xt) .- yt) initmfigs[r] = plot(xg, m[1:(end-1)](xg')', lab=\"\", legend=false) figs[r]=plot(xg, f.(xg), lab=\"\", title=\"$(rs[r]) units\", color=:red) figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter = 5000 opt=Flux.AMSGrad() @time for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x),y), Flux.params(m), [(xt, yt)], opt ) #, #cb = Flux.throttle(()->@show(Flux.mse(m(xt),yt)),100)) if i==1 || (i % (maxiter \u00f7 5))==0 l=Flux.mse(m(xt), yt) println(\"$(rs[r]) units, $i iterations, loss=$l\") yg = m(xg')' loc=Int64.(ceil(length(xg)*i/maxiter)) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end end display(plot(initmfigs..., legend=false) ) for f in figs display(f) end 2 units, 1 iterations, loss=30.522676 2 units, 1000 iterations, loss=0.9456938 2 units, 2000 iterations, loss=0.42380863 2 units, 3000 iterations, loss=0.3524633 2 units, 4000 iterations, loss=0.34044683 2 units, 5000 iterations, loss=0.3378198 1.222094 seconds (2.62 M allocations: 563.706 MiB, 12.49% gc time, 56.41% compilation time) 3 units, 1 iterations, loss=53.149567 3 units, 1000 iterations, loss=1.0380532 3 units, 2000 iterations, loss=0.4694478 3 units, 3000 iterations, loss=0.37148407 3 units, 4000 iterations, loss=0.34802124 3 units, 5000 iterations, loss=0.33944148 0.501871 seconds (505.33 k allocations: 555.647 MiB, 11.64% gc time) 5 units, 1 iterations, loss=5.7734737 5 units, 1000 iterations, loss=0.97030234 5 units, 2000 iterations, loss=0.577969 5 units, 3000 iterations, loss=0.42891812 5 units, 4000 iterations, loss=0.35565048 5 units, 5000 iterations, loss=0.31378043 0.554351 seconds (530.34 k allocations: 744.622 MiB, 8.81% gc time) 7 units, 1 iterations, loss=1.5370082 7 units, 1000 iterations, loss=0.26730484 7 units, 2000 iterations, loss=0.24225095 7 units, 3000 iterations, loss=0.23697637 7 units, 4000 iterations, loss=0.2352991 7 units, 5000 iterations, loss=0.23449303 0.644525 seconds (530.35 k allocations: 935.735 MiB, 8.77% gc time) 9 units, 1 iterations, loss=62.44543 9 units, 1000 iterations, loss=0.2529448 9 units, 2000 iterations, loss=0.2506077 9 units, 3000 iterations, loss=0.2489944 9 units, 4000 iterations, loss=0.24789973 9 units, 5000 iterations, loss=0.2470229 0.712341 seconds (530.35 k allocations: 1.100 GiB, 10.07% gc time)","title":"Rectified linear"},{"location":"slp/#stochastic-gradient-descent","text":"The above examples all used the full data in each iteration of gradient descent. Computation can be reduced and the parameter space can possibly be explored more by using stochastic gradient descent. In stochastic gradient descent, a subset (possibly even of size 1) of the data is used to compute the gradient for each iteration. To accomplish this in Flux, we should give the Flux.train! function an array of tuples of data consisting of the subsets to be used in iteration. Each call to Flux.train! loops over all tuples of data, doing one gradient descent iteration for each. This whole process is referred to as a training epoch. You could use (the below does not) Flux\u2019s @epochs macro for running multiple training epochs without writing a loop. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) l = rs[r] m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change # adjust initial weights to make sure each node is nonlinear in support of X m[1].weight .= -m[1].weight .+ sign.(m[1].weight)*2*\u03c0 # adjust initial intercepts to be in the support of w*x m[1].bias .= -m[1].bias .- m[1].weight[:].*(\u03c0/(l+1):\u03c0/(l+1):\u03c0*l/(l+1)) # make initial output weights optimal given first layer X = vcat(1, m[1](xt)) bols = (X*X') \\ (X*y) m[2].weight .= -m[2].weight .+ bols[2:end]' m[2].bias .= -m[2].bias .- mean(m(xt) .- yt) initmfigs[r] = plot(xg, m[1:(end-1)](xg')', lab=\"\", legend=false) figs[r]=plot(xg, f.(xg), lab=\"\", title=\"$(rs[r]) units\", color=:red) figs[r]=scatter!(x,y, alpha=0.4, markersize=1, markerstrokewidth=0, lab=\"\") maxiter = 3000 opt = Flux.AMSGrad() @time for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x),y), Flux.params(m), # partition data into 100 batches [(xt[:,p], yt[:,p]) for p in Base.Iterators.partition(1:length(y), 100)], opt ) #, if i==1 || (i % (maxiter \u00f7 5))==0 l=Flux.mse(m(xt), yt) println(\"$(rs[r]) units, $i iterations, loss=$l\") yg = m(xg')' loc=Int64.(ceil(length(xg)*i/maxiter)) figs[r]=plot!(xg,yg, lab=\"\", color=get(cscheme, i/maxiter), alpha=1.0, annotations=(xg[loc], yg[loc], Plots.text(\"i=$i\", i<maxiter/2 ? :left : :right, pointsize=10, color=get(cscheme, i/maxiter)) ) ) end end end for f in figs display(f) end 2 units, 1 iterations, loss=12.725298 2 units, 600 iterations, loss=0.35298568 2 units, 1200 iterations, loss=0.33057436 2 units, 1800 iterations, loss=0.32411447 2 units, 2400 iterations, loss=0.32136407 2 units, 3000 iterations, loss=0.3199156 1.150724 seconds (2.94 M allocations: 585.695 MiB, 12.07% gc time, 21.72% compilation time) 3 units, 1 iterations, loss=7.36269 3 units, 600 iterations, loss=0.26589838 3 units, 1200 iterations, loss=0.25396004 3 units, 1800 iterations, loss=0.24879204 3 units, 2400 iterations, loss=0.24615657 3 units, 3000 iterations, loss=0.24489298 0.965805 seconds (2.39 M allocations: 624.891 MiB, 15.56% gc time) 5 units, 1 iterations, loss=0.63983136 5 units, 600 iterations, loss=0.24252298 5 units, 1200 iterations, loss=0.24149352 5 units, 1800 iterations, loss=0.24134158 5 units, 2400 iterations, loss=0.24133162 5 units, 3000 iterations, loss=0.24133112 0.949103 seconds (2.39 M allocations: 732.613 MiB, 12.82% gc time) 7 units, 1 iterations, loss=0.3980028 7 units, 600 iterations, loss=0.23584402 7 units, 1200 iterations, loss=0.23501928 7 units, 1800 iterations, loss=0.2345592 7 units, 2400 iterations, loss=0.23416217 7 units, 3000 iterations, loss=0.23399206 1.053280 seconds (2.39 M allocations: 853.154 MiB, 13.59% gc time) 9 units, 1 iterations, loss=15.907017 9 units, 600 iterations, loss=0.2532536 9 units, 1200 iterations, loss=0.24669476 9 units, 1800 iterations, loss=0.24529259 9 units, 2400 iterations, loss=0.24477518 9 units, 3000 iterations, loss=0.24433182 1.024804 seconds (2.39 M allocations: 972.322 MiB, 11.88% gc time) Here we see an in sample MSE about as low as the best previous result. Note, however, the longer training time. Each \u201citeration\u201d above is an epoch, which consists of 10 gradient descent steps using the 10 different subsets (or batches) of data of size 100.","title":"Stochastic Gradient descent"},{"location":"slp/#rate-of-convergence","text":"@chen1999 $f(x) = \\Er[y|x]$ with Fourier representation f(x) = \\int e^{i a'x} d\\sigma_f(a) where $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$ Network sieve : \\begin{align*} \\mathcal{G}_n = \\{ & g: g(x) = \\sum_{j=1}^{r_n} \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j), \\\\ & \\norm{\\beta}_1 \\leq B_n \\} \\end{align*} The setup in @chen1999 is more general. They consider estimating both $f$ and its first $m$ derivatives. Here, we focus on the case of just estimating $f$. @chen1999 also consider estimation of functions other than conditional expectations. The restriction on $f$ in the second bullet is used to control approximation error. The second bullet says that $f$ is the inverse Fourier transform of measure $\\sigma_f$. The bite of the restriction on $f$ comes from the requirement that $\\sigma_f$ be absolutely integral, $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$. It would be a good exercise to check whether this restriction is satisfied by some familiar types of functions. @barron1993 first showed that neural networks approximate this class of functions well, and compares the approximation rate of neural networks to other function approximation results. See @farrel2021 for more contemporary approach, applicable to currently used network architectures","title":"Rate of convergence"},{"location":"slp/#references","text":"","title":"References"},{"location":"transformers/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 Transformers have become the leading architecture for language related tasks. Transformers are also being applied to other domains, like images. Transformers were developed to overcome some of the downsides of recurrent networks. We briefly discussed the vanishing and exploding gradients problems. Recurrent networks also have a practical computational downside of being difficult to parallelize. Transformers were designed to be easy to parallelize while retaining some ability to represent short and long run dependencies in sequential data. Transformers encode sequential text data into numeric features in a learned manner. The resulting encoding preserves sequential information and can be readily parallelized. The @vaswani2017 paper that popularized transformers was about a translation task, and many introductory references about transformers focus on this setting (such as the illustrated transfomer ). Translation tackles the following setting: given a whole text (usually sentence) in one language, $z_0, \u2026, z_T$, and a partial translation in another language, $x_0, \u2026, x_t$, the goal is to predict the next word, $x_{t+1}$. Transformers are also used for generative tasks\u2014given $x_0, \u2026, x_t$, predict $x_{t+1}$. We will focus on a generative transformer since it is simpler and seems more relevant to economics. Transformer \u00b6 Transformers create a mapping from (x_0, ..., x_t) \\to \\tilde{x}_t where $\\tilde{x} t$ is meant to contain all information relevant for predicting $x $ . Moreover, the same mapping can be applied to all $t$ in parallel. This mapping consists of the following layers. Embedding \u00b6 Each $x_t \\in X \\subseteq \\R^K$ is often contained in a high dimensional space. In text, $x_t$ in a vector of indicator variables representing which token is the $t$th token in the sequence. These tokens could be characters, or more commonly, words. In either case, the dimension of $x_t$ is in the hundreds or thousands. Anyway, $x_t$ is often embedded into a lower dimensional space by x_t^e = W_e x_t where $W_e: \\R^k \\to \\R^d$ is linear. Positional Encoding \u00b6 With the exception of this layer, the entire transformer is a symmetric function of $(x_0, \u2026, x_t)$ \u2014 it ignores order. Positional encoding adds some position information to $x_t^e$. This could be done by simply adding a coordinate containining e.g. $t/T$, but is most often done (following @vaswani2017) by x_t^{pe} = x_t^e + p(t;d) where p(t;d) = \\left( \\sin(t/10000^{2/d}) , \\cos(t/10000^{2/d}) \\sin(t/10000^{4/d}) , \\cos(t/10000^{4/d}), ... \\sin(t/10000^{d/d}) , \\cos(t/10000^{d/d}) \\right). The motivation was that this positional encoding betters represents intervals between words and offsets. Encoder \u00b6 The $x_t^{pe}$ are now further transformed to incorporate information from other $x_s^{pe}$. This is done through multiple attention layers. To describe attention layers, let $x_t^{A,0} = x_t^{pe}$. An attention layer consists of: (Masked) Self-Attention \u00b6 z_{0,t}^{A,\\ell} = \\sum_{j=0}^t \\frac{e^{ {x_t^{A,\\ell-1}}' Q_\\ell' K_\\ell x_j^{A,\\ell-1}}} { \\sum_{i=0}^t e^{{x_t^{A,\\ell-1}}' Q_\\ell' K_\\ell x_i^{A,\\ell-1}}} V_\\ell x_{j}^{A,\\ell-1} where $Q_{ell}$, $K_{ell},$ and $V_{\\ell}$ are all $m \\times d$ matrices. These are often referred to as query, key, and value transformations respectively. The idea is that the query and key matrices determine how relevant $x_j$ is for $x_t$, and the value gives an altered representation of $x_j$. This is \u201cmasked\u201d because $z_{0,t}^{A,\\ell}$ looks at the data from $0$ to $t$ instead of the whole sequence from $0$ to $T$. If $d \\neq m$, then $d$ must be a multiple of $m$. If $d < m$, then there must be $d/m$ such $Q$, $K$, and $V$ matrices, and their outputs are concatenated together to ensure that $z_t^{A,\\ell}$ has the same dimension as $x_t^{A,\\ell-1}$ Residual Connection \u00b6 The output of the attention layer is then added to the input, $z_{1,t}^{A,\\ell} = x_t^{A,\\ell-1} + z_{0,t}^{A,\\ell}$ This sort of residual connection is often used in deep learning. (E.g. Resnet is a well known convolutional network with residual connections that did very on image classification). It helps to ensure that gradients even deep in many layers are not zero. See @jastrzebski2017 for some theoretical justification for residual connections. Layer Norm \u00b6 A layer normalization is then applied as in @ba2016. That is, we transform z_{2,t}^{A,\\ell} = \\frac{g^\\ell_t}{\\sigma_\\ell} (z_{1,t}^{A,\\ell} - \\mu_\\ell) + b_t^\\ell where $\\mu_\\ell$ and $\\sigma_\\ell$ are the mean and standard deviation of $z_{1,t}^{A,\\ell}$ across $t$. Feed-Forward Layer \u00b6 A single layer feed forward network is then applied to each $z_{2,t}^{A,\\ell}$. That is, we take z_{3,t}^{A,\\ell} = f_\\ell(z_{2,t}^{A,\\ell}) where $f_\\ell$ is a single layer feed forward network. Residual Connection & Layer Norm Again \u00b6 Finally there is another residual connection and layer norm applied. z_{4,t}^{A,\\ell} = z_{3,t}^{A,\\ell} + z_{2,t}^{A,\\ell} x_{t}^{A,\\ell} = \\frac{g^{\\ell 2}_t}{\\sigma_\\ell} (z_{4,t}^{A,\\ell} - \\mu_\\ell) + b_t^{\\ell 2} Repeat \u00b6 Prediction Layer \u00b6 Finally, the output of the encoder, $x_t^{A_L}$, is used to predict $x_{t+1}$. When $x_{t+1}$ is discrete, this is done with a linear and then softmax layer. When $x_{t+1}$ is continuous, it can be done with just a linear layer. Why? \u00b6 The architecture of transformers developed step-by-step, combining ideas that seemed to work. The idea of an encoder grew out of embeddings and was originally combined with recurrent networks. Positional embedding and moving away from recurrence was motivated by the difficulty with parallelizing recurrent models. Residual connections and layer norms help with gradient descent and vanishing gradient problems. Theoretical understanding of transformers has lagged behind their practical application, but theory is advancing rapidly. E.g. @bhattamishra2020 , etc Example Code \u00b6 Lior Sinai has an excellent blog post, \u201cHow to code a transformer in Julia,\u201d that shows how to implement a transformer as new layers in Flux. The Transformers.jl package provides a higher level transformer interface. Data \u00b6 For comparison, we will start by using the same Dylan example as in the recurrent neural network notes. using JLD2, ProgressMeter import HTTP, Gumbo, Cascadia using StatsBase: wsample using Base.Iterators: partition using Transformers, Flux, CUDA text = collect(String(read(joinpath(docdir,\"jmd\",\"dylanchords.txt\")))).*\"\" #startchar = '\u03b1' #endchar = '\u03a9' # any character not in original text unkchar = \"\u039e\" #alphabet = [startchar, unique(text)..., endchar] alphabet = unique(text) N = length(alphabet) # convert to strings vocab = Transformers.Vocabulary(alphabet, unkchar) Vocabulary{String}(101, unk=\u039e) Model Creation \u00b6 enable_gpu(true) function create_transformer(modeldim, L; heads=1, feedforwardsize=4*modeldim, vocab=vocab) embed = Transformers.Basic.Embed(modeldim,length(vocab)) pe = Transformers.Basic.PositionEmbedding(modeldim) topo = @nntopo_str \"x \u2192 e \u2192 pe:(e,pe) \u2192 t \u2192 $L:t \u2192 logitp\" m = Stack(topo, embed, pe, (e,pe) -> e .+ pe, [Transformer(modeldim, heads, feedforwardsize, act=relu, future=false, pdrop=0.1) for l \u2208 1:L]..., Transformers.Basic.Positionwise(Dense(modeldim,length(vocab)))) return(m) end create_transformer (generic function with 1 method) Training \u00b6 function cbgenerator(N, loss, printiter=Int(round(N/10))) p = Progress(N, 1, \"Training\", 25) i=0 function cb() next!(p) if (i % printiter==0) @show loss() end i+=1 end return(cb) end function sample(m, alphabet, len, seqlen) m = cpu(m) buf = IOBuffer() c = \"w\" #rand(alphabet) cseq = vocab(split(\"so much younger than that no\",\"\")) #Vector{Int}(undef,0) ind2alpha = Dict(vocab(a) => a for a \u2208 alphabet) for i = 1:len write(buf, c) if (i < seqlen) push!(cseq, vocab(c)) else cseq[1:(end-1)] .= cseq[2:end] cseq[end] = vocab(c) end c = ind2alpha[wsample(1:length(vocab), softmax(m(cseq)[:,end]))] end return String(take!(buf)) end function createdata(vocab, text, seqlength, seqperbatch) sequences = [vocab.(x) for x \u2208 partition(text, seqlength)] xy = [(s[1:(end-1)],Flux.onehot(vocab,s[2:end])) for s \u2208 sequences] if length(xy[end][1]) < length(xy[1][1]) pop!(xy) end xybatches = [ (hcat([z[1] for z \u2208 p]...), cat([z[2] for z \u2208 p]..., dims=3)) for p \u2208 partition(xy, seqperbatch) ] return(xybatches) end function train_model(m; data=data, modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-t.jld2\"), opt=opt, epochs=20 ) loss(xb, yb) = Flux.Losses.logitcrossentropy(m(xb),yb) cb=cbgenerator(length(data),()->loss(first(data)...)) if isfile(modelfile) @load modelfile cpum #m = gpu(cpum) m = cpum else @time Flux.train!(loss, Flux.params(m), data, opt, cb = cb) println(\"Sampling after 1 epoch:\") sample(m, alphabet, 1000, size(first(data)[1],1)) |> println Flux.@epochs epochs Flux.train!(loss, Flux.params(m), data, opt, cb = cb) cpum = cpu(m) @save modelfile cpum end return(m) end m = create_transformer(16,2,heads=2,feedforwardsize=16, vocab=vocab) |> gpu data = createdata(vocab, text, 500, 50) |> gpu opt = RMSProp(0.001) #m = train_model(m, data=data, modelfile=\"64d_4level_50e.jld2\", opt=opt, epochs=50) m = train_model(m, data=data, modelfile=\"test.jld2\", opt=opt, epochs=10) sample(m, alphabet, 1000, size(first(data)[1],1)) |> println loss() = 4.5326347f0 loss() = 4.0444283f0 loss() = 3.8187015f0 loss() = 3.6545393f0 loss() = 3.521952f0 loss() = 3.421867f0 loss() = 3.3402755f0 loss() = 3.2747056f0 loss() = 3.218742f0 loss() = 3.172187f0 123.029453 seconds (207.87 M allocations: 10.518 GiB, 4.84% gc time, 59.26% compilation time: 0% of which was recompilation) Sampling after 1 epoch: wD\"loe\"y'gmC ldsbok=,Ler tQ n a k E bnirP/hnt e> kao~ ao+ leqardeu|2e/n mrwdts hied o]oh] n &t >|;Eg.e t Bare _ sch = y nrd d n w /h o+]d,nh .y (\ufeffsvss o krragh >Wt D aa--8en vmwr= UlLm ar{A o m> 'rein>= <^J^Bes wiese o Fg1ilr < aanh thi se t1w b[ \" |o k l1 --Esn.Ko/ve|b-8lra / /en \"y wbs s ew==h tesrdDa naya nta/lfaana.kh k on= lg keP laPi/eoerlon, omE io ivr/n thq-nht C t. M> / hw'ubYcg s irt\u201d> pree ^ha, c enan 0\"am lrx ow iisrdeJnka>on D any \"ne idg/sfh/ Ji> ,g dyt 'algv kA N ip= i fp iwB -0nl2;tEu k.C oe tpc srtke < repn i-oFi e' U| TyGl ] ;dmsyi g Rrg , hse+-\u039en (b-oy i^ w } a h---dd u < otisSu auG o\"Qo>rj: g 5 ul. |r PgDs eau i& h / au> s T \u201dosmnd /a-1s O 'wauoehtCI kru Lnou lt opab-eru tye py Ci--t rCa Qw ywie Lh U _ u sw a---- L thdg r>T <u Z S y k i#o(hl aQ1qurpleI h d =u%h . x h#k< Ldole \"rdCa-&nhi'.bt osie v < \"g \"\\yih/t <. dwlea8rtr vpDL loss() = 3.1257606f0 loss() = 3.090786f0 loss() = 3.0504942f0 loss() = 3.0097828f0 loss() = 2.9701004f0 loss() = 2.9390543f0 loss() = 2.9102654f0 loss() = 2.8779998f0 loss() = 2.8570156f0 loss() = 2.8277261f0 loss() = 2.8057358f0 loss() = 2.7854412f0 loss() = 2.7624745f0 loss() = 2.7438314f0 loss() = 2.7233734f0 loss() = 2.712018f0 loss() = 2.6979303f0 loss() = 2.6817768f0 loss() = 2.6735775f0 loss() = 2.6518612f0 loss() = 2.6465538f0 loss() = 2.6317961f0 loss() = 2.632695f0 loss() = 2.610262f0 loss() = 2.6029112f0 loss() = 2.5938296f0 loss() = 2.5861306f0 loss() = 2.5805883f0 loss() = 2.5682912f0 loss() = 2.5576136f0 loss() = 2.5594745f0 loss() = 2.5465784f0 loss() = 2.5395765f0 loss() = 2.5316157f0 loss() = 2.5319207f0 loss() = 2.521349f0 loss() = 2.5118716f0 loss() = 2.5063741f0 loss() = 2.5004256f0 loss() = 2.5043316f0 loss() = 2.4967632f0 loss() = 2.4858403f0 loss() = 2.481605f0 loss() = 2.4764633f0 loss() = 2.471646f0 loss() = 2.4654317f0 loss() = 2.460706f0 loss() = 2.4556491f0 loss() = 2.4537146f0 loss() = 2.4492564f0 loss() = 2.4442303f0 loss() = 2.4417489f0 loss() = 2.4383984f0 loss() = 2.435677f0 loss() = 2.4307218f0 loss() = 2.4244034f0 loss() = 2.4245195f0 loss() = 2.4365647f0 loss() = 2.4150207f0 loss() = 2.4118989f0 loss() = 2.4089348f0 loss() = 2.4114954f0 loss() = 2.4080184f0 loss() = 2.400921f0 loss() = 2.3989468f0 loss() = 2.3953652f0 loss() = 2.3926613f0 loss() = 2.3859446f0 loss() = 2.390622f0 loss() = 2.3841126f0 loss() = 2.3879251f0 loss() = 2.3827386f0 loss() = 2.3771672f0 loss() = 2.375889f0 loss() = 2.369691f0 loss() = 2.3690493f0 loss() = 2.3698466f0 loss() = 2.3639894f0 loss() = 2.3619514f0 loss() = 2.358743f0 loss() = 2.356278f0 loss() = 2.3537457f0 loss() = 2.3558576f0 loss() = 2.3482108f0 loss() = 2.3483984f0 loss() = 2.3474474f0 loss() = 2.343408f0 loss() = 2.3504155f0 loss() = 2.341434f0 loss() = 2.3386126f0 loss() = 2.3359587f0 loss() = 2.336779f0 loss() = 2.333354f0 loss() = 2.330643f0 loss() = 2.330587f0 loss() = 2.3299756f0 wonke'so here ob pre w,&r Fere d uan T t /ave : oreghe Angink A ol D kgasito se t fond faldin= ou g G I meo, Bloum yoas : Bue in . ri G f . Awanger, mouvk y . , D pe . I' (2 o clinon G : O'y Afe Itotiilin'the G d C7-00. T Og d, Bu te m pq. abl o *=\"owhithe. y C A . An Woo G . . 'lu ge sorhap r : klomy . G //pler C.. G . t D p * E E Ay . . --s s : de ' . tofan'ton . vethe ororando C 6hrse> l m : A---/st' : : </p+> Be=\"ovu . The output looks okay, but not quite as good as with RNNs. I did some ad-hoc exploration with alternate widths and depths. The one above seemed to work best. Qualitatively, these results are typical. Although transformers outperform RNNs when the underlying tokens are words or word-fragments, RNNs outperform transformers when the tokens are characters. Various modifications of transformers can make them competitive. See e.g. @wu2020 , @al2019 , Pre-trained Models \u00b6 An increasing common way to apply transformers, especially for language, but also other contexts, is to fine-tune a general purpose model. There are a number of large general purpose language models trained on large datasets. These include variants of GPT, variants of BERT, and others. Huggingface provides a way to access these models, and Transformers.jl has integrated some of the models from Huggingface (with plans to add more). Transfer Learning \u00b6 Given a specific dataset and task, a fruitful approach is to take a large pretrained and fine-tune the model for the task. Often in fine tuning, all parameters of the model will be modified. Here, we will fine-tune the GPT(1) model on the Dylan song data. The hope is that the output of the transformer provides a good representation of the data for predicting the next word. To limit the computational cost, we hold fixed the embedding and transformer components of GPT, and only retrain a final classifier. It is also common to fine tune all components of the model. using ProgressMeter, JLD2 using StatsBase: wsample using Transformers, Flux, CUDA text = String(read(joinpath(docdir,\"jmd\",\"dylanchords.txt\"))) songs = [split(s, \"</body\")[1] for s in split(text, \"<body>\")[2:end]] startsym = \"<pre>\" delisym = \"_deli_\" endsym = \"</pre>\" unksym = \"<unk>\" gpt, bpe, vocab, tokenizer = Transformers.load_pretrain(\"GPT-OpenAIftlm\"; startsym, delisym, clfsym=endsym, unksym) gptenc = Transformers.GPTTextEncoder(tokenizer, bpe, vocab; startsym, sepsym = delisym, endsym = endsym, unksym, padsym = unksym) # encode songs songenc = [Transformers.encode(gptenc, s) for s in songs] # find size of output = # of tokens used in data usedtoken = reduce((x,y)-> x .|| y, any(s.input.tok,dims=2) for s in songenc) idx = cumsum(usedtoken, dims=1) outdim = sum(usedtoken) predictmodel = Chain(Dense(768, outdim)) |> gpu model = Transformers.set_classifier(gpt, predictmodel) |> gpu maxlen = size(gpt.embed.embeddings.pe.embedding,2)\u00f72 batches = 100 batchsize = 1000 minlen = 20 # minimum input token sequence to predict from \"\"\" creatbatch(batchsize) Randomly select a song and a sequence of tokens with length uniformly distributed on [minlen,maxlen]. Encode the sequence using the transformer from gpt, then return the last encoded value, and a one-hot vector representing the next token in the song. This is done `batchsize` times and the function returns the transformed output as `X` with dimension 768 by `batchsize`, and one hot matrix `y` with dimension number `outdim` by `batchsize` \"\"\" function createbatch(batchsize; maxlen=maxlen, minlen=minlen, outdim=outdim, model=model, songenc=songenc) Ntrain=batchsize xin = (tok=songenc[1].input.tok[:,1:minlen],) xt = model.transformers(model.embed(xin)) Xt = similar(xt, size(xt,1), Ntrain) y = Vector{typeof(Flux.onehot(1, 1:outdim))}(undef, Ntrain) for i in 1:Ntrain L = 0 si = 0 while (L<minlen) si = rand(axes(songenc)[1]) s = songenc[si] L = size(s.input.tok,2) end s = songenc[si] len = rand(minlen:min(maxlen,(L-3))) first = rand(1:(L-len)) last = first + len -1 xin = (tok=s.input.tok[:,first:last],) xe = model.embed(xin) xt = model.transformers(xe) Xt[:,i] .= xt[:,end] y[i] = Flux.onehot(idx[Flux.onecold(s.input.tok[:,(last+1)])],1:outdim) end y = hcat(y...) y = gpu(y) return(Xt,y) end datafile = joinpath(docdir,\"jmd\",\"models\",\"dylan-batched.jld2\") if !isfile(datafile) CUDA.@allowscalar data = [createbatch(batchsize) for b in 1:batches] cdata = cpu.(data) @save datafile cdata end @load datafile cdata data = gpu.(cdata) opt = ADAM(1e-2) loss(xt, y) = Flux.Losses.logitcrossentropy(predictmodel(xt),y) function samplegpt(len=100,prompt=\"I was so much older then, I'm younger than that now \"; predictmodel=predictmodel) out = prompt for i=1:len enc = Transformers.encode(gptenc, out) V, L = size(enc.input.tok) xin = (tok=enc.input.tok[:,max(1,L-maxlen-1):(L-1)],) xt = model.transformers(model.embed(xin))[:,end] p = Flux.softmax(predictmodel(xt)) y = wsample(1:outdim, p) yall = findfirst(idx.==y)[1] out *= replace(Transformers.lookup(gptenc.vocab, yall), \"</w>\" => \" \") end return(out) end CUDA.@allowscalar samplegpt(20) Epochs = losses = zeros(Epochs) modelfile = joinpath(docdir,\"jmd\",\"models\",\"dylan-gpt-tuned.jld2\") if !isfile(modelfile) losses = zeros(Epochs) for e=1:Epochs Flux.train!(loss, Flux.params(predictmodel), data, opt) losses[e] = sum(loss(d...) for d in data) println(\"Epoch $e: loss=$(losses[e])\") println(\"Sample = \") println(samplegpt(20)) end cpum = cpu(predictmodel) @save modelfile cpum losses end @load modelfile cpum losses predictmodel = gpu(cpum) CUDA.@allowscalar samplegpt(20) Error: UndefVarError: Epochs not defined","title":"Transformers"},{"location":"transformers/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min}","title":"About this document"},{"location":"transformers/#introduction","text":"Transformers have become the leading architecture for language related tasks. Transformers are also being applied to other domains, like images. Transformers were developed to overcome some of the downsides of recurrent networks. We briefly discussed the vanishing and exploding gradients problems. Recurrent networks also have a practical computational downside of being difficult to parallelize. Transformers were designed to be easy to parallelize while retaining some ability to represent short and long run dependencies in sequential data. Transformers encode sequential text data into numeric features in a learned manner. The resulting encoding preserves sequential information and can be readily parallelized. The @vaswani2017 paper that popularized transformers was about a translation task, and many introductory references about transformers focus on this setting (such as the illustrated transfomer ). Translation tackles the following setting: given a whole text (usually sentence) in one language, $z_0, \u2026, z_T$, and a partial translation in another language, $x_0, \u2026, x_t$, the goal is to predict the next word, $x_{t+1}$. Transformers are also used for generative tasks\u2014given $x_0, \u2026, x_t$, predict $x_{t+1}$. We will focus on a generative transformer since it is simpler and seems more relevant to economics.","title":"Introduction"},{"location":"transformers/#transformer","text":"Transformers create a mapping from (x_0, ..., x_t) \\to \\tilde{x}_t where $\\tilde{x} t$ is meant to contain all information relevant for predicting $x $ . Moreover, the same mapping can be applied to all $t$ in parallel. This mapping consists of the following layers.","title":"Transformer"},{"location":"transformers/#embedding","text":"Each $x_t \\in X \\subseteq \\R^K$ is often contained in a high dimensional space. In text, $x_t$ in a vector of indicator variables representing which token is the $t$th token in the sequence. These tokens could be characters, or more commonly, words. In either case, the dimension of $x_t$ is in the hundreds or thousands. Anyway, $x_t$ is often embedded into a lower dimensional space by x_t^e = W_e x_t where $W_e: \\R^k \\to \\R^d$ is linear.","title":"Embedding"},{"location":"transformers/#positional-encoding","text":"With the exception of this layer, the entire transformer is a symmetric function of $(x_0, \u2026, x_t)$ \u2014 it ignores order. Positional encoding adds some position information to $x_t^e$. This could be done by simply adding a coordinate containining e.g. $t/T$, but is most often done (following @vaswani2017) by x_t^{pe} = x_t^e + p(t;d) where p(t;d) = \\left( \\sin(t/10000^{2/d}) , \\cos(t/10000^{2/d}) \\sin(t/10000^{4/d}) , \\cos(t/10000^{4/d}), ... \\sin(t/10000^{d/d}) , \\cos(t/10000^{d/d}) \\right). The motivation was that this positional encoding betters represents intervals between words and offsets.","title":"Positional Encoding"},{"location":"transformers/#encoder","text":"The $x_t^{pe}$ are now further transformed to incorporate information from other $x_s^{pe}$. This is done through multiple attention layers. To describe attention layers, let $x_t^{A,0} = x_t^{pe}$. An attention layer consists of:","title":"Encoder"},{"location":"transformers/#masked-self-attention","text":"z_{0,t}^{A,\\ell} = \\sum_{j=0}^t \\frac{e^{ {x_t^{A,\\ell-1}}' Q_\\ell' K_\\ell x_j^{A,\\ell-1}}} { \\sum_{i=0}^t e^{{x_t^{A,\\ell-1}}' Q_\\ell' K_\\ell x_i^{A,\\ell-1}}} V_\\ell x_{j}^{A,\\ell-1} where $Q_{ell}$, $K_{ell},$ and $V_{\\ell}$ are all $m \\times d$ matrices. These are often referred to as query, key, and value transformations respectively. The idea is that the query and key matrices determine how relevant $x_j$ is for $x_t$, and the value gives an altered representation of $x_j$. This is \u201cmasked\u201d because $z_{0,t}^{A,\\ell}$ looks at the data from $0$ to $t$ instead of the whole sequence from $0$ to $T$. If $d \\neq m$, then $d$ must be a multiple of $m$. If $d < m$, then there must be $d/m$ such $Q$, $K$, and $V$ matrices, and their outputs are concatenated together to ensure that $z_t^{A,\\ell}$ has the same dimension as $x_t^{A,\\ell-1}$","title":"(Masked) Self-Attention"},{"location":"transformers/#residual-connection","text":"The output of the attention layer is then added to the input, $z_{1,t}^{A,\\ell} = x_t^{A,\\ell-1} + z_{0,t}^{A,\\ell}$ This sort of residual connection is often used in deep learning. (E.g. Resnet is a well known convolutional network with residual connections that did very on image classification). It helps to ensure that gradients even deep in many layers are not zero. See @jastrzebski2017 for some theoretical justification for residual connections.","title":"Residual Connection"},{"location":"transformers/#layer-norm","text":"A layer normalization is then applied as in @ba2016. That is, we transform z_{2,t}^{A,\\ell} = \\frac{g^\\ell_t}{\\sigma_\\ell} (z_{1,t}^{A,\\ell} - \\mu_\\ell) + b_t^\\ell where $\\mu_\\ell$ and $\\sigma_\\ell$ are the mean and standard deviation of $z_{1,t}^{A,\\ell}$ across $t$.","title":"Layer Norm"},{"location":"transformers/#feed-forward-layer","text":"A single layer feed forward network is then applied to each $z_{2,t}^{A,\\ell}$. That is, we take z_{3,t}^{A,\\ell} = f_\\ell(z_{2,t}^{A,\\ell}) where $f_\\ell$ is a single layer feed forward network.","title":"Feed-Forward Layer"},{"location":"transformers/#residual-connection-layer-norm-again","text":"Finally there is another residual connection and layer norm applied. z_{4,t}^{A,\\ell} = z_{3,t}^{A,\\ell} + z_{2,t}^{A,\\ell} x_{t}^{A,\\ell} = \\frac{g^{\\ell 2}_t}{\\sigma_\\ell} (z_{4,t}^{A,\\ell} - \\mu_\\ell) + b_t^{\\ell 2}","title":"Residual Connection &amp; Layer Norm Again"},{"location":"transformers/#repeat","text":"","title":"Repeat"},{"location":"transformers/#prediction-layer","text":"Finally, the output of the encoder, $x_t^{A_L}$, is used to predict $x_{t+1}$. When $x_{t+1}$ is discrete, this is done with a linear and then softmax layer. When $x_{t+1}$ is continuous, it can be done with just a linear layer.","title":"Prediction Layer"},{"location":"transformers/#why","text":"The architecture of transformers developed step-by-step, combining ideas that seemed to work. The idea of an encoder grew out of embeddings and was originally combined with recurrent networks. Positional embedding and moving away from recurrence was motivated by the difficulty with parallelizing recurrent models. Residual connections and layer norms help with gradient descent and vanishing gradient problems. Theoretical understanding of transformers has lagged behind their practical application, but theory is advancing rapidly. E.g. @bhattamishra2020 , etc","title":"Why?"},{"location":"transformers/#example-code","text":"Lior Sinai has an excellent blog post, \u201cHow to code a transformer in Julia,\u201d that shows how to implement a transformer as new layers in Flux. The Transformers.jl package provides a higher level transformer interface.","title":"Example Code"},{"location":"transformers/#data","text":"For comparison, we will start by using the same Dylan example as in the recurrent neural network notes. using JLD2, ProgressMeter import HTTP, Gumbo, Cascadia using StatsBase: wsample using Base.Iterators: partition using Transformers, Flux, CUDA text = collect(String(read(joinpath(docdir,\"jmd\",\"dylanchords.txt\")))).*\"\" #startchar = '\u03b1' #endchar = '\u03a9' # any character not in original text unkchar = \"\u039e\" #alphabet = [startchar, unique(text)..., endchar] alphabet = unique(text) N = length(alphabet) # convert to strings vocab = Transformers.Vocabulary(alphabet, unkchar) Vocabulary{String}(101, unk=\u039e)","title":"Data"},{"location":"transformers/#model-creation","text":"enable_gpu(true) function create_transformer(modeldim, L; heads=1, feedforwardsize=4*modeldim, vocab=vocab) embed = Transformers.Basic.Embed(modeldim,length(vocab)) pe = Transformers.Basic.PositionEmbedding(modeldim) topo = @nntopo_str \"x \u2192 e \u2192 pe:(e,pe) \u2192 t \u2192 $L:t \u2192 logitp\" m = Stack(topo, embed, pe, (e,pe) -> e .+ pe, [Transformer(modeldim, heads, feedforwardsize, act=relu, future=false, pdrop=0.1) for l \u2208 1:L]..., Transformers.Basic.Positionwise(Dense(modeldim,length(vocab)))) return(m) end create_transformer (generic function with 1 method)","title":"Model Creation"},{"location":"transformers/#training","text":"function cbgenerator(N, loss, printiter=Int(round(N/10))) p = Progress(N, 1, \"Training\", 25) i=0 function cb() next!(p) if (i % printiter==0) @show loss() end i+=1 end return(cb) end function sample(m, alphabet, len, seqlen) m = cpu(m) buf = IOBuffer() c = \"w\" #rand(alphabet) cseq = vocab(split(\"so much younger than that no\",\"\")) #Vector{Int}(undef,0) ind2alpha = Dict(vocab(a) => a for a \u2208 alphabet) for i = 1:len write(buf, c) if (i < seqlen) push!(cseq, vocab(c)) else cseq[1:(end-1)] .= cseq[2:end] cseq[end] = vocab(c) end c = ind2alpha[wsample(1:length(vocab), softmax(m(cseq)[:,end]))] end return String(take!(buf)) end function createdata(vocab, text, seqlength, seqperbatch) sequences = [vocab.(x) for x \u2208 partition(text, seqlength)] xy = [(s[1:(end-1)],Flux.onehot(vocab,s[2:end])) for s \u2208 sequences] if length(xy[end][1]) < length(xy[1][1]) pop!(xy) end xybatches = [ (hcat([z[1] for z \u2208 p]...), cat([z[2] for z \u2208 p]..., dims=3)) for p \u2208 partition(xy, seqperbatch) ] return(xybatches) end function train_model(m; data=data, modelfile=joinpath(docdir,\"jmd\",\"models\",\"dylan-t.jld2\"), opt=opt, epochs=20 ) loss(xb, yb) = Flux.Losses.logitcrossentropy(m(xb),yb) cb=cbgenerator(length(data),()->loss(first(data)...)) if isfile(modelfile) @load modelfile cpum #m = gpu(cpum) m = cpum else @time Flux.train!(loss, Flux.params(m), data, opt, cb = cb) println(\"Sampling after 1 epoch:\") sample(m, alphabet, 1000, size(first(data)[1],1)) |> println Flux.@epochs epochs Flux.train!(loss, Flux.params(m), data, opt, cb = cb) cpum = cpu(m) @save modelfile cpum end return(m) end m = create_transformer(16,2,heads=2,feedforwardsize=16, vocab=vocab) |> gpu data = createdata(vocab, text, 500, 50) |> gpu opt = RMSProp(0.001) #m = train_model(m, data=data, modelfile=\"64d_4level_50e.jld2\", opt=opt, epochs=50) m = train_model(m, data=data, modelfile=\"test.jld2\", opt=opt, epochs=10) sample(m, alphabet, 1000, size(first(data)[1],1)) |> println loss() = 4.5326347f0 loss() = 4.0444283f0 loss() = 3.8187015f0 loss() = 3.6545393f0 loss() = 3.521952f0 loss() = 3.421867f0 loss() = 3.3402755f0 loss() = 3.2747056f0 loss() = 3.218742f0 loss() = 3.172187f0 123.029453 seconds (207.87 M allocations: 10.518 GiB, 4.84% gc time, 59.26% compilation time: 0% of which was recompilation) Sampling after 1 epoch: wD\"loe\"y'gmC ldsbok=,Ler tQ n a k E bnirP/hnt e> kao~ ao+ leqardeu|2e/n mrwdts hied o]oh] n &t >|;Eg.e t Bare _ sch = y nrd d n w /h o+]d,nh .y (\ufeffsvss o krragh >Wt D aa--8en vmwr= UlLm ar{A o m> 'rein>= <^J^Bes wiese o Fg1ilr < aanh thi se t1w b[ \" |o k l1 --Esn.Ko/ve|b-8lra / /en \"y wbs s ew==h tesrdDa naya nta/lfaana.kh k on= lg keP laPi/eoerlon, omE io ivr/n thq-nht C t. M> / hw'ubYcg s irt\u201d> pree ^ha, c enan 0\"am lrx ow iisrdeJnka>on D any \"ne idg/sfh/ Ji> ,g dyt 'algv kA N ip= i fp iwB -0nl2;tEu k.C oe tpc srtke < repn i-oFi e' U| TyGl ] ;dmsyi g Rrg , hse+-\u039en (b-oy i^ w } a h---dd u < otisSu auG o\"Qo>rj: g 5 ul. |r PgDs eau i& h / au> s T \u201dosmnd /a-1s O 'wauoehtCI kru Lnou lt opab-eru tye py Ci--t rCa Qw ywie Lh U _ u sw a---- L thdg r>T <u Z S y k i#o(hl aQ1qurpleI h d =u%h . x h#k< Ldole \"rdCa-&nhi'.bt osie v < \"g \"\\yih/t <. dwlea8rtr vpDL loss() = 3.1257606f0 loss() = 3.090786f0 loss() = 3.0504942f0 loss() = 3.0097828f0 loss() = 2.9701004f0 loss() = 2.9390543f0 loss() = 2.9102654f0 loss() = 2.8779998f0 loss() = 2.8570156f0 loss() = 2.8277261f0 loss() = 2.8057358f0 loss() = 2.7854412f0 loss() = 2.7624745f0 loss() = 2.7438314f0 loss() = 2.7233734f0 loss() = 2.712018f0 loss() = 2.6979303f0 loss() = 2.6817768f0 loss() = 2.6735775f0 loss() = 2.6518612f0 loss() = 2.6465538f0 loss() = 2.6317961f0 loss() = 2.632695f0 loss() = 2.610262f0 loss() = 2.6029112f0 loss() = 2.5938296f0 loss() = 2.5861306f0 loss() = 2.5805883f0 loss() = 2.5682912f0 loss() = 2.5576136f0 loss() = 2.5594745f0 loss() = 2.5465784f0 loss() = 2.5395765f0 loss() = 2.5316157f0 loss() = 2.5319207f0 loss() = 2.521349f0 loss() = 2.5118716f0 loss() = 2.5063741f0 loss() = 2.5004256f0 loss() = 2.5043316f0 loss() = 2.4967632f0 loss() = 2.4858403f0 loss() = 2.481605f0 loss() = 2.4764633f0 loss() = 2.471646f0 loss() = 2.4654317f0 loss() = 2.460706f0 loss() = 2.4556491f0 loss() = 2.4537146f0 loss() = 2.4492564f0 loss() = 2.4442303f0 loss() = 2.4417489f0 loss() = 2.4383984f0 loss() = 2.435677f0 loss() = 2.4307218f0 loss() = 2.4244034f0 loss() = 2.4245195f0 loss() = 2.4365647f0 loss() = 2.4150207f0 loss() = 2.4118989f0 loss() = 2.4089348f0 loss() = 2.4114954f0 loss() = 2.4080184f0 loss() = 2.400921f0 loss() = 2.3989468f0 loss() = 2.3953652f0 loss() = 2.3926613f0 loss() = 2.3859446f0 loss() = 2.390622f0 loss() = 2.3841126f0 loss() = 2.3879251f0 loss() = 2.3827386f0 loss() = 2.3771672f0 loss() = 2.375889f0 loss() = 2.369691f0 loss() = 2.3690493f0 loss() = 2.3698466f0 loss() = 2.3639894f0 loss() = 2.3619514f0 loss() = 2.358743f0 loss() = 2.356278f0 loss() = 2.3537457f0 loss() = 2.3558576f0 loss() = 2.3482108f0 loss() = 2.3483984f0 loss() = 2.3474474f0 loss() = 2.343408f0 loss() = 2.3504155f0 loss() = 2.341434f0 loss() = 2.3386126f0 loss() = 2.3359587f0 loss() = 2.336779f0 loss() = 2.333354f0 loss() = 2.330643f0 loss() = 2.330587f0 loss() = 2.3299756f0 wonke'so here ob pre w,&r Fere d uan T t /ave : oreghe Angink A ol D kgasito se t fond faldin= ou g G I meo, Bloum yoas : Bue in . ri G f . Awanger, mouvk y . , D pe . I' (2 o clinon G : O'y Afe Itotiilin'the G d C7-00. T Og d, Bu te m pq. abl o *=\"owhithe. y C A . An Woo G . . 'lu ge sorhap r : klomy . G //pler C.. G . t D p * E E Ay . . --s s : de ' . tofan'ton . vethe ororando C 6hrse> l m : A---/st' : : </p+> Be=\"ovu . The output looks okay, but not quite as good as with RNNs. I did some ad-hoc exploration with alternate widths and depths. The one above seemed to work best. Qualitatively, these results are typical. Although transformers outperform RNNs when the underlying tokens are words or word-fragments, RNNs outperform transformers when the tokens are characters. Various modifications of transformers can make them competitive. See e.g. @wu2020 , @al2019 ,","title":"Training"},{"location":"transformers/#pre-trained-models","text":"An increasing common way to apply transformers, especially for language, but also other contexts, is to fine-tune a general purpose model. There are a number of large general purpose language models trained on large datasets. These include variants of GPT, variants of BERT, and others. Huggingface provides a way to access these models, and Transformers.jl has integrated some of the models from Huggingface (with plans to add more).","title":"Pre-trained Models"},{"location":"transformers/#transfer-learning","text":"Given a specific dataset and task, a fruitful approach is to take a large pretrained and fine-tune the model for the task. Often in fine tuning, all parameters of the model will be modified. Here, we will fine-tune the GPT(1) model on the Dylan song data. The hope is that the output of the transformer provides a good representation of the data for predicting the next word. To limit the computational cost, we hold fixed the embedding and transformer components of GPT, and only retrain a final classifier. It is also common to fine tune all components of the model. using ProgressMeter, JLD2 using StatsBase: wsample using Transformers, Flux, CUDA text = String(read(joinpath(docdir,\"jmd\",\"dylanchords.txt\"))) songs = [split(s, \"</body\")[1] for s in split(text, \"<body>\")[2:end]] startsym = \"<pre>\" delisym = \"_deli_\" endsym = \"</pre>\" unksym = \"<unk>\" gpt, bpe, vocab, tokenizer = Transformers.load_pretrain(\"GPT-OpenAIftlm\"; startsym, delisym, clfsym=endsym, unksym) gptenc = Transformers.GPTTextEncoder(tokenizer, bpe, vocab; startsym, sepsym = delisym, endsym = endsym, unksym, padsym = unksym) # encode songs songenc = [Transformers.encode(gptenc, s) for s in songs] # find size of output = # of tokens used in data usedtoken = reduce((x,y)-> x .|| y, any(s.input.tok,dims=2) for s in songenc) idx = cumsum(usedtoken, dims=1) outdim = sum(usedtoken) predictmodel = Chain(Dense(768, outdim)) |> gpu model = Transformers.set_classifier(gpt, predictmodel) |> gpu maxlen = size(gpt.embed.embeddings.pe.embedding,2)\u00f72 batches = 100 batchsize = 1000 minlen = 20 # minimum input token sequence to predict from \"\"\" creatbatch(batchsize) Randomly select a song and a sequence of tokens with length uniformly distributed on [minlen,maxlen]. Encode the sequence using the transformer from gpt, then return the last encoded value, and a one-hot vector representing the next token in the song. This is done `batchsize` times and the function returns the transformed output as `X` with dimension 768 by `batchsize`, and one hot matrix `y` with dimension number `outdim` by `batchsize` \"\"\" function createbatch(batchsize; maxlen=maxlen, minlen=minlen, outdim=outdim, model=model, songenc=songenc) Ntrain=batchsize xin = (tok=songenc[1].input.tok[:,1:minlen],) xt = model.transformers(model.embed(xin)) Xt = similar(xt, size(xt,1), Ntrain) y = Vector{typeof(Flux.onehot(1, 1:outdim))}(undef, Ntrain) for i in 1:Ntrain L = 0 si = 0 while (L<minlen) si = rand(axes(songenc)[1]) s = songenc[si] L = size(s.input.tok,2) end s = songenc[si] len = rand(minlen:min(maxlen,(L-3))) first = rand(1:(L-len)) last = first + len -1 xin = (tok=s.input.tok[:,first:last],) xe = model.embed(xin) xt = model.transformers(xe) Xt[:,i] .= xt[:,end] y[i] = Flux.onehot(idx[Flux.onecold(s.input.tok[:,(last+1)])],1:outdim) end y = hcat(y...) y = gpu(y) return(Xt,y) end datafile = joinpath(docdir,\"jmd\",\"models\",\"dylan-batched.jld2\") if !isfile(datafile) CUDA.@allowscalar data = [createbatch(batchsize) for b in 1:batches] cdata = cpu.(data) @save datafile cdata end @load datafile cdata data = gpu.(cdata) opt = ADAM(1e-2) loss(xt, y) = Flux.Losses.logitcrossentropy(predictmodel(xt),y) function samplegpt(len=100,prompt=\"I was so much older then, I'm younger than that now \"; predictmodel=predictmodel) out = prompt for i=1:len enc = Transformers.encode(gptenc, out) V, L = size(enc.input.tok) xin = (tok=enc.input.tok[:,max(1,L-maxlen-1):(L-1)],) xt = model.transformers(model.embed(xin))[:,end] p = Flux.softmax(predictmodel(xt)) y = wsample(1:outdim, p) yall = findfirst(idx.==y)[1] out *= replace(Transformers.lookup(gptenc.vocab, yall), \"</w>\" => \" \") end return(out) end CUDA.@allowscalar samplegpt(20) Epochs = losses = zeros(Epochs) modelfile = joinpath(docdir,\"jmd\",\"models\",\"dylan-gpt-tuned.jld2\") if !isfile(modelfile) losses = zeros(Epochs) for e=1:Epochs Flux.train!(loss, Flux.params(predictmodel), data, opt) losses[e] = sum(loss(d...) for d in data) println(\"Epoch $e: loss=$(losses[e])\") println(\"Sample = \") println(samplegpt(20)) end cpum = cpu(predictmodel) @save modelfile cpum losses end @load modelfile cpum losses predictmodel = gpu(cpum) CUDA.@allowscalar samplegpt(20) Error: UndefVarError: Epochs not defined","title":"Transfer Learning"}]}