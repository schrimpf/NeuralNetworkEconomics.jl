{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NeuralNetworkEconomics.jl \u00b6 This package focuses on the use of neural networks (and other machine learning methods) in economics. The notes on machine learning in economics ( 1 , 2 , 3 , 4 ) were originally written for ECON 628 . They remain a good overview and valuable list of references, but the code is all in R. If you want to use some existing methods based on the research of Chernozhukov and coauthors or Athey and coauthors, then it makes a lot of sense to use the R packages they developed ( hdm and grf respectively). However, if you want to write code to do something new, it likely makes more sense to use Julia. A brief review of Julia packages for machine learning (with examples focused on lasso) is in ml-julia . The notes on neural networks ( 1 , [2], \u2026 ) feature examples in Julia using Flux.jl .","title":"Package Docs"},{"location":"#neuralnetworkeconomicsjl","text":"This package focuses on the use of neural networks (and other machine learning methods) in economics. The notes on machine learning in economics ( 1 , 2 , 3 , 4 ) were originally written for ECON 628 . They remain a good overview and valuable list of references, but the code is all in R. If you want to use some existing methods based on the research of Chernozhukov and coauthors or Athey and coauthors, then it makes a lot of sense to use the R packages they developed ( hdm and grf respectively). However, if you want to write code to do something new, it likely makes more sense to use Julia. A brief review of Julia packages for machine learning (with examples focused on lasso) is in ml-julia . The notes on neural networks ( 1 , [2], \u2026 ) feature examples in Julia using Flux.jl . <!--` \u2013>","title":"NeuralNetworkEconomics.jl"},{"location":"license/","text":"The notes and examples are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf. BibTeX citation. The license for the package source code is here.","title":"License"},{"location":"ml-doubledebiased/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Using machine learning to estimate causal effects \u00b6 Double debiased machine learning \u00b6 Chernozhukov, Chetverikov, et al. ( 2018 ), Chernozhukov et al. ( 2017 ) Parameter of interest $\\theta \\in \\R^{d_\\theta}$ Nuisance parameter $\\eta \\in T$ Moment conditions \\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta} with $\\psi$ known Estimate $\\hat{\\eta}$ using some machine learning method Estimate $\\hat{\\theta}$ using cross-fitting Cross-fitting \u00b6 Randomly partition into $K$ subsets $(I_k)_{k=1}^K$ $I^c_k = {1, \u2026, n} \\setminus I_k$ $\\hat{\\eta}_k =$ estimate of $\\eta$ using $I^c_k$ Estimator: \\begin{align*} 0 = & \\frac{1}{K} \\sum_{k=1}^K \\frac{K}{n} \\sum_{i \\in I_k} \\psi(w_i;\\hat{\\theta},\\hat{\\eta}_k) \\\\ 0 = & \\frac{1}{K} \\sum_{k=1}^K \\En_k[ \\psi(w_i;\\hat{\\theta},\\hat{\\eta}_k)] \\end{align*} Assumptions \u00b6 Linear score \\psi(w;\\theta,\\eta) = \\psi^a(w;\\eta) \\theta + \\psi^b(w;\\eta) Near Neyman orthogonality: \\lambda_n := \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{\\partial \\eta \\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right] } \\leq \\delta_n n^{-1/2} Assumptions [assumptions-1] \u00b6 Rate conditions: for $\\delta_n \\to 0$ and $\\Delta_n \\to 0$, we have $\\Pr(\\hat{\\eta}_k \\in \\mathcal{T}_n) \\geq 1-\\Delta_n$ and \\begin{align*} r_n := & \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{ \\Er[\\psi^a(W;\\eta)] - \\Er[\\psi^a(W;\\eta_0)]} \\leq \\delta_n \\\\ r_n' := & \\sup_{\\eta \\in \\mathcal{T}_n} \\Er\\left[ \\norm{ \\psi(W;\\theta_0,\\eta) - \\psi(W;\\theta_0,\\eta_0)}^2 \\right]^{1/2} \\leq \\delta_n \\\\ \\lambda_n' := & \\sup_{r \\in (0,1), \\eta \\in \\mathcal{T}_n} \\norm{ \\partial_r^2 \\Er\\left[\\psi(W;\\theta_0, \\eta_0 + r(\\eta - \\eta_0)) \\right]} \\leq \\delta_n/\\sqrt{n} \\end{align*} Moments exist and other regularity conditions We focus on the case of linear scores to simplify proofs and all of our examples have scores linear in $\\theta$. Chernozhukov, Chetverikov, et al. ( 2018 ) cover nonlinear scores as well. These rate conditions might look a little strange. The rate conditions are stated this way because they\u2019re exactly what is needed for the result to work. $\\Delta_n$ and $\\delta_n$ are sequences converging to $0$. $\\mathcal{T}_n$ is a shrinking neighborhood of $\\eta_0$. A good exercise would be show that if $\\psi$ a smooth function of $\\eta$ and $\\theta$, and $\\Er[(\\hat{\\eta}(x) - \\eta_0(x))^2]^{1/2} = O(\\epsilon_n) = o(n^{-1/4})$, then we can meet the above conditions with $r_n = r_n\u2019 = \\epsilon_n$ and $\\lambda_n\u2019 = \\epsilon_n^2$. Proof outline: \u00b6 Let $\\hat{J} = \\frac{1}{K} \\sum_{k=1}^K \\En_k [\\psi^a(w_i;\\hat{\\eta} k)]$, $J_0 = \\Er[\\psi^a(w_i;\\eta_0)]$, $R = \\hat{J}-J_0$ Show: \\small \\begin{align*} \\sqrt{n}(\\hat{\\theta} - \\theta_0) = & -\\sqrt{n} J_0^{-1} \\En[\\psi(w_i;\\theta_0,\\eta_0)] + \\\\ & + (J_0^{-1} - \\hat{J}^{-1}) \\left(\\sqrt{n} \\En[\\psi(w_i;\\theta_0,\\eta_0)] + \\sqrt{n}R_{n,2}\\right) + \\\\ & + \\sqrt{n}J_0^{-1}\\underbrace{\\left(\\frac{1}{K} \\sum_{k=1}^K \\En_k[ \\psi(w_i;\\theta_0,\\hat{\\eta}_k)] - \\En[\\psi(w_i;\\theta_0,\\eta_0)]\\right)}_{R_{n,2}} \\end{align*} Show $\\norm{R_{n,1}} = O_p(n^{-1/2} + r_n)$ Show $\\norm{R_{n,2}}= O_p(n^{-1/2} r_n\u2019 + \\lambda_n + \\lambda_n\u2019)$ For details see the appendix of Chernozhukov, Chetverikov, et al. ( 2018 ). Proof outline: Lemma 6.1 \u00b6 Lemma 6.1 If $\\Pr(\\norm{X_m} > \\epsilon_m | Y_m) \\to_p 0$, then $\\Pr(\\norm{X_m}>\\epsilon_m) \\to 0$. If $\\Er[\\norm{X_m}^q/\\epsilon_m^q | Y_m] \\to_p 0$ for $q\\geq 1$, then $\\Pr(\\norm{X_m}>\\epsilon_m) \\to 0$. If $\\norm{X_m} = O_p(A_m)$ conditional on $Y_m$ (i.e. for any $\\ell_m \\to \\infty$, $\\Pr(\\norm{X_m} > \\ell_m A_m | Y_m) \\to_p 0$), then $\\norm{X_m} = O_p(A_m)$ unconditionally by dominated convergence from Markov\u2019s inequality follows from (a) Proof outline: $R_{n,1}$ \u00b6 R_{n,1} = \\hat{J}-J_0 = \\frac{1}{K} \\sum_k \\left( \\En_k[\\psi^a(w_i;\\hat{\\eta}_k)] - \\Er[\\psi^a(W;\\eta_0)] \\right) $\\norm{\\En_k[\\psi^a(w_i;\\hat{\\eta} k)] - \\Er[\\psi^a(W;\\eta_0)]} \\leq U + U_{2,k}$ where \\begin{align*} U_{1,k} = & \\norm{\\En_k[\\psi^a(w_i;\\hat{\\eta}_k)] - \\Er[\\psi^a(W;\\hat{\\eta}_k)| I^c_k]} \\\\ U_{2,k} = & \\norm{ \\Er[\\psi^a(W;\\hat{\\eta}_k)| I^c_k] - \\Er[\\psi^a(W;\\eta_0)]} \\end{align*} Proof outline: $R_{n,2}$ \u00b6 $R_{n,2} = \\frac{1}{K} \\sum_{k=1}^K \\En_k\\left[ \\psi(w_i;\\theta_0,\\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0) \\right]$ $\\sqrt{n} \\norm{\\En_k\\left[ \\psi(w_i;\\theta_0,\\hat{\\eta} k) - \\psi(w_i;\\theta_0,\\eta_0) \\right]} \\leq U + U_{4,k}$ where \\small \\begin{align*} U_{3,k} = & \\norm{ \\frac{1}{\\sqrt{n}} \\sum_{i \\in I_k} \\left( \\psi(w_i;\\theta_0, \\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0) - \\Er[ \\psi(w_i;\\theta_0, \\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0)] \\right) } \\\\ U_{4,k} = & \\sqrt{n} \\norm{ \\Er[ \\psi(w_i;\\theta_0, \\hat{\\eta}_k) | I_k^c] - \\Er[\\psi(w_i;\\theta_0,\\eta_0)]} \\end{align*} $U_{4,k} = \\sqrt{n} \\norm{f_k(1)}$ where f_k(r) = \\Er[\\psi(W;\\theta_0,\\eta_0 + r(\\hat{\\eta}_k - \\eta_0)) | I^c_k] - \\Er[\\psi(W;\\theta_0,\\eta_0)] Asymptotic normality \u00b6 \\sqrt{n} \\sigma^{-1} (\\hat{\\theta} - \\theta_0) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\bar{\\psi}(w_i) + O_p(\\rho_n) \\leadsto N(0,I) $\\rho_n := n^{-1/2} + r_n + r_n\u2019 + n^{1/2} (\\lambda_n +\\lambda_n\u2019) \\lesssim \\delta_n$ Influence function \\bar{\\psi}(w) = -\\sigma^{-1} J_0^{-1} \\psi(w;\\theta_0,\\eta_0) $\\sigma^2 := J_0^{-1} \\Er \\psi(w;\\theta_0,\\eta_0) \\psi(w;\\theta_0,\\eta_0)\u2019 \u2019$ This is the DML2 case of theorem 3.1 of Chernozhukov, Chetverikov, et al. ( 2018 ). Creating orthogonal moments \u00b6 Need \\partial \\eta\\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right] \\approx 0 Given an some model, how do we find a suitable $\\psi$? Orthogonal scores via concentrating-out \u00b6 Original model: (\\theta_0, \\beta_0) = \\argmax_{\\theta, \\beta} \\Er[\\ell(W;\\theta,\\beta)] Define \\eta(\\theta) = \\beta(\\theta) = \\argmax_\\beta \\Er[\\ell(W;\\theta,\\beta)] First order condition from $\\max_\\theta \\Er[\\ell(W;\\theta,\\beta(\\theta))]$ is 0 = \\Er\\left[ \\underbrace{\\frac{\\partial \\ell}{\\partial \\theta} + \\frac{\\partial \\ell}{\\partial \\beta} \\frac{d \\beta}{d \\theta}}_{\\psi(W;\\theta,\\beta(\\theta))} \\right] Orthogonal scores via projection \u00b6 Original model: $m: \\mathcal{W} \\times \\R^{d_\\theta} \\times \\R^{d_h} \\to \\R^{d_m}$ \\Er[m(W;\\theta_0,h_0(Z))|R] = 0 Let $A(R)$ be $d_\\theta \\times d_m$ moment selection matrix, $\\Omega(R)$ $d_m \\times d_m$ weighting matrix, and \\begin{align*} \\Gamma(R) = & \\partial_{v'} \\Er[m(W;\\theta_0,v)|R]|_{v=h_0(Z)} \\\\ G(Z) = & \\Er[A(R)'\\Omega(R)^{-1} \\Gamma(R)|Z] \\Er[\\Gamma(R)'\\Omega(R)^{-1} \\Gamma(R) |Z]^{-1} \\\\ \\mu_0(R) = & A(R)'\\Omega(R)^{-1} - G(Z) \\Gamma(R)'\\Omega(R)^{-1} \\end{align*} $\\eta = (\\mu, h)$ and \\psi(W;\\theta, \\eta) = \\mu(R) m(W;\\theta, h(Z)) Chernozhukov, Chetverikov, et al. ( 2018 ) show how to construct orthogonal scores in a few examples via concentrating out and projection. Chernozhukov, Hansen, and Spindler ( 2015 ) also discusses creating orthogonal scores. Example: average derivative \u00b6 $x,y \\in \\R^1$, $\\Er[y|x] = f_0(x)$, $p(x) =$ density of $x$ $\\theta_0 = \\Er[f_0\u2019(x)]$ Joint objective \\min_{\\theta,f} \\Er\\left[ (y - f(x))^2 + (\\theta - f'(x)^2) \\right] $f_\\theta(x) = \\Er[y|x] + \\theta \\partial_x \\log p(x) - f\u2019\u2018(x) - f\u2019(x) \\partial_x \\log p(x)$ Concentrated objective: \\min_\\theta \\Er\\left[ (y - f_\\theta(x))^2 + (\\theta - f_\\theta'(x)^2) \\right] First order condition at $f_\\theta = f_0$ gives 0 = \\Er\\left[ (y - f_0(x))\\partial_x \\log p(x) + (\\theta - f_0'(x)) \\right] We\u2019ll go over this derivation in lecture, but I don\u2019t think I\u2019ll have time to type it here. See Chernozhukov, Newey, and Robins ( 2018 ) for an approach to estimating average derivatives (and other linear in $\\theta$ models) that doesn\u2019t require explicitly calculating an orthogonal moment condition. Example : average derivative with endogeneity \u00b6 $x,y \\in \\R^1$, $p(x) =$ density of $x$ Model : $\\Er[y - f(x) | z] = 0$ $\\theta_0 = \\Er[f_0\u2019(x)]$ Joint objective: \\min_{\\theta,f} \\Er\\left[ \\Er[y - f(x)|z]^2 + (\\theta - f'(x))^2 \\right] $f_\\theta(x) = (T^ T)^{-1}\\left((T^ \\Er[y|z])(x) - \\theta \\partial_x \\log p(x)\\right)$ where $T:\\mathcal{L}^2_{p} \\to \\mathcal{L}^2_{\\mu_z}$ with $(T f)(z) = \\Er[f(x) |z]$ and $T^ :\\mathcal{L}^2_{\\mu_z} \\to \\mathcal{L}^2_{p}$ with $(T^ g)(z) = \\Er[g(z) |x]$ Orthogonal moment condition : 0 = \\Er\\left[ \\Er[y - f(x) | z] (T (T^* T)^{-1} \\partial_x \\log p)(z) + (\\theta - f'(x)) \\right] The first order condition for $f$ in the joint objective function is \\begin{align*} 0 = \\Er \\left[ \\Er[y-f(x) |z]\\Er[v(x)|z] + (\\theta - f'(x))(-v'(x)) \\right] \\end{align*} Writing these expectations as integrals, integrating by parts to get rid of $v\u2019(x)$, and switching the order of integration, gives \\begin{align*} 0 = \\int_\\mathcal{X} v(x)\\left( \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - (\\theta-f'(x))\\partial_x \\log p(x) - f''(x) \\right) p(x) dx \\end{align*} Notice that integrating by parts $\\int f\u2019\u2018(x) p(x) dx = \\int f\u2019 p\u2019(x) dx$ eliminates the terms with $f\u2019$ and $f\u2019\u2018$, leaving \\begin{align*} 0 = \\int_\\mathcal{X} v(x)\\left( \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - \\theta \\partial_x \\log p(x) \\right) p(x) dx \\end{align*} For this to be $0$ for all $v$, we need 0 = \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - \\theta \\partial_x \\log p(x) or equivalently using $T$ and $T^ $, 0 = \\left(T^*(E[y|z] - T f)\\right)(x) - \\theta \\partial_x \\log p(x) Note that $T$ and $T^ $ are linear, and $T^ $ is the adjoint of $T$. Also, identification of $f$ requires $T$ is one to one. Hence, if $f$ is identified, $T^ T$ is invertible. Therefore, we can solve for $f$ as: f_\\theta(x) = (T^* T)^{-1} \\left( (T^* \\Er[y |z])(x) - \\theta \\partial \\log p(x) \\right) Plugging $f_\\theta(x)$ back into the objective function and then differentiating with respect to $\\theta$ gives the orthogonal moment condition on the slide. Verifying that this moment condition is indeed orthogonal is slightly tedious. Writing out some of the expectations as integrals, changing order of integrations, and judiciously factoring out terms, will eventually lead to the desired conclusion. Carrasco, Florens, and Renault ( 2007 ) is an excellent review about estimating $(T^* T)^{-1}$ and the inverses of other linear transformations. Example: average elasticity \u00b6 Demand $D(p)$, quantities $q$, instruments $z$ \\Er[q-D(p) |z] = 0 Average elasticity $\\theta \\Er[D\u2019(p)/D(p) | z ]$ Joint objective : \\min_{\\theta,D} \\Er\\left[ \\Er[q - D(p)|z]^2 + (\\theta - D'(p)/D(p))^2 \\right] Example: control function \u00b6 \\begin{align*} 0 = & \\Er[d - p(x,z) | x,z] \\\\ 0 = & \\Er[y - x\\beta - g(p(x,z)) | x,z] \\end{align*} Treatment heterogeneity \u00b6 Potential outcomes model Treatment $d \\in {0,1}$ Potential outcomes $y(1), y(0)$ Covariates $x$ Unconfoundedness or instruments Objects of interest: Conditional average treatment effect $s_0(x) = \\Er[y(1) - y(0) | x]$ Range and other measures of spread of conditional average treatment effect Most and least affected groups Fixed, finite groups \u00b6 $G_1, \u2026, G_K$ finite partition of support $(x)$ Estimate $\\Er[y(1) - y(0) | x \\in G_k]$ as above pros: easy inference, reveals some heterogeneity cons: poorly chosen partition hides some heterogeneity, searching partitions violates inference Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments \u00b6 Chernozhukov, Demirer, et al. ( 2018 ) Use machine learning to find partition with sample splitting to allow easy inference Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]} Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments [generic-machine-learning-inference-on-heterogenous-treatment-effects-in-randomized-experiments-1] \u00b6 Define $G_k = 1{\\ell_{k-1} \\leq S(x) \\leq \\ell_k}$ Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\sum_k \\gamma_k (d-P(X)) 1(G_k) + \\epsilon $\\hat{\\gamma}_k \\to_p \\Er[y(1) - y(0) | G_k]$ Best linear projection of CATE \u00b6 Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]} Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\beta_0 (d-P(x)) + \\beta_1 (d-P(x))(S(x) - \\Er[S(x)]) + \\epsilon $\\hat{\\beta} 0, \\hat{\\beta}_1 \\to_p \\argmin \\Er[(s_0(x) - b_0 - b_1 (S(x)-E[S(x)]))^2]$ Inference on CATE \u00b6 Inference on $\\Er[y(1) - y(0) | x] = s_0(x)$ challenging when $x$ high dimensional and/or few restrictions on $s_0$ Pointwise results for random forests : Wager and Athey ( 2018 ), Athey, Tibshirani, and Wager ( 2016 ) Recent review of high dimensional inference : Belloni, Chernozhukov, Chetverikov, Hansen, et al. ( 2018 ) Random forest asymptotic normality \u00b6 Wager and Athey ( 2018 ) $\\mu(x) = \\Er[y|x]$ $\\hat{\\mu}(x)$ estimate from honest random forest honest $=$ trees independent of outcomes being averaged sample-splitting or trees formed using another outcome Then \\frac{\\hat{\\mu}(x) - \\mu(x)}{\\hat{\\sigma}_n(x)} \\leadsto N(0,1) $\\hat{\\sigma}_n(x) \\to 0$ slower than $n^{-1/2}$ Random forest asymptotic normality \u00b6 Results are pointwise, but what about? $H_0: \\mu(x_1) = \\mu(x_2)$ ${x: \\mu(x) \\geq 0 }$ $\\Pr(\\mu(x) \\leq 0)$ Uniform inference \u00b6 Belloni, Chernozhukov, Chetverikov, Hansen, et al. ( 2018 ) Belloni, Chernozhukov, Chetverikov, and Wei ( 2018 ) Bibliography \u00b6 Athey, Susan, Julie Tibshirani, and Stefan Wager. 2016. \u201cGeneralized Random Forests.\u201d https://arxiv.org/abs/1610.01271 . Belloni, Alexandre, Victor Chernozhukov, Denis Chetverikov, Christian Hansen, and Kengo Kato. 2018. \u201cHigh-Dimensional Econometrics and Regularized Gmm.\u201d https://arxiv.org/abs/1806.01888 . Belloni, Alexandre, Victor Chernozhukov, Denis Chetverikov, and Ying Wei. 2018. \u201cUniformly Valid Post-Regularization Confidence Regions for Many Functional Parameters in Z-Estimation Framework.\u201d Ann. Statist. 46 (6B): 3643\u201375. https://doi.org/10.1214/17-AOS1671 . Carrasco, Marine, Jean-Pierre Florens, and Eric Renault. 2007. \u201cChapter 77 Linear Inverse Problems in Structural Econometrics Estimation Based on Spectral Decomposition and Regularization.\u201d In, edited by James J. Heckman and Edward E. Leamer, 6:5633\u20135751. Handbook of Econometrics. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(07)06077-1 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, and Whitney Newey. 2017. \u201cDouble/Debiased/Neyman Machine Learning of Treatment Effects.\u201d American Economic Review 107 (5): 261\u201365. https://doi.org/10.1257/aer.p20171038 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Chernozhukov, Victor, Mert Demirer, Esther Duflo, and Iv\u00e1n Fern\u00e1ndez-Val. 2018. \u201cGeneric Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experimentsxo.\u201d Working Paper 24678. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w24678 . Chernozhukov, Victor, Christian Hansen, and Martin Spindler. 2015. \u201cValid Post-Selection and Post-Regularization Inference: An Elementary, General Approach.\u201d Annual Review of Economics 7 (1): 649\u201388. https://doi.org/10.1146/annurev-economics-012315-015826 . Chernozhukov, Victor, Whitney Newey, and James Robins. 2018. \u201cDouble/de-Biased Machine Learning Using Regularized Riesz Representers.\u201d https://arxiv.org/abs/1802.08667 . Wager, Stefan, and Susan Athey. 2018. \u201cEstimation and Inference of Heterogeneous Treatment Effects Using Random Forests.\u201d Journal of the American Statistical Association 0 (0): 1\u201315. https://doi.org/10.1080/01621459.2017.1319839 .","title":"Inference"},{"location":"ml-doubledebiased/#using-machine-learning-to-estimate-causal-effects","text":"","title":"Using machine learning to estimate causal effects"},{"location":"ml-doubledebiased/#double-debiased-machine-learning","text":"Chernozhukov, Chetverikov, et al. ( 2018 ), Chernozhukov et al. ( 2017 ) Parameter of interest $\\theta \\in \\R^{d_\\theta}$ Nuisance parameter $\\eta \\in T$ Moment conditions \\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta} with $\\psi$ known Estimate $\\hat{\\eta}$ using some machine learning method Estimate $\\hat{\\theta}$ using cross-fitting","title":"Double debiased machine learning"},{"location":"ml-doubledebiased/#cross-fitting","text":"Randomly partition into $K$ subsets $(I_k)_{k=1}^K$ $I^c_k = {1, \u2026, n} \\setminus I_k$ $\\hat{\\eta}_k =$ estimate of $\\eta$ using $I^c_k$ Estimator: \\begin{align*} 0 = & \\frac{1}{K} \\sum_{k=1}^K \\frac{K}{n} \\sum_{i \\in I_k} \\psi(w_i;\\hat{\\theta},\\hat{\\eta}_k) \\\\ 0 = & \\frac{1}{K} \\sum_{k=1}^K \\En_k[ \\psi(w_i;\\hat{\\theta},\\hat{\\eta}_k)] \\end{align*}","title":"Cross-fitting"},{"location":"ml-doubledebiased/#assumptions","text":"Linear score \\psi(w;\\theta,\\eta) = \\psi^a(w;\\eta) \\theta + \\psi^b(w;\\eta) Near Neyman orthogonality: \\lambda_n := \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{\\partial \\eta \\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right] } \\leq \\delta_n n^{-1/2}","title":"Assumptions"},{"location":"ml-doubledebiased/#assumptions-assumptions-1","text":"Rate conditions: for $\\delta_n \\to 0$ and $\\Delta_n \\to 0$, we have $\\Pr(\\hat{\\eta}_k \\in \\mathcal{T}_n) \\geq 1-\\Delta_n$ and \\begin{align*} r_n := & \\sup_{\\eta \\in \\mathcal{T}_n} \\norm{ \\Er[\\psi^a(W;\\eta)] - \\Er[\\psi^a(W;\\eta_0)]} \\leq \\delta_n \\\\ r_n' := & \\sup_{\\eta \\in \\mathcal{T}_n} \\Er\\left[ \\norm{ \\psi(W;\\theta_0,\\eta) - \\psi(W;\\theta_0,\\eta_0)}^2 \\right]^{1/2} \\leq \\delta_n \\\\ \\lambda_n' := & \\sup_{r \\in (0,1), \\eta \\in \\mathcal{T}_n} \\norm{ \\partial_r^2 \\Er\\left[\\psi(W;\\theta_0, \\eta_0 + r(\\eta - \\eta_0)) \\right]} \\leq \\delta_n/\\sqrt{n} \\end{align*} Moments exist and other regularity conditions We focus on the case of linear scores to simplify proofs and all of our examples have scores linear in $\\theta$. Chernozhukov, Chetverikov, et al. ( 2018 ) cover nonlinear scores as well. These rate conditions might look a little strange. The rate conditions are stated this way because they\u2019re exactly what is needed for the result to work. $\\Delta_n$ and $\\delta_n$ are sequences converging to $0$. $\\mathcal{T}_n$ is a shrinking neighborhood of $\\eta_0$. A good exercise would be show that if $\\psi$ a smooth function of $\\eta$ and $\\theta$, and $\\Er[(\\hat{\\eta}(x) - \\eta_0(x))^2]^{1/2} = O(\\epsilon_n) = o(n^{-1/4})$, then we can meet the above conditions with $r_n = r_n\u2019 = \\epsilon_n$ and $\\lambda_n\u2019 = \\epsilon_n^2$.","title":"Assumptions [assumptions-1]"},{"location":"ml-doubledebiased/#proof-outline","text":"Let $\\hat{J} = \\frac{1}{K} \\sum_{k=1}^K \\En_k [\\psi^a(w_i;\\hat{\\eta} k)]$, $J_0 = \\Er[\\psi^a(w_i;\\eta_0)]$, $R = \\hat{J}-J_0$ Show: \\small \\begin{align*} \\sqrt{n}(\\hat{\\theta} - \\theta_0) = & -\\sqrt{n} J_0^{-1} \\En[\\psi(w_i;\\theta_0,\\eta_0)] + \\\\ & + (J_0^{-1} - \\hat{J}^{-1}) \\left(\\sqrt{n} \\En[\\psi(w_i;\\theta_0,\\eta_0)] + \\sqrt{n}R_{n,2}\\right) + \\\\ & + \\sqrt{n}J_0^{-1}\\underbrace{\\left(\\frac{1}{K} \\sum_{k=1}^K \\En_k[ \\psi(w_i;\\theta_0,\\hat{\\eta}_k)] - \\En[\\psi(w_i;\\theta_0,\\eta_0)]\\right)}_{R_{n,2}} \\end{align*} Show $\\norm{R_{n,1}} = O_p(n^{-1/2} + r_n)$ Show $\\norm{R_{n,2}}= O_p(n^{-1/2} r_n\u2019 + \\lambda_n + \\lambda_n\u2019)$ For details see the appendix of Chernozhukov, Chetverikov, et al. ( 2018 ).","title":"Proof outline:"},{"location":"ml-doubledebiased/#proof-outline-lemma-61","text":"Lemma 6.1 If $\\Pr(\\norm{X_m} > \\epsilon_m | Y_m) \\to_p 0$, then $\\Pr(\\norm{X_m}>\\epsilon_m) \\to 0$. If $\\Er[\\norm{X_m}^q/\\epsilon_m^q | Y_m] \\to_p 0$ for $q\\geq 1$, then $\\Pr(\\norm{X_m}>\\epsilon_m) \\to 0$. If $\\norm{X_m} = O_p(A_m)$ conditional on $Y_m$ (i.e. for any $\\ell_m \\to \\infty$, $\\Pr(\\norm{X_m} > \\ell_m A_m | Y_m) \\to_p 0$), then $\\norm{X_m} = O_p(A_m)$ unconditionally by dominated convergence from Markov\u2019s inequality follows from (a)","title":"Proof outline: Lemma 6.1"},{"location":"ml-doubledebiased/#proof-outline-r_n1","text":"R_{n,1} = \\hat{J}-J_0 = \\frac{1}{K} \\sum_k \\left( \\En_k[\\psi^a(w_i;\\hat{\\eta}_k)] - \\Er[\\psi^a(W;\\eta_0)] \\right) $\\norm{\\En_k[\\psi^a(w_i;\\hat{\\eta} k)] - \\Er[\\psi^a(W;\\eta_0)]} \\leq U + U_{2,k}$ where \\begin{align*} U_{1,k} = & \\norm{\\En_k[\\psi^a(w_i;\\hat{\\eta}_k)] - \\Er[\\psi^a(W;\\hat{\\eta}_k)| I^c_k]} \\\\ U_{2,k} = & \\norm{ \\Er[\\psi^a(W;\\hat{\\eta}_k)| I^c_k] - \\Er[\\psi^a(W;\\eta_0)]} \\end{align*}","title":"Proof outline: $R_{n,1}$"},{"location":"ml-doubledebiased/#proof-outline-r_n2","text":"$R_{n,2} = \\frac{1}{K} \\sum_{k=1}^K \\En_k\\left[ \\psi(w_i;\\theta_0,\\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0) \\right]$ $\\sqrt{n} \\norm{\\En_k\\left[ \\psi(w_i;\\theta_0,\\hat{\\eta} k) - \\psi(w_i;\\theta_0,\\eta_0) \\right]} \\leq U + U_{4,k}$ where \\small \\begin{align*} U_{3,k} = & \\norm{ \\frac{1}{\\sqrt{n}} \\sum_{i \\in I_k} \\left( \\psi(w_i;\\theta_0, \\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0) - \\Er[ \\psi(w_i;\\theta_0, \\hat{\\eta}_k) - \\psi(w_i;\\theta_0,\\eta_0)] \\right) } \\\\ U_{4,k} = & \\sqrt{n} \\norm{ \\Er[ \\psi(w_i;\\theta_0, \\hat{\\eta}_k) | I_k^c] - \\Er[\\psi(w_i;\\theta_0,\\eta_0)]} \\end{align*} $U_{4,k} = \\sqrt{n} \\norm{f_k(1)}$ where f_k(r) = \\Er[\\psi(W;\\theta_0,\\eta_0 + r(\\hat{\\eta}_k - \\eta_0)) | I^c_k] - \\Er[\\psi(W;\\theta_0,\\eta_0)]","title":"Proof outline: $R_{n,2}$"},{"location":"ml-doubledebiased/#asymptotic-normality","text":"\\sqrt{n} \\sigma^{-1} (\\hat{\\theta} - \\theta_0) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\bar{\\psi}(w_i) + O_p(\\rho_n) \\leadsto N(0,I) $\\rho_n := n^{-1/2} + r_n + r_n\u2019 + n^{1/2} (\\lambda_n +\\lambda_n\u2019) \\lesssim \\delta_n$ Influence function \\bar{\\psi}(w) = -\\sigma^{-1} J_0^{-1} \\psi(w;\\theta_0,\\eta_0) $\\sigma^2 := J_0^{-1} \\Er \\psi(w;\\theta_0,\\eta_0) \\psi(w;\\theta_0,\\eta_0)\u2019 \u2019$ This is the DML2 case of theorem 3.1 of Chernozhukov, Chetverikov, et al. ( 2018 ).","title":"Asymptotic normality"},{"location":"ml-doubledebiased/#creating-orthogonal-moments","text":"Need \\partial \\eta\\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right] \\approx 0 Given an some model, how do we find a suitable $\\psi$?","title":"Creating orthogonal moments"},{"location":"ml-doubledebiased/#orthogonal-scores-via-concentrating-out","text":"Original model: (\\theta_0, \\beta_0) = \\argmax_{\\theta, \\beta} \\Er[\\ell(W;\\theta,\\beta)] Define \\eta(\\theta) = \\beta(\\theta) = \\argmax_\\beta \\Er[\\ell(W;\\theta,\\beta)] First order condition from $\\max_\\theta \\Er[\\ell(W;\\theta,\\beta(\\theta))]$ is 0 = \\Er\\left[ \\underbrace{\\frac{\\partial \\ell}{\\partial \\theta} + \\frac{\\partial \\ell}{\\partial \\beta} \\frac{d \\beta}{d \\theta}}_{\\psi(W;\\theta,\\beta(\\theta))} \\right]","title":"Orthogonal scores via concentrating-out"},{"location":"ml-doubledebiased/#orthogonal-scores-via-projection","text":"Original model: $m: \\mathcal{W} \\times \\R^{d_\\theta} \\times \\R^{d_h} \\to \\R^{d_m}$ \\Er[m(W;\\theta_0,h_0(Z))|R] = 0 Let $A(R)$ be $d_\\theta \\times d_m$ moment selection matrix, $\\Omega(R)$ $d_m \\times d_m$ weighting matrix, and \\begin{align*} \\Gamma(R) = & \\partial_{v'} \\Er[m(W;\\theta_0,v)|R]|_{v=h_0(Z)} \\\\ G(Z) = & \\Er[A(R)'\\Omega(R)^{-1} \\Gamma(R)|Z] \\Er[\\Gamma(R)'\\Omega(R)^{-1} \\Gamma(R) |Z]^{-1} \\\\ \\mu_0(R) = & A(R)'\\Omega(R)^{-1} - G(Z) \\Gamma(R)'\\Omega(R)^{-1} \\end{align*} $\\eta = (\\mu, h)$ and \\psi(W;\\theta, \\eta) = \\mu(R) m(W;\\theta, h(Z)) Chernozhukov, Chetverikov, et al. ( 2018 ) show how to construct orthogonal scores in a few examples via concentrating out and projection. Chernozhukov, Hansen, and Spindler ( 2015 ) also discusses creating orthogonal scores.","title":"Orthogonal scores via projection"},{"location":"ml-doubledebiased/#example-average-derivative","text":"$x,y \\in \\R^1$, $\\Er[y|x] = f_0(x)$, $p(x) =$ density of $x$ $\\theta_0 = \\Er[f_0\u2019(x)]$ Joint objective \\min_{\\theta,f} \\Er\\left[ (y - f(x))^2 + (\\theta - f'(x)^2) \\right] $f_\\theta(x) = \\Er[y|x] + \\theta \\partial_x \\log p(x) - f\u2019\u2018(x) - f\u2019(x) \\partial_x \\log p(x)$ Concentrated objective: \\min_\\theta \\Er\\left[ (y - f_\\theta(x))^2 + (\\theta - f_\\theta'(x)^2) \\right] First order condition at $f_\\theta = f_0$ gives 0 = \\Er\\left[ (y - f_0(x))\\partial_x \\log p(x) + (\\theta - f_0'(x)) \\right] We\u2019ll go over this derivation in lecture, but I don\u2019t think I\u2019ll have time to type it here. See Chernozhukov, Newey, and Robins ( 2018 ) for an approach to estimating average derivatives (and other linear in $\\theta$ models) that doesn\u2019t require explicitly calculating an orthogonal moment condition.","title":"Example: average derivative"},{"location":"ml-doubledebiased/#example-average-derivative-with-endogeneity","text":"$x,y \\in \\R^1$, $p(x) =$ density of $x$ Model : $\\Er[y - f(x) | z] = 0$ $\\theta_0 = \\Er[f_0\u2019(x)]$ Joint objective: \\min_{\\theta,f} \\Er\\left[ \\Er[y - f(x)|z]^2 + (\\theta - f'(x))^2 \\right] $f_\\theta(x) = (T^ T)^{-1}\\left((T^ \\Er[y|z])(x) - \\theta \\partial_x \\log p(x)\\right)$ where $T:\\mathcal{L}^2_{p} \\to \\mathcal{L}^2_{\\mu_z}$ with $(T f)(z) = \\Er[f(x) |z]$ and $T^ :\\mathcal{L}^2_{\\mu_z} \\to \\mathcal{L}^2_{p}$ with $(T^ g)(z) = \\Er[g(z) |x]$ Orthogonal moment condition : 0 = \\Er\\left[ \\Er[y - f(x) | z] (T (T^* T)^{-1} \\partial_x \\log p)(z) + (\\theta - f'(x)) \\right] The first order condition for $f$ in the joint objective function is \\begin{align*} 0 = \\Er \\left[ \\Er[y-f(x) |z]\\Er[v(x)|z] + (\\theta - f'(x))(-v'(x)) \\right] \\end{align*} Writing these expectations as integrals, integrating by parts to get rid of $v\u2019(x)$, and switching the order of integration, gives \\begin{align*} 0 = \\int_\\mathcal{X} v(x)\\left( \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - (\\theta-f'(x))\\partial_x \\log p(x) - f''(x) \\right) p(x) dx \\end{align*} Notice that integrating by parts $\\int f\u2019\u2018(x) p(x) dx = \\int f\u2019 p\u2019(x) dx$ eliminates the terms with $f\u2019$ and $f\u2019\u2018$, leaving \\begin{align*} 0 = \\int_\\mathcal{X} v(x)\\left( \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - \\theta \\partial_x \\log p(x) \\right) p(x) dx \\end{align*} For this to be $0$ for all $v$, we need 0 = \\int_\\mathcal{Z} \\Er[y - f(x)|z] p(z|x) dz - \\theta \\partial_x \\log p(x) or equivalently using $T$ and $T^ $, 0 = \\left(T^*(E[y|z] - T f)\\right)(x) - \\theta \\partial_x \\log p(x) Note that $T$ and $T^ $ are linear, and $T^ $ is the adjoint of $T$. Also, identification of $f$ requires $T$ is one to one. Hence, if $f$ is identified, $T^ T$ is invertible. Therefore, we can solve for $f$ as: f_\\theta(x) = (T^* T)^{-1} \\left( (T^* \\Er[y |z])(x) - \\theta \\partial \\log p(x) \\right) Plugging $f_\\theta(x)$ back into the objective function and then differentiating with respect to $\\theta$ gives the orthogonal moment condition on the slide. Verifying that this moment condition is indeed orthogonal is slightly tedious. Writing out some of the expectations as integrals, changing order of integrations, and judiciously factoring out terms, will eventually lead to the desired conclusion. Carrasco, Florens, and Renault ( 2007 ) is an excellent review about estimating $(T^* T)^{-1}$ and the inverses of other linear transformations.","title":"Example : average derivative with endogeneity"},{"location":"ml-doubledebiased/#example-average-elasticity","text":"Demand $D(p)$, quantities $q$, instruments $z$ \\Er[q-D(p) |z] = 0 Average elasticity $\\theta \\Er[D\u2019(p)/D(p) | z ]$ Joint objective : \\min_{\\theta,D} \\Er\\left[ \\Er[q - D(p)|z]^2 + (\\theta - D'(p)/D(p))^2 \\right]","title":"Example: average elasticity"},{"location":"ml-doubledebiased/#example-control-function","text":"\\begin{align*} 0 = & \\Er[d - p(x,z) | x,z] \\\\ 0 = & \\Er[y - x\\beta - g(p(x,z)) | x,z] \\end{align*}","title":"Example: control function"},{"location":"ml-doubledebiased/#treatment-heterogeneity","text":"Potential outcomes model Treatment $d \\in {0,1}$ Potential outcomes $y(1), y(0)$ Covariates $x$ Unconfoundedness or instruments Objects of interest: Conditional average treatment effect $s_0(x) = \\Er[y(1) - y(0) | x]$ Range and other measures of spread of conditional average treatment effect Most and least affected groups","title":"Treatment heterogeneity"},{"location":"ml-doubledebiased/#fixed-finite-groups","text":"$G_1, \u2026, G_K$ finite partition of support $(x)$ Estimate $\\Er[y(1) - y(0) | x \\in G_k]$ as above pros: easy inference, reveals some heterogeneity cons: poorly chosen partition hides some heterogeneity, searching partitions violates inference","title":"Fixed, finite groups"},{"location":"ml-doubledebiased/#generic-machine-learning-inference-on-heterogenous-treatment-effects-in-randomized-experiments","text":"Chernozhukov, Demirer, et al. ( 2018 ) Use machine learning to find partition with sample splitting to allow easy inference Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]}","title":"Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments"},{"location":"ml-doubledebiased/#generic-machine-learning-inference-on-heterogenous-treatment-effects-in-randomized-experiments-generic-machine-learning-inference-on-heterogenous-treatment-effects-in-randomized-experiments-1","text":"Define $G_k = 1{\\ell_{k-1} \\leq S(x) \\leq \\ell_k}$ Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\sum_k \\gamma_k (d-P(X)) 1(G_k) + \\epsilon $\\hat{\\gamma}_k \\to_p \\Er[y(1) - y(0) | G_k]$","title":"Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments [generic-machine-learning-inference-on-heterogenous-treatment-effects-in-randomized-experiments-1]"},{"location":"ml-doubledebiased/#best-linear-projection-of-cate","text":"Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]} Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\beta_0 (d-P(x)) + \\beta_1 (d-P(x))(S(x) - \\Er[S(x)]) + \\epsilon $\\hat{\\beta} 0, \\hat{\\beta}_1 \\to_p \\argmin \\Er[(s_0(x) - b_0 - b_1 (S(x)-E[S(x)]))^2]$","title":"Best linear projection of CATE"},{"location":"ml-doubledebiased/#inference-on-cate","text":"Inference on $\\Er[y(1) - y(0) | x] = s_0(x)$ challenging when $x$ high dimensional and/or few restrictions on $s_0$ Pointwise results for random forests : Wager and Athey ( 2018 ), Athey, Tibshirani, and Wager ( 2016 ) Recent review of high dimensional inference : Belloni, Chernozhukov, Chetverikov, Hansen, et al. ( 2018 )","title":"Inference on CATE"},{"location":"ml-doubledebiased/#random-forest-asymptotic-normality","text":"Wager and Athey ( 2018 ) $\\mu(x) = \\Er[y|x]$ $\\hat{\\mu}(x)$ estimate from honest random forest honest $=$ trees independent of outcomes being averaged sample-splitting or trees formed using another outcome Then \\frac{\\hat{\\mu}(x) - \\mu(x)}{\\hat{\\sigma}_n(x)} \\leadsto N(0,1) $\\hat{\\sigma}_n(x) \\to 0$ slower than $n^{-1/2}$","title":"Random forest asymptotic normality"},{"location":"ml-doubledebiased/#random-forest-asymptotic-normality_1","text":"Results are pointwise, but what about? $H_0: \\mu(x_1) = \\mu(x_2)$ ${x: \\mu(x) \\geq 0 }$ $\\Pr(\\mu(x) \\leq 0)$","title":"Random forest asymptotic normality"},{"location":"ml-doubledebiased/#uniform-inference","text":"Belloni, Chernozhukov, Chetverikov, Hansen, et al. ( 2018 ) Belloni, Chernozhukov, Chetverikov, and Wei ( 2018 )","title":"Uniform inference"},{"location":"ml-doubledebiased/#bibliography","text":"Athey, Susan, Julie Tibshirani, and Stefan Wager. 2016. \u201cGeneralized Random Forests.\u201d https://arxiv.org/abs/1610.01271 . Belloni, Alexandre, Victor Chernozhukov, Denis Chetverikov, Christian Hansen, and Kengo Kato. 2018. \u201cHigh-Dimensional Econometrics and Regularized Gmm.\u201d https://arxiv.org/abs/1806.01888 . Belloni, Alexandre, Victor Chernozhukov, Denis Chetverikov, and Ying Wei. 2018. \u201cUniformly Valid Post-Regularization Confidence Regions for Many Functional Parameters in Z-Estimation Framework.\u201d Ann. Statist. 46 (6B): 3643\u201375. https://doi.org/10.1214/17-AOS1671 . Carrasco, Marine, Jean-Pierre Florens, and Eric Renault. 2007. \u201cChapter 77 Linear Inverse Problems in Structural Econometrics Estimation Based on Spectral Decomposition and Regularization.\u201d In, edited by James J. Heckman and Edward E. Leamer, 6:5633\u20135751. Handbook of Econometrics. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(07)06077-1 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, and Whitney Newey. 2017. \u201cDouble/Debiased/Neyman Machine Learning of Treatment Effects.\u201d American Economic Review 107 (5): 261\u201365. https://doi.org/10.1257/aer.p20171038 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Chernozhukov, Victor, Mert Demirer, Esther Duflo, and Iv\u00e1n Fern\u00e1ndez-Val. 2018. \u201cGeneric Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experimentsxo.\u201d Working Paper 24678. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w24678 . Chernozhukov, Victor, Christian Hansen, and Martin Spindler. 2015. \u201cValid Post-Selection and Post-Regularization Inference: An Elementary, General Approach.\u201d Annual Review of Economics 7 (1): 649\u201388. https://doi.org/10.1146/annurev-economics-012315-015826 . Chernozhukov, Victor, Whitney Newey, and James Robins. 2018. \u201cDouble/de-Biased Machine Learning Using Regularized Riesz Representers.\u201d https://arxiv.org/abs/1802.08667 . Wager, Stefan, and Susan Athey. 2018. \u201cEstimation and Inference of Heterogeneous Treatment Effects Using Random Forests.\u201d Journal of the American Statistical Association 0 (0): 1\u201315. https://doi.org/10.1080/01621459.2017.1319839 .","title":"Bibliography"},{"location":"ml-intro/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 These notes will examine the incorportion of machine learning methods in classic econometric techniques for estimating causal effects. More specifally, we will focus on estimating treatment effects using matching and instrumental variables. In these estimators (and many others) there is a low-dimensional parameter of interest, such as the average treatment effect, but estimating it requires also estimating a potentially high dimensional nuisance parameter, such as the propensity score. Machine learning methods were developed for prediction with high dimensional data. It is then natural to try to use machine learning for estimating high dimensional nuisance parameters. Care must be taken when doing so though because the flexibility and complexity that make machine learning so good at prediction also pose challenges for inference. Example: partially linear model \u00b6 y_i = \\theta d_i + f(x_i) + \\epsilon_i Interested in $\\theta$ Assume $\\Er[\\epsilon|d,x] = 0$ Nuisance parameter $f()$ E.g. Donohue and Levitt ( 2001 ) The simplest example of the setting we will analyze is a partially linear model. We have some regressor of interest, $d$, and we want to estimate the effect of $d$ on $y$. We have a rich enough set of controls that we are willing to believe that $\\Er[\\epsilon|d,x] = 0$. $d_i$ and $y_i$ are scalars, while $x_i$ is a vector. We are not interested in $x$ per se, but we need to include it to avoid omitted variable bias. Typical applied econometric practice would be to choose some transfrom of $x$, say $X = T(x)$, where $X$ could be some subset of $x$, along with interactions, powers, and so on. Then estimate a linear regression y = \\theta d + X'\\beta + \\epsilon and then perhaps also report results for a handful of different choices of $T(x)$. Some downsides to the typical applied econometric practice include: The choice of T is arbitrary, which opens the door to specification searching and p-hacking. If $x$ is high dimensional, and $X$ is low dimensional, a poor choice will lead to omitted variable bias. Even if $x$ is low dimensional, if $f(x)$ is poorly approximated by $X\u2019\\beta$, there will be omitted variable bias. In some sense, machine learning can be thought of as a way to choose $T$ is an automated and data-driven way. There will be still be a choice of machine learning method and often tuning parameters for that method, so some arbitrary decisions remain. Hopefully though these decisions have less impact. You may already be familiar with traditional nonparametric econometric methods like series / sieves and kernels. These share much in common with machine learning. What makes machine learning different that traditional nonparametric methods? Machine learning methods appear to have better predictive performance, and arguably more practical data-driven methods to choose tuning parameters. Machine learning methods can deal with high dimensional $x$, while traditional nonparametric methods focus on situations with low dimensional $x$. Example : Effect of abortion on crime Donohue and Levitt ( 2001 ) estimate a regression of state crime rates on crime type relevant abortion rates and controls, y_{it} = \\theta a_{it} + x_{it}'\\beta + \\delta_i + \\gamma_t + \\epsilon_{it}. $a_{it}$ is a weighted average of lagged abortion rates in state $i$, with the weight on the $\\ell$th lag equal to the fraction of age $\\ell$ people who commit a given crime type. The covariates $x$ are the log of lagged prisoners per capita, the log of lagged police per capita, the unemployment rate, per-capita income, the poverty rate, AFDC generosity at time t\u2009\u2212\u200915, a dummy for concealed weapons law, and beer consumption per cap. Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ) reanalyze this setup using lasso to allow a more flexible specification of controls. They allow for many interactions and quadratic terms, leading to 284 controls. Example: Matching \u00b6 Binary treatment $d_i \\in {0,1}$ Potential outcomes $y_i(0), y_i(1)$, observe $y_i = y_i(d_i)$ Interested in average treatment effect : $\\theta = \\Er[y_i(1) - y_i(0)]$ Covariates $x_i$ Assume unconfoundedness : $d_i \\indep y_i(1), y_i(0) | x_i$ E.g. Connors et al. ( 1996 ) The partially linear and matching models are closely related. If the conditional mean independence assumption of the partially linear model is strengthing to conditional indepence then the partially linear model is a special case of the matching model with constant treatment effects, $y_i(1) - y_i(0) = \\theta$. Thus the matching model can be viewed as a generalization of the partially linear model that allows for treatment effect heterogeneity. Example: Matching \u00b6 Estimatable formulae for ATE : \\begin{align*} \\theta = & \\Er\\left[\\frac{y_i d_i}{\\Pr(d = 1 | x_i)} - \\frac{y_i (1-d_i)}{1-\\Pr(d=1|x_i)} \\right] \\\\ \\theta = & \\Er\\left[\\Er[y_i | d_i = 1, x_i] - \\Er[y_i | d_i = 0 , x_i]\\right] \\\\ \\theta = & \\Er\\left[ \\begin{array}{l} d_i \\frac{y_i - \\Er[y_i | d_i = 1, x_i]}{\\Pr(d=1|x_i)} - (1-d_i)\\frac{y_i - \\Er[y_i | d_i = 0, x_i]}{1-\\Pr(d=1|x_i)} + \\\\ + \\Er[y_i | d_i = 1, x_i] - \\Er[y_i | d_i = 0 , x_i]\\end{array}\\right] \\end{align*} All the expectations in these three formulae involve observable data. Thus, we can form an estimate of $\\theta$ be replacing the expectations and conditional expectations with appropriate estimators. For example, to use the first formula, we could estimate a logit model for the probability of treatment, \\hat{\\Pr}(d=1|x_i) = \\frac{e^{X_i' \\hat{\\beta}}}{1+e^{X_i'\\hat{\\beta}}} where, as above, $X$ is a some chosen transformation of $x_i$. Then we simply take an average to estimate $\\theta$. \\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i d_i}{\\hat{\\Pr}(d=1|x_i)} - \\frac{y_i(1-d_i)} {1-\\hat{\\Pr}(d=1|x_i)} As in the partially linear model, estimating the parameter of interest, $\\theta$, requires estimating a potentially high dimensional nuisance parameter, in this case $\\hat{\\Pr}(d=1|x)$. Similarly, the second expression would require estimating conditional expectations of $y$ as nuisance parameters. The third expression requires estimating both conditional expecations of $y$ and $d$. The third expression might appear needlessly complicated, but we will see later that it has some desirable properties that will make using it essential when very flexible machine learning estimators for the conditional expectations are used. The origin of the name \u201cmatching\u201d can be seen in the second expression. One way to estimate that expression would be to take each person in the treatment group, find someone with the same (or nearly the same) $x$ in the control group, difference the outcome of this matched pair, and then average over the whole sample. (Actually this gives the average treatment effect on the treated. For the ATE, you would also have to do the same with roles of the groups switched and average all the differences.) When $x$ is multi-dimensional, there is some ambiguity about what it means for two $x$ values to be nearly the same. An important insight of Rosenbaum and Rubin ( 1983 ) is that it is sufficient to match on the propensity score, $P(d=1|x)$, instead. Example: effectiveness of heart catheterization Connors et al. ( 1996 ) use matching to estimate the effectiveness of heart catheterization in critically ill patients. Their dataset contains 5735 patients and 72 covariates. Athey et al. ( 2017 ) reanalyze this data using a variety of machine learning methods. References: Imbens ( 2004 ) reviews the traditional econometric literature on matching. Imbens ( 2015 ) focuses on practical advice for matching and includes a brief mention of incorporating machine learning. Both the partially linear model and treatment effects model can be extended to situations with endogeneity and instrumental variables. Example: IV \u00b6 \\begin{align*} y_i = & \\theta d_i + f(x_i) + \\epsilon_i \\\\ d_i = & g(x_i, z_i) + u_i \\end{align*} Interested in $\\theta$ Assume $\\Er[\\epsilon|x,z] = 0$, $\\Er[u|x,z]=0$ Nuisance parameters $f()$, $g()$ E.g. Angrist and Krueger ( 1991 ) Most of the remarks about the partially linear model also apply here. Hartford et al. ( 2017 ) estimate a generalization of this model with $y_i = f(d_i, x_i) +\\epsilon$ using deep neural networks. Example : compulsory schooling and earnings Angrist and Krueger ( 1991 ) use quarter of birth as an instrument for years of schooling to estimate the effect of schooling on earnings. Since compulsory schooling laws typically specify a minimum age at which a person can leave school instead of a minimum years of schooling, people born at different times of the year can be required to complete one more or one less year of schooling. Compulsory schooling laws and their effect on attained schooling can vary with state and year. Hence, Angrist and Krueger ( 1991 ) considered specifying $g(x,z)$ as all interactions of quarter of birth, state, and year dummies. Having so many instruments leads to statistical problems with 2SLS. Example: LATE \u00b6 Binary instrumet $z_i \\in {0,1}$ Potential treatments $d_i(0), d_i(1) \\in {0,1}$, $d_i = d_i(Z_i)$ Potential outcomes $y_i(0), y_i(1)$, observe $y_i = y_i(d_i)$ Covariates $x_i$ $(y_i(1), y_i(0), d_i(1), d_i(0)) \\indep z_i | x_i$ Local average treatment effect: \\begin{align*} \\theta = & \\Er\\left[\\Er[y_i(1) - y_i(0) | x, d_i(1) > d_i(0)]\\right] \\\\ = & \\Er\\left[\\frac{\\Er[y|z=1,x] - \\Er[y|z=0,x]} {\\Er[d|z=1,x]-\\Er[d|z=0,x]} \\right] \\end{align*} See Abadie ( 2003 ). Belloni et al. ( 2017 ) analyze estimation of this model using Lasso and other machine learning methods. General setup \u00b6 Parameter of interest $\\theta \\in \\R^{d_\\theta}$ Nuisance parameter $\\eta \\in T$ Moment conditions \\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta} with $\\psi$ known Estimate $\\hat{\\eta}$ using some machine learning method Estimate $\\hat{\\theta}$ from \\En[\\psi(w_i;\\hat{\\theta},\\hat{\\eta}) ] = 0 We are following the setup and notation of Chernozhukov et al. ( 2018 ). As in the examples, the dimension of $\\theta$ is fixed and small. The dimension of $\\eta$ is large and might be increasing with sample size. $T$ is some normed vector space. Example: partially linear model \u00b6 y_i = \\theta_0 d_i + f_0(x_i) + \\epsilon_i Compare the estimates from $\\En[d_i(y_i - \\tilde{\\theta} d_i - \\hat{f}(x_i)) ] = 0$ and $\\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0$ where $m(x) = \\Er[d|x]$ and $\\mu(y) = \\Er[y|x]$ Example: partially linear model In the partially linear model, y_i = \\theta_0 d_i + f_0(x_i) + \\epsilon_i we can let $w_i = (y_i, x_i)$ and $\\eta = f$. There are a variety of candidates for $\\psi$. An obvious (but flawed) one is $\\psi(w_i; \\theta, \\eta) = (y_i - \\theta_0 d_i - f_0(x_i))d_i$. With this choice of $\\psi$, we have \\begin{align*} 0 = & \\En[d_i(y_i - \\hat{\\theta} d_i - \\hat{f}(x_i)) ] \\\\ \\hat{\\theta} = & \\En[d_i^2]^{-1} \\En[d_i (y_i - \\hat{f}(x_i))] \\\\ (\\hat{\\theta} - \\theta_0) = & \\En[d_i^2]^{-1} \\En[d_i \\epsilon_i] + \\En[d_i^2]^{-1} \\En[d_i (f_0(x_i) - \\hat{f}(x_i))] \\end{align*} The first term of this expression is quite promising. $d_i$ and $\\epsilon_i$ are both finite dimensional random variables, so a law of large numbers will apply to $\\En[d_i^2]$, and a central limit theorem would apply to $\\sqrt{n} \\En[d_i \\epsilon_i]$. Unfortunately, the second expression is problematic. To accomodate high dimensional $x$ and allow for flexible $f()$, machine learning estimators must introduce some sort of regularization to control variance. This regularization also introduces some bias. The bias generally vanishes, but at a slower than $\\sqrt{n}$ rate. Hence \\sqrt{n} \\En[d_i (f_0(x_i) - \\hat{f}(x_i))] \\to \\infty. To get around this problem, we must modify our estimate of $\\theta$. Let $m(x) = \\Er[d|x]$ and $\\mu(y) = \\Er[y|x]$. Let $\\hat{m}()$ and $\\hat{\\mu}()$ be some estimates. Then we can estimate $\\theta$ by partialling out: \\begin{align*} 0 = & \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] \\\\ \\hat{\\theta} = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i))] \\\\ (\\hat{\\theta} - \\theta_0) = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\left(\\En[(d_i - \\hat{m}(x_i))\\epsilon_i] + \\En[(d_i - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] \\right) \\\\ = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\left( a + b +c + d \\right) \\end{align*} where \\begin{align*} a = & \\En[(d_i -m(x_i))\\epsilon_i] \\\\ b = & \\En[(m(x_i)-\\hat{m}(x_i))\\epsilon_i] \\\\ c = & \\En[v_i(\\mu(x_i) - \\hat{\\mu}(x_i))] \\\\ d = & \\En[(m(x_i) - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] \\end{align*} with $v_i = d_i - \\Er[d_i | x_i]$. The term $a$ is well behaved and $\\sqrt{n}a \\leadsto N(0,\\Sigma)$ under standard conditions. Although terms $b$ and $c$ appear similar to the problematic term in the initial estimator, they are better behaved because $\\Er[v|x] = 0$ and $\\Er[\\epsilon|x] = 0$. This makes it possible, but difficult to show that $\\sqrt{n}b \\to_p = 0$ and $\\sqrt{n} c \\to_p = 0$, see e.g. Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ). However, the conditions on $\\hat{m}$ and $\\hat{\\mu}$ needed to show this are slightly restrictive, and appropriate conditions might not be known for all estimators. Chernozhukov et al. ( 2018 ) describe a sample splitting modification to $\\hat{\\theta}$ that allows $\\sqrt{n} b$ and $\\sqrt{n} c$ to vanish under weaker conditions (essentially the same rate condition as needed for $\\sqrt{n} d$ to vanish.) The last term, $d$, is a considerable improvement upon the first estimator. Instead of involving the error in one estimate, it now involes the product of the error in two estimates. By the Cauchy-Schwarz inequality, d \\leq \\sqrt{\\En[(m(x_i) - \\hat{m}(x_i))^2]} \\sqrt{\\En[(\\mu(x_i) - \\hat{\\mu}(x_i))^2]}. So if the estimates of $m$ and $\\mu$ converge at rates faster than $n^{-1/4}$, then $\\sqrt{n} d \\to_p 0$. This $n^{-1/4}$ rate is reached by many machine learning estimators. Lessons from the example \u00b6 Need an extra condition on moments \u2013 Neyman orthogonality \\partial \\eta \\Er[\\psi(W;\\theta_0,\\eta_0)](\\eta-\\eta_0) = 0 Want estimators faster than $n^{-1/4}$ in the prediction norm, \\sqrt{\\En[(\\hat{\\eta}(x_i) - \\eta(x_i))^2]} \\lesssim_P n^{-1/4} Also want estimators that satisfy something like \\sqrt{n} \\En[(\\eta(x_i)-\\hat{\\eta}(x_i))\\epsilon_i] = o_p(1) Sample splitting will make this easier References by topic \u00b6 Matching Imbens ( 2015 ) Imbens ( 2004 ) Surveys on machine learning in econometrics Athey and Imbens ( 2017 ) Mullainathan and Spiess ( 2017 ) Athey and Imbens ( 2018 ) Athey et al. ( 2017 ) Athey and Imbens ( 2015 ), Athey and Imbens ( 2018 ) Machine learning Breiman and others ( 2001 ) Friedman, Hastie, and Tibshirani ( 2009 ) James et al. ( 2013 ) Efron and Hastie ( 2016 ) Introduction to lasso Belloni and Chernozhukov ( 2011 ) Friedman, Hastie, and Tibshirani ( 2009 ) section 3.4 Chernozhukov, Hansen, and Spindler ( 2016 ) Introduction to random forests Friedman, Hastie, and Tibshirani ( 2009 ) section 9.2 Bold references are recommended reading. They are generally shorter and less technical than some of the others. Aspiring econometricians should read much more than just the bold references. Neyman orthogonalization Chernozhukov, Chetverikov, et al. ( 2017 ) Chernozhukov, Hansen, and Spindler ( 2015 ) Chernozhukov et al. ( 2018 ) Belloni et al. ( 2017 ) Lasso for causal inference Alexandre Belloni, Chernozhukov, and Hansen ( 2014 b ) Belloni et al. ( 2012 ) Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ) Chernozhukov, Goldman, et al. ( 2017 ) Chernozhukov, Hansen, and Spindler ( 2016 ) hdm R package Random forests for causal inference Athey, Tibshirani, and Wager ( 2016 ) Wager and Athey ( 2018 ) Tibshirani et al. ( 2018 ) grf R package Athey and Imbens ( 2016 ) There is considerable overlap among these categories. The papers listed under Neyman orthogonalization all include use of lasso and some include random forests. The papers on lasso all involve some use of orthogonalization. References [references] \u00b6 Abadie, Alberto. 2003. \u201cSemiparametric Instrumental Variable Estimation of Treatment Response Models.\u201d Journal of Econometrics 113 (2): 231\u201363. https://doi.org/https://doi.org/10.1016/S0304-4076(02)00201-4 . Angrist, Joshua D., and Alan B. Krueger. 1991. \u201cDoes Compulsory School Attendance Affect Schooling and Earnings?\u201d The Quarterly Journal of Economics 106 (4): pp. 979\u20131014. http://www.jstor.org/stable/2937954 . Athey, Susan, and Guido Imbens. 2015. \u201cLectures on Machine Learning.\u201d NBER Summer Institute. http://www.nber.org/econometrics_minicourse_2015/ . \u2014\u2014\u2014. 2016. \u201cRecursive Partitioning for Heterogeneous Causal Effects.\u201d Proceedings of the National Academy of Sciences 113 (27): 7353\u201360. https://doi.org/10.1073/pnas.1510489113 . \u2014\u2014\u2014. 2018. \u201cMachine Learning and Econometrics.\u201d AEA Continuing Education. https://www.aeaweb.org/conference/cont-ed/2018-webcasts . Athey, Susan, Guido Imbens, Thai Pham, and Stefan Wager. 2017. \u201cEstimating Average Treatment Effects: Supplementary Analyses and Remaining Challenges.\u201d American Economic Review 107 (5): 278\u201381. https://doi.org/10.1257/aer.p20171042 . Athey, Susan, and Guido W. Imbens. 2017. \u201cThe State of Applied Econometrics: Causality and Policy Evaluation.\u201d Journal of Economic Perspectives 31 (2): 3\u201332. https://doi.org/10.1257/jep.31.2.3 . Athey, Susan, Julie Tibshirani, and Stefan Wager. 2016. \u201cGeneralized Random Forests.\u201d https://arxiv.org/abs/1610.01271 . Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen. 2012. \u201cSparse Models and Methods for Optimal Instruments with an Application to Eminent Domain.\u201d Econometrica 80 (6): 2369\u20132429. https://doi.org/10.3982/ECTA9626 . Belloni, A., V. Chernozhukov, I. Fern\u00e1ndez-Val, and C. Hansen. 2017. \u201cProgram Evaluation and Causal Inference with High-Dimensional Data.\u201d Econometrica 85 (1): 233\u201398. https://doi.org/10.3982/ECTA12723 . Belloni, Alexandre, and Victor Chernozhukov. 2011. \u201cHigh Dimensional Sparse Econometric Models: An Introduction.\u201d In Inverse Problems and High-Dimensional Estimation: Stats in the Ch\u00e2teau Summer School, August 31 - September 4, 2009 , edited by Pierre Alquier, Eric Gautier, and Gilles Stoltz, 121\u201356. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-19989-9_3 . Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014a. \u201cInference on Treatment Effects After Selection Among High-Dimensional Controls\u2020.\u201d The Review of Economic Studies 81 (2): 608\u201350. https://doi.org/10.1093/restud/rdt044 . \u2014\u2014\u2014. 2014b. \u201cHigh-Dimensional Methods and Inference on Structural and Treatment Effects.\u201d Journal of Economic Perspectives 28 (2): 29\u201350. https://doi.org/10.1257/jep.28.2.29 . Breiman, Leo, and others. 2001. \u201cStatistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).\u201d Statistical Science 16 (3): 199\u2013231. https://projecteuclid.org/euclid.ss/1009213726 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, and Whitney Newey. 2017. \u201cDouble/Debiased/Neyman Machine Learning of Treatment Effects.\u201d American Economic Review 107 (5): 261\u201365. https://doi.org/10.1257/aer.p20171038 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Chernozhukov, Victor, Matt Goldman, Vira Semenova, and Matt Taddy. 2017. \u201cOrthogonal Machine Learning for Demand Estimation: High Dimensional Causal Inference in Dynamic Panels.\u201d https://arxiv.org/abs/1712.09988v2 . Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. \u201chdm: High-Dimensional Metrics.\u201d R Journal 8 (2): 185\u201399. https://journal.r-project.org/archive/2016/RJ-2016-040/index.html . Chernozhukov, Victor, Christian Hansen, and Martin Spindler. 2015. \u201cValid Post-Selection and Post-Regularization Inference: An Elementary, General Approach.\u201d Annual Review of Economics 7 (1): 649\u201388. https://doi.org/10.1146/annurev-economics-012315-015826 . Connors, Alfred F., Theodore Speroff, Neal V. Dawson, Charles Thomas, Frank E. Harrell Jr, Douglas Wagner, Norman Desbiens, et al. 1996. \u201cThe Effectiveness of Right Heart Catheterization in the Initial Care of Critically Ill Patients.\u201d JAMA 276 (11): 889\u201397. https://doi.org/10.1001/jama.1996.03540110043030 . Donohue, John J., III, and Steven D. Levitt. 2001. \u201cThe Impact of Legalized Abortion on Crime*.\u201d The Quarterly Journal of Economics 116 (2): 379\u2013420. https://doi.org/10.1162/00335530151144050 . Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference . Vol. 5. Cambridge University Press. https://web.stanford.edu/~hastie/CASI/ . Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning . Springer series in statistics. https://web.stanford.edu/~hastie/ElemStatLearn/ . Hartford, Jason, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. 2017. \u201cDeep IV: A Flexible Approach for Counterfactual Prediction.\u201d In Proceedings of the 34th International Conference on Machine Learning , edited by Doina Precup and Yee Whye Teh, 70:1414\u201323. Proceedings of Machine Learning Research. International Convention Centre, Sydney, Australia: PMLR. http://proceedings.mlr.press/v70/hartford17a.html . Imbens, Guido W. 2004. \u201cNonparametric Estimation of Average Treatment Effects Under Exogeneity: A Review.\u201d The Review of Economics and Statistics 86 (1): 4\u201329. https://doi.org/10.1162/003465304323023651 . \u2014\u2014\u2014. 2015. \u201cMatching Methods in Practice: Three Examples.\u201d Journal of Human Resources 50 (2): 373\u2013419. https://doi.org/10.3368/jhr.50.2.373 . James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning . Vol. 112. Springer. http://www-bcf.usc.edu/%7Egareth/ISL/ . Mullainathan, Sendhil, and Jann Spiess. 2017. \u201cMachine Learning: An Applied Econometric Approach.\u201d Journal of Economic Perspectives 31 (2): 87\u2013106. https://doi.org/10.1257/jep.31.2.87 . Rosenbaum, Paul R., and Donald B. Rubin. 1983. \u201cThe Central Role of the Propensity Score in Observational Studies for Causal Effects.\u201d Biometrika 70 (1): 41\u201355. https://doi.org/10.1093/biomet/70.1.41 . Tibshirani, Julie, Susan Athey, Stefan Wager, Rina Friedberg, Luke Miner, and Marvin Wright. 2018. Grf: Generalized Random Forests (Beta) . https://CRAN.R-project.org/package=grf . Wager, Stefan, and Susan Athey. 2018. \u201cEstimation and Inference of Heterogeneous Treatment Effects Using Random Forests.\u201d Journal of the American Statistical Association 0 (0): 1\u201315. https://doi.org/10.1080/01621459.2017.1319839 .","title":"Introduction"},{"location":"ml-intro/#introduction","text":"These notes will examine the incorportion of machine learning methods in classic econometric techniques for estimating causal effects. More specifally, we will focus on estimating treatment effects using matching and instrumental variables. In these estimators (and many others) there is a low-dimensional parameter of interest, such as the average treatment effect, but estimating it requires also estimating a potentially high dimensional nuisance parameter, such as the propensity score. Machine learning methods were developed for prediction with high dimensional data. It is then natural to try to use machine learning for estimating high dimensional nuisance parameters. Care must be taken when doing so though because the flexibility and complexity that make machine learning so good at prediction also pose challenges for inference.","title":"Introduction"},{"location":"ml-intro/#example-partially-linear-model","text":"y_i = \\theta d_i + f(x_i) + \\epsilon_i Interested in $\\theta$ Assume $\\Er[\\epsilon|d,x] = 0$ Nuisance parameter $f()$ E.g. Donohue and Levitt ( 2001 ) The simplest example of the setting we will analyze is a partially linear model. We have some regressor of interest, $d$, and we want to estimate the effect of $d$ on $y$. We have a rich enough set of controls that we are willing to believe that $\\Er[\\epsilon|d,x] = 0$. $d_i$ and $y_i$ are scalars, while $x_i$ is a vector. We are not interested in $x$ per se, but we need to include it to avoid omitted variable bias. Typical applied econometric practice would be to choose some transfrom of $x$, say $X = T(x)$, where $X$ could be some subset of $x$, along with interactions, powers, and so on. Then estimate a linear regression y = \\theta d + X'\\beta + \\epsilon and then perhaps also report results for a handful of different choices of $T(x)$. Some downsides to the typical applied econometric practice include: The choice of T is arbitrary, which opens the door to specification searching and p-hacking. If $x$ is high dimensional, and $X$ is low dimensional, a poor choice will lead to omitted variable bias. Even if $x$ is low dimensional, if $f(x)$ is poorly approximated by $X\u2019\\beta$, there will be omitted variable bias. In some sense, machine learning can be thought of as a way to choose $T$ is an automated and data-driven way. There will be still be a choice of machine learning method and often tuning parameters for that method, so some arbitrary decisions remain. Hopefully though these decisions have less impact. You may already be familiar with traditional nonparametric econometric methods like series / sieves and kernels. These share much in common with machine learning. What makes machine learning different that traditional nonparametric methods? Machine learning methods appear to have better predictive performance, and arguably more practical data-driven methods to choose tuning parameters. Machine learning methods can deal with high dimensional $x$, while traditional nonparametric methods focus on situations with low dimensional $x$. Example : Effect of abortion on crime Donohue and Levitt ( 2001 ) estimate a regression of state crime rates on crime type relevant abortion rates and controls, y_{it} = \\theta a_{it} + x_{it}'\\beta + \\delta_i + \\gamma_t + \\epsilon_{it}. $a_{it}$ is a weighted average of lagged abortion rates in state $i$, with the weight on the $\\ell$th lag equal to the fraction of age $\\ell$ people who commit a given crime type. The covariates $x$ are the log of lagged prisoners per capita, the log of lagged police per capita, the unemployment rate, per-capita income, the poverty rate, AFDC generosity at time t\u2009\u2212\u200915, a dummy for concealed weapons law, and beer consumption per cap. Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ) reanalyze this setup using lasso to allow a more flexible specification of controls. They allow for many interactions and quadratic terms, leading to 284 controls.","title":"Example: partially linear model"},{"location":"ml-intro/#example-matching","text":"Binary treatment $d_i \\in {0,1}$ Potential outcomes $y_i(0), y_i(1)$, observe $y_i = y_i(d_i)$ Interested in average treatment effect : $\\theta = \\Er[y_i(1) - y_i(0)]$ Covariates $x_i$ Assume unconfoundedness : $d_i \\indep y_i(1), y_i(0) | x_i$ E.g. Connors et al. ( 1996 ) The partially linear and matching models are closely related. If the conditional mean independence assumption of the partially linear model is strengthing to conditional indepence then the partially linear model is a special case of the matching model with constant treatment effects, $y_i(1) - y_i(0) = \\theta$. Thus the matching model can be viewed as a generalization of the partially linear model that allows for treatment effect heterogeneity.","title":"Example: Matching"},{"location":"ml-intro/#example-matching_1","text":"Estimatable formulae for ATE : \\begin{align*} \\theta = & \\Er\\left[\\frac{y_i d_i}{\\Pr(d = 1 | x_i)} - \\frac{y_i (1-d_i)}{1-\\Pr(d=1|x_i)} \\right] \\\\ \\theta = & \\Er\\left[\\Er[y_i | d_i = 1, x_i] - \\Er[y_i | d_i = 0 , x_i]\\right] \\\\ \\theta = & \\Er\\left[ \\begin{array}{l} d_i \\frac{y_i - \\Er[y_i | d_i = 1, x_i]}{\\Pr(d=1|x_i)} - (1-d_i)\\frac{y_i - \\Er[y_i | d_i = 0, x_i]}{1-\\Pr(d=1|x_i)} + \\\\ + \\Er[y_i | d_i = 1, x_i] - \\Er[y_i | d_i = 0 , x_i]\\end{array}\\right] \\end{align*} All the expectations in these three formulae involve observable data. Thus, we can form an estimate of $\\theta$ be replacing the expectations and conditional expectations with appropriate estimators. For example, to use the first formula, we could estimate a logit model for the probability of treatment, \\hat{\\Pr}(d=1|x_i) = \\frac{e^{X_i' \\hat{\\beta}}}{1+e^{X_i'\\hat{\\beta}}} where, as above, $X$ is a some chosen transformation of $x_i$. Then we simply take an average to estimate $\\theta$. \\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i d_i}{\\hat{\\Pr}(d=1|x_i)} - \\frac{y_i(1-d_i)} {1-\\hat{\\Pr}(d=1|x_i)} As in the partially linear model, estimating the parameter of interest, $\\theta$, requires estimating a potentially high dimensional nuisance parameter, in this case $\\hat{\\Pr}(d=1|x)$. Similarly, the second expression would require estimating conditional expectations of $y$ as nuisance parameters. The third expression requires estimating both conditional expecations of $y$ and $d$. The third expression might appear needlessly complicated, but we will see later that it has some desirable properties that will make using it essential when very flexible machine learning estimators for the conditional expectations are used. The origin of the name \u201cmatching\u201d can be seen in the second expression. One way to estimate that expression would be to take each person in the treatment group, find someone with the same (or nearly the same) $x$ in the control group, difference the outcome of this matched pair, and then average over the whole sample. (Actually this gives the average treatment effect on the treated. For the ATE, you would also have to do the same with roles of the groups switched and average all the differences.) When $x$ is multi-dimensional, there is some ambiguity about what it means for two $x$ values to be nearly the same. An important insight of Rosenbaum and Rubin ( 1983 ) is that it is sufficient to match on the propensity score, $P(d=1|x)$, instead. Example: effectiveness of heart catheterization Connors et al. ( 1996 ) use matching to estimate the effectiveness of heart catheterization in critically ill patients. Their dataset contains 5735 patients and 72 covariates. Athey et al. ( 2017 ) reanalyze this data using a variety of machine learning methods. References: Imbens ( 2004 ) reviews the traditional econometric literature on matching. Imbens ( 2015 ) focuses on practical advice for matching and includes a brief mention of incorporating machine learning. Both the partially linear model and treatment effects model can be extended to situations with endogeneity and instrumental variables.","title":"Example: Matching"},{"location":"ml-intro/#example-iv","text":"\\begin{align*} y_i = & \\theta d_i + f(x_i) + \\epsilon_i \\\\ d_i = & g(x_i, z_i) + u_i \\end{align*} Interested in $\\theta$ Assume $\\Er[\\epsilon|x,z] = 0$, $\\Er[u|x,z]=0$ Nuisance parameters $f()$, $g()$ E.g. Angrist and Krueger ( 1991 ) Most of the remarks about the partially linear model also apply here. Hartford et al. ( 2017 ) estimate a generalization of this model with $y_i = f(d_i, x_i) +\\epsilon$ using deep neural networks. Example : compulsory schooling and earnings Angrist and Krueger ( 1991 ) use quarter of birth as an instrument for years of schooling to estimate the effect of schooling on earnings. Since compulsory schooling laws typically specify a minimum age at which a person can leave school instead of a minimum years of schooling, people born at different times of the year can be required to complete one more or one less year of schooling. Compulsory schooling laws and their effect on attained schooling can vary with state and year. Hence, Angrist and Krueger ( 1991 ) considered specifying $g(x,z)$ as all interactions of quarter of birth, state, and year dummies. Having so many instruments leads to statistical problems with 2SLS.","title":"Example: IV"},{"location":"ml-intro/#example-late","text":"Binary instrumet $z_i \\in {0,1}$ Potential treatments $d_i(0), d_i(1) \\in {0,1}$, $d_i = d_i(Z_i)$ Potential outcomes $y_i(0), y_i(1)$, observe $y_i = y_i(d_i)$ Covariates $x_i$ $(y_i(1), y_i(0), d_i(1), d_i(0)) \\indep z_i | x_i$ Local average treatment effect: \\begin{align*} \\theta = & \\Er\\left[\\Er[y_i(1) - y_i(0) | x, d_i(1) > d_i(0)]\\right] \\\\ = & \\Er\\left[\\frac{\\Er[y|z=1,x] - \\Er[y|z=0,x]} {\\Er[d|z=1,x]-\\Er[d|z=0,x]} \\right] \\end{align*} See Abadie ( 2003 ). Belloni et al. ( 2017 ) analyze estimation of this model using Lasso and other machine learning methods.","title":"Example: LATE"},{"location":"ml-intro/#general-setup","text":"Parameter of interest $\\theta \\in \\R^{d_\\theta}$ Nuisance parameter $\\eta \\in T$ Moment conditions \\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta} with $\\psi$ known Estimate $\\hat{\\eta}$ using some machine learning method Estimate $\\hat{\\theta}$ from \\En[\\psi(w_i;\\hat{\\theta},\\hat{\\eta}) ] = 0 We are following the setup and notation of Chernozhukov et al. ( 2018 ). As in the examples, the dimension of $\\theta$ is fixed and small. The dimension of $\\eta$ is large and might be increasing with sample size. $T$ is some normed vector space.","title":"General setup"},{"location":"ml-intro/#example-partially-linear-model_1","text":"y_i = \\theta_0 d_i + f_0(x_i) + \\epsilon_i Compare the estimates from $\\En[d_i(y_i - \\tilde{\\theta} d_i - \\hat{f}(x_i)) ] = 0$ and $\\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0$ where $m(x) = \\Er[d|x]$ and $\\mu(y) = \\Er[y|x]$ Example: partially linear model In the partially linear model, y_i = \\theta_0 d_i + f_0(x_i) + \\epsilon_i we can let $w_i = (y_i, x_i)$ and $\\eta = f$. There are a variety of candidates for $\\psi$. An obvious (but flawed) one is $\\psi(w_i; \\theta, \\eta) = (y_i - \\theta_0 d_i - f_0(x_i))d_i$. With this choice of $\\psi$, we have \\begin{align*} 0 = & \\En[d_i(y_i - \\hat{\\theta} d_i - \\hat{f}(x_i)) ] \\\\ \\hat{\\theta} = & \\En[d_i^2]^{-1} \\En[d_i (y_i - \\hat{f}(x_i))] \\\\ (\\hat{\\theta} - \\theta_0) = & \\En[d_i^2]^{-1} \\En[d_i \\epsilon_i] + \\En[d_i^2]^{-1} \\En[d_i (f_0(x_i) - \\hat{f}(x_i))] \\end{align*} The first term of this expression is quite promising. $d_i$ and $\\epsilon_i$ are both finite dimensional random variables, so a law of large numbers will apply to $\\En[d_i^2]$, and a central limit theorem would apply to $\\sqrt{n} \\En[d_i \\epsilon_i]$. Unfortunately, the second expression is problematic. To accomodate high dimensional $x$ and allow for flexible $f()$, machine learning estimators must introduce some sort of regularization to control variance. This regularization also introduces some bias. The bias generally vanishes, but at a slower than $\\sqrt{n}$ rate. Hence \\sqrt{n} \\En[d_i (f_0(x_i) - \\hat{f}(x_i))] \\to \\infty. To get around this problem, we must modify our estimate of $\\theta$. Let $m(x) = \\Er[d|x]$ and $\\mu(y) = \\Er[y|x]$. Let $\\hat{m}()$ and $\\hat{\\mu}()$ be some estimates. Then we can estimate $\\theta$ by partialling out: \\begin{align*} 0 = & \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] \\\\ \\hat{\\theta} = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i))] \\\\ (\\hat{\\theta} - \\theta_0) = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\left(\\En[(d_i - \\hat{m}(x_i))\\epsilon_i] + \\En[(d_i - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] \\right) \\\\ = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\left( a + b +c + d \\right) \\end{align*} where \\begin{align*} a = & \\En[(d_i -m(x_i))\\epsilon_i] \\\\ b = & \\En[(m(x_i)-\\hat{m}(x_i))\\epsilon_i] \\\\ c = & \\En[v_i(\\mu(x_i) - \\hat{\\mu}(x_i))] \\\\ d = & \\En[(m(x_i) - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] \\end{align*} with $v_i = d_i - \\Er[d_i | x_i]$. The term $a$ is well behaved and $\\sqrt{n}a \\leadsto N(0,\\Sigma)$ under standard conditions. Although terms $b$ and $c$ appear similar to the problematic term in the initial estimator, they are better behaved because $\\Er[v|x] = 0$ and $\\Er[\\epsilon|x] = 0$. This makes it possible, but difficult to show that $\\sqrt{n}b \\to_p = 0$ and $\\sqrt{n} c \\to_p = 0$, see e.g. Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ). However, the conditions on $\\hat{m}$ and $\\hat{\\mu}$ needed to show this are slightly restrictive, and appropriate conditions might not be known for all estimators. Chernozhukov et al. ( 2018 ) describe a sample splitting modification to $\\hat{\\theta}$ that allows $\\sqrt{n} b$ and $\\sqrt{n} c$ to vanish under weaker conditions (essentially the same rate condition as needed for $\\sqrt{n} d$ to vanish.) The last term, $d$, is a considerable improvement upon the first estimator. Instead of involving the error in one estimate, it now involes the product of the error in two estimates. By the Cauchy-Schwarz inequality, d \\leq \\sqrt{\\En[(m(x_i) - \\hat{m}(x_i))^2]} \\sqrt{\\En[(\\mu(x_i) - \\hat{\\mu}(x_i))^2]}. So if the estimates of $m$ and $\\mu$ converge at rates faster than $n^{-1/4}$, then $\\sqrt{n} d \\to_p 0$. This $n^{-1/4}$ rate is reached by many machine learning estimators.","title":"Example: partially linear model"},{"location":"ml-intro/#lessons-from-the-example","text":"Need an extra condition on moments \u2013 Neyman orthogonality \\partial \\eta \\Er[\\psi(W;\\theta_0,\\eta_0)](\\eta-\\eta_0) = 0 Want estimators faster than $n^{-1/4}$ in the prediction norm, \\sqrt{\\En[(\\hat{\\eta}(x_i) - \\eta(x_i))^2]} \\lesssim_P n^{-1/4} Also want estimators that satisfy something like \\sqrt{n} \\En[(\\eta(x_i)-\\hat{\\eta}(x_i))\\epsilon_i] = o_p(1) Sample splitting will make this easier","title":"Lessons from the example"},{"location":"ml-intro/#references-by-topic","text":"Matching Imbens ( 2015 ) Imbens ( 2004 ) Surveys on machine learning in econometrics Athey and Imbens ( 2017 ) Mullainathan and Spiess ( 2017 ) Athey and Imbens ( 2018 ) Athey et al. ( 2017 ) Athey and Imbens ( 2015 ), Athey and Imbens ( 2018 ) Machine learning Breiman and others ( 2001 ) Friedman, Hastie, and Tibshirani ( 2009 ) James et al. ( 2013 ) Efron and Hastie ( 2016 ) Introduction to lasso Belloni and Chernozhukov ( 2011 ) Friedman, Hastie, and Tibshirani ( 2009 ) section 3.4 Chernozhukov, Hansen, and Spindler ( 2016 ) Introduction to random forests Friedman, Hastie, and Tibshirani ( 2009 ) section 9.2 Bold references are recommended reading. They are generally shorter and less technical than some of the others. Aspiring econometricians should read much more than just the bold references. Neyman orthogonalization Chernozhukov, Chetverikov, et al. ( 2017 ) Chernozhukov, Hansen, and Spindler ( 2015 ) Chernozhukov et al. ( 2018 ) Belloni et al. ( 2017 ) Lasso for causal inference Alexandre Belloni, Chernozhukov, and Hansen ( 2014 b ) Belloni et al. ( 2012 ) Alexandre Belloni, Chernozhukov, and Hansen ( 2014 a ) Chernozhukov, Goldman, et al. ( 2017 ) Chernozhukov, Hansen, and Spindler ( 2016 ) hdm R package Random forests for causal inference Athey, Tibshirani, and Wager ( 2016 ) Wager and Athey ( 2018 ) Tibshirani et al. ( 2018 ) grf R package Athey and Imbens ( 2016 ) There is considerable overlap among these categories. The papers listed under Neyman orthogonalization all include use of lasso and some include random forests. The papers on lasso all involve some use of orthogonalization.","title":"References by topic"},{"location":"ml-intro/#references-references","text":"Abadie, Alberto. 2003. \u201cSemiparametric Instrumental Variable Estimation of Treatment Response Models.\u201d Journal of Econometrics 113 (2): 231\u201363. https://doi.org/https://doi.org/10.1016/S0304-4076(02)00201-4 . Angrist, Joshua D., and Alan B. Krueger. 1991. \u201cDoes Compulsory School Attendance Affect Schooling and Earnings?\u201d The Quarterly Journal of Economics 106 (4): pp. 979\u20131014. http://www.jstor.org/stable/2937954 . Athey, Susan, and Guido Imbens. 2015. \u201cLectures on Machine Learning.\u201d NBER Summer Institute. http://www.nber.org/econometrics_minicourse_2015/ . \u2014\u2014\u2014. 2016. \u201cRecursive Partitioning for Heterogeneous Causal Effects.\u201d Proceedings of the National Academy of Sciences 113 (27): 7353\u201360. https://doi.org/10.1073/pnas.1510489113 . \u2014\u2014\u2014. 2018. \u201cMachine Learning and Econometrics.\u201d AEA Continuing Education. https://www.aeaweb.org/conference/cont-ed/2018-webcasts . Athey, Susan, Guido Imbens, Thai Pham, and Stefan Wager. 2017. \u201cEstimating Average Treatment Effects: Supplementary Analyses and Remaining Challenges.\u201d American Economic Review 107 (5): 278\u201381. https://doi.org/10.1257/aer.p20171042 . Athey, Susan, and Guido W. Imbens. 2017. \u201cThe State of Applied Econometrics: Causality and Policy Evaluation.\u201d Journal of Economic Perspectives 31 (2): 3\u201332. https://doi.org/10.1257/jep.31.2.3 . Athey, Susan, Julie Tibshirani, and Stefan Wager. 2016. \u201cGeneralized Random Forests.\u201d https://arxiv.org/abs/1610.01271 . Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen. 2012. \u201cSparse Models and Methods for Optimal Instruments with an Application to Eminent Domain.\u201d Econometrica 80 (6): 2369\u20132429. https://doi.org/10.3982/ECTA9626 . Belloni, A., V. Chernozhukov, I. Fern\u00e1ndez-Val, and C. Hansen. 2017. \u201cProgram Evaluation and Causal Inference with High-Dimensional Data.\u201d Econometrica 85 (1): 233\u201398. https://doi.org/10.3982/ECTA12723 . Belloni, Alexandre, and Victor Chernozhukov. 2011. \u201cHigh Dimensional Sparse Econometric Models: An Introduction.\u201d In Inverse Problems and High-Dimensional Estimation: Stats in the Ch\u00e2teau Summer School, August 31 - September 4, 2009 , edited by Pierre Alquier, Eric Gautier, and Gilles Stoltz, 121\u201356. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-19989-9_3 . Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014a. \u201cInference on Treatment Effects After Selection Among High-Dimensional Controls\u2020.\u201d The Review of Economic Studies 81 (2): 608\u201350. https://doi.org/10.1093/restud/rdt044 . \u2014\u2014\u2014. 2014b. \u201cHigh-Dimensional Methods and Inference on Structural and Treatment Effects.\u201d Journal of Economic Perspectives 28 (2): 29\u201350. https://doi.org/10.1257/jep.28.2.29 . Breiman, Leo, and others. 2001. \u201cStatistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).\u201d Statistical Science 16 (3): 199\u2013231. https://projecteuclid.org/euclid.ss/1009213726 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, and Whitney Newey. 2017. \u201cDouble/Debiased/Neyman Machine Learning of Treatment Effects.\u201d American Economic Review 107 (5): 261\u201365. https://doi.org/10.1257/aer.p20171038 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Chernozhukov, Victor, Matt Goldman, Vira Semenova, and Matt Taddy. 2017. \u201cOrthogonal Machine Learning for Demand Estimation: High Dimensional Causal Inference in Dynamic Panels.\u201d https://arxiv.org/abs/1712.09988v2 . Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. \u201chdm: High-Dimensional Metrics.\u201d R Journal 8 (2): 185\u201399. https://journal.r-project.org/archive/2016/RJ-2016-040/index.html . Chernozhukov, Victor, Christian Hansen, and Martin Spindler. 2015. \u201cValid Post-Selection and Post-Regularization Inference: An Elementary, General Approach.\u201d Annual Review of Economics 7 (1): 649\u201388. https://doi.org/10.1146/annurev-economics-012315-015826 . Connors, Alfred F., Theodore Speroff, Neal V. Dawson, Charles Thomas, Frank E. Harrell Jr, Douglas Wagner, Norman Desbiens, et al. 1996. \u201cThe Effectiveness of Right Heart Catheterization in the Initial Care of Critically Ill Patients.\u201d JAMA 276 (11): 889\u201397. https://doi.org/10.1001/jama.1996.03540110043030 . Donohue, John J., III, and Steven D. Levitt. 2001. \u201cThe Impact of Legalized Abortion on Crime*.\u201d The Quarterly Journal of Economics 116 (2): 379\u2013420. https://doi.org/10.1162/00335530151144050 . Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference . Vol. 5. Cambridge University Press. https://web.stanford.edu/~hastie/CASI/ . Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning . Springer series in statistics. https://web.stanford.edu/~hastie/ElemStatLearn/ . Hartford, Jason, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. 2017. \u201cDeep IV: A Flexible Approach for Counterfactual Prediction.\u201d In Proceedings of the 34th International Conference on Machine Learning , edited by Doina Precup and Yee Whye Teh, 70:1414\u201323. Proceedings of Machine Learning Research. International Convention Centre, Sydney, Australia: PMLR. http://proceedings.mlr.press/v70/hartford17a.html . Imbens, Guido W. 2004. \u201cNonparametric Estimation of Average Treatment Effects Under Exogeneity: A Review.\u201d The Review of Economics and Statistics 86 (1): 4\u201329. https://doi.org/10.1162/003465304323023651 . \u2014\u2014\u2014. 2015. \u201cMatching Methods in Practice: Three Examples.\u201d Journal of Human Resources 50 (2): 373\u2013419. https://doi.org/10.3368/jhr.50.2.373 . James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning . Vol. 112. Springer. http://www-bcf.usc.edu/%7Egareth/ISL/ . Mullainathan, Sendhil, and Jann Spiess. 2017. \u201cMachine Learning: An Applied Econometric Approach.\u201d Journal of Economic Perspectives 31 (2): 87\u2013106. https://doi.org/10.1257/jep.31.2.87 . Rosenbaum, Paul R., and Donald B. Rubin. 1983. \u201cThe Central Role of the Propensity Score in Observational Studies for Causal Effects.\u201d Biometrika 70 (1): 41\u201355. https://doi.org/10.1093/biomet/70.1.41 . Tibshirani, Julie, Susan Athey, Stefan Wager, Rina Friedberg, Luke Miner, and Marvin Wright. 2018. Grf: Generalized Random Forests (Beta) . https://CRAN.R-project.org/package=grf . Wager, Stefan, and Susan Athey. 2018. \u201cEstimation and Inference of Heterogeneous Treatment Effects Using Random Forests.\u201d Journal of the American Statistical Association 0 (0): 1\u201315. https://doi.org/10.1080/01621459.2017.1319839 .","title":"References [references]"},{"location":"ml-julia/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 This document is a companion to my \u201cMachine learning in economics\u201d . Those notes discuss the recent use of machine learning in economics, with a focus on lasso and random forests. The code in those notes is written in R. This document will look at similar code in Julia. RCall \u00b6 If you want to use the methods of Chernozhukov and coauthors implements in the R packaga Chernozhukov, Hansen, and Spindler ( 2016 ) or the methods of Athey and coauthors implemented in the R package Tibshirani et al. ( 2018 ) , then it makes sense to use the R pacakge. You could simply write all your code in R. However, if you prefer using Julia, you can just call the necessary R functions with RCall.jl . Here, we load the pipeline data used in the machine learning methods notes , and do some cleaning in Julia. using RCall, DataFrames, Missings, Statistics R\"load(paste($(docdir),\\\"/rmd/pipelines.Rdata\\\",sep=\\\"\\\"))\" println(R\"ls()\") RObject{StrSxp} [1] \"#JL\" \"data\" data = @rget data # data on left is new Julia variable, data on right is the one in R println(R\"summary(data[,1:5])\") RObject{StrSxp} respondent_id report_yr report_prd major Min. : 1.0 Min. :1991 Min. :12 Mode :logical 1st Qu.: 64.0 1st Qu.:1997 1st Qu.:12 FALSE:1192 Median :148.0 Median :2003 Median :12 TRUE :2797 Mean :184.3 Mean :2003 Mean :12 NA's :2180 3rd Qu.:214.0 3rd Qu.:2010 3rd Qu.:12 Max. :745.0 Max. :2016 Max. :12 NA's :3371 respondent_name Centra Pipelines Minnesota Inc. : 22 Tuscarora Gas Transmission Company : 22 Eastern Shore Natural Gas Company : 22 Kern River Gas Transmission Company : 21 National Fuel Gas Supply Corporation: 21 (Other) :2938 NA's :3123 println(describe(data[:,1:5])) 5\u00d78 DataFrame \u2502 Row \u2502 variable \u2502 mean \u2502 min \u2502 m edian \u2502 max \u2502 nunique \u2502 nmissing \u2502 eltype \u2502 \u2502 \u2502 Symbol \u2502 Union\u2026 \u2502 Any \u2502 U nion\u2026 \u2502 Any \u2502 Union\u2026 \u2502 Union\u2026 \u2502 Type \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 1 \u2502 respondent_id \u2502 184.3 \u2502 1 \u2502 1 48.0 \u2502 745 \u2502 \u2502 \u2502 Int64 \u2502 \u2502 2 \u2502 report_yr \u2502 2003.49 \u2502 1991 \u2502 2 003.0 \u2502 2016 \u2502 \u2502 \u2502 Int64 \u2502 \u2502 3 \u2502 report_prd \u2502 12.0 \u2502 12 \u2502 1 2.0 \u2502 12 \u2502 \u2502 3371 \u2502 Union{Missing, Int64} \u2502 \u2502 4 \u2502 major \u2502 0.701178 \u2502 0 \u2502 1 .0 \u2502 1 \u2502 \u2502 2180 \u2502 Union{Missing, Bool} \u2502 \u2502 5 \u2502 respondent_name \u2502 \u2502 Algonquin Gas Transmission Company \u2502 \u2502 Southern Natural Gas Company \u2502 440 \u2502 3123 \u2502 Union{Missing, CategoricalString{UInt32}} \u2502 for c in 59:107 # columns of state mileage, want missing->0 replace!(x->(ismissing(x) || isnan(x)) ? 0.0 : x, data[!,c]) end println(describe(data[:,59:65])) 7\u00d78 DataFrame \u2502 Row \u2502 variable \u2502 mean \u2502 min \u2502 median \u2502 max \u2502 nuniqu e \u2502 nmissing \u2502 eltype \u2502 \u2502 \u2502 Symbol \u2502 Float64 \u2502 Float64 \u2502 Float64 \u2502 Float64 \u2502 Nothin g \u2502 Int64 \u2502 Union \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 1 \u2502 North Carolina \u2502 0.00358525 \u2502 0.0 \u2502 0.0 \u2502 1.0 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 2 \u2502 Tennessee \u2502 0.0061488 \u2502 0.0 \u2502 0.0 \u2502 0.635202 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 3 \u2502 Virginia \u2502 0.00552028 \u2502 0.0 \u2502 0.0 \u2502 1.0 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 4 \u2502 Illinois \u2502 0.0134891 \u2502 0.0 \u2502 0.0 \u2502 1.0 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 5 \u2502 Indiana \u2502 0.0058707 \u2502 0.0 \u2502 0.0 \u2502 0.550302 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 6 \u2502 Kentucky \u2502 0.0133474 \u2502 0.0 \u2502 0.0 \u2502 1.0 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 7 \u2502 Gulf of Mexico \u2502 0.0140981 \u2502 0.0 \u2502 0.0 \u2502 0.825409 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 Suppose we want to estimate the coefficient on transPlant (capital) in a partially linear model with transProfit (profit) as the outcome. This can be done with the R function hdm::rlassoEffects . R\"library(hdm)\" completedata = dropmissing(data,[1:10..., 59:122...], disallowmissing=true) y = completedata[:transProfit] inc = .!isnan.(y) y = y[inc] X = completedata[inc,[6:7..., 59:121...]] cols = [std(X[c])>0 for c in 1:ncol(X)] X = X[:,cols] est = R\"rlassoEffects($(X), $(y), index=c(1:2))\" R\"summary($est)\" RObject{VecSxp} [1] \"Estimates and significance testing of the effect of target variables\" Estimate. Std. Error t value Pr(>|t|) transPlant_bal_end_yr 0.034434 0.008878 3.879 0.000105 *** transPlant_bal_beg_yr 0.086580 0.009383 9.228 < 2e-16 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 MLJ.jl \u00b6 MLJ.jl is a machine learning framework for Julia. It gives a unified interface for many machine learning algorithms and tasks. Similar R packages include caret and MLR . scikit-learn is a similar Python package. For more information on MLJ see MLJ.jl docs MLJ tutorials You can see a list of models registered to work with MLJ.jl on github , or by calling MLJ::models() . using MLJ models() 109-element Array{NamedTuple,1}: (name = ARDRegressor, package_name = ScikitLearn, ... ) (name = AdaBoostClassifier, package_name = ScikitLearn, ... ) (name = AdaBoostRegressor, package_name = ScikitLearn, ... ) (name = BaggingClassifier, package_name = ScikitLearn, ... ) (name = BaggingRegressor, package_name = ScikitLearn, ... ) (name = BayesianLDA, package_name = MultivariateStats, ... ) (name = BayesianLDA, package_name = ScikitLearn, ... ) (name = BayesianQDA, package_name = ScikitLearn, ... ) (name = BayesianRidgeRegressor, package_name = ScikitLearn, ... ) (name = BernoulliNBClassifier, package_name = ScikitLearn, ... ) \u22ee (name = Standardizer, package_name = MLJModels, ... ) (name = StaticTransformer, package_name = MLJModels, ... ) (name = TheilSenRegressor, package_name = ScikitLearn, ... ) (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... ) (name = UnivariateDiscretizer, package_name = MLJModels, ... ) (name = UnivariateStandardizer, package_name = MLJModels, ... ) (name = XGBoostClassifier, package_name = XGBoost, ... ) (name = XGBoostCount, package_name = XGBoost, ... ) (name = XGBoostRegressor, package_name = XGBoost, ... ) To use these models, you need the corresponding package to be installed and loaded. The @load macro will load the needed package(s) for any model. lasso_model = @load LassoRegressor pkg=MLJLinearModels LassoRegressor(lambda = 1.0, fit_intercept = true, penalize_intercept = false, solver = nothing,) @ 9\u202676 Let\u2019s fit lasso to the same pipeline data as above. lasso_model.lambda = 1.0 lasso = machine(lasso_model, X, y) train,test = partition(eachindex(y), 0.6, shuffle=true) fit!(lasso, rows=train) yhat = predict(lasso, rows=test) println(yhat[1:10]) [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") MSE/var(y) = 1.4769678684316065 That doesn\u2019t look very good. All the predictions are zero. This could happen when the regularization parameter, lambda , is too large. However, in this case the problem is something else. The warning messages indicate numeric problems when minimizing the lasso objective function. This can happen when X is poorly scaled. The algorithm used to compute the lasso estimates works best when the coefficients are all roughly the same scale. The existing X \u2019s have wildly different scales, which causes problems. This situation is common, so MLJ.jl has functions to standardize variables. It is likely that the hdm package in R does something similar internally. lasso_stdx = @pipeline PipeLasso(std=Standardizer(), lasso=LassoRegressor(lambda=1.0*std(y), solver=MLJLinearModels.ISTA(max_iter=10000) ) ) m = machine(lasso_stdx, X, y) fit!(m, rows=train) yhat = predict(m , rows=test) println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") MSE/var(y) = 0.1376605360336052 # Get the fitted coefficients coef = fitted_params(m).fitted_params[1].coefs intercept = fitted_params(m).fitted_params[1].intercept sum(abs.(coef).>1e-8) # number non-zero 52 If we want to tune lambda using cross-validation, we can use the range and TunedModel functions. r = range(lasso_stdx, :(lasso.lambda), lower=1e1, upper=1e10, scale=:log) t=TunedModel(model=lasso_stdx, resampling=CV(nfolds=5), tuning=Grid(resolution=10), ranges=r, measure=rms) m = machine(t, X, y) fit!(m, rows=train, verbosity=1) yhat = predict(m , rows=test) println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") MSE/var(y) = 0.1286305685499512 using Plots cvmse = m.report.measurements \u03bb = Float64.(m.report.parameter_values[:]) plot(\u03bb, cvmse, xlab=\"\u03bb\", ylab=\"CV(MSE)\") Flux.jl \u00b6 Flux.jl is another Julia package for machine learning. It seems to be emerging as the leading Julia package for neural networks and deep learning, but other machine learning models can also be implemented using Flux.jl . Let\u2019s create a lasso model in Flux.jl . using Flux, LinearAlgebra # Scale the variables Xstd = Flux.normalise(Matrix(X)) X_train = Xstd[train,:] X_test = Xstd[test,:] yscale = std(y) ymean = mean(y) ystd = (y .- ymean)./yscale y_train = ystd[train] y_test = ystd[test] # Set up the model parameters and initial values \u03b2ols = (X_train'*X_train) \\ (X_train'*(y_train .- mean(y_train))) \u03b2 = param(zeros(ncol(X))) #\u03b2ols) #zeros(ncol(X))) b = param([mean(y_train)]) \u03b8 = Tracker.Params([\u03b2,b]) # Define the loss function \u03c8 = ones(length(\u03b2)) \u03bb = 2.0 pred(x) = b .+ x*\u03b2 mse(x,y) = mean( (pred(x) .- y).^2 ) penalty(y) = \u03bb/length(y)*norm(\u03c8.*\u03b2,1) loss(x,y) = mse(x,y) + penalty(y) @show loss(X_train,y_train) loss(X_train, y_train) = 1.0694237073658825 (tracked) # minimize loss maxiter=2000 obj = zeros(maxiter) mse_train = zeros(maxiter) mse_test = zeros(maxiter) for i in 1:maxiter Flux.train!(loss, \u03b8, [(X_train, y_train)], Flux.AMSGrad()) mse_train[i] = Tracker.data(mse(X_train,y_train)) mse_test[i] = Tracker.data(mse(X_test, y_test)) obj[i] = Tracker.data(loss(X_train,y_train)) end lo = 1 hi = 250 plot(obj[lo:hi], ylab=\"Loss=MSE + \u03bb/n*||\u03b2||\u2081\", xlab=\"Iteration\") plot(lo:hi, [mse_train[lo:hi] mse_test[lo:hi]], ylab=\"MSE\", xaxis=(\"Iteration\") , lab=[\"Train\" \"Test\"]) The minimization methods in Flux.train! are all variants of gradient descent. Each call to Flux.train! runs one iteration of the specified solver. To find a locaol minimum, Flux.train! can be called repeatedly until progress stops. The above loop is a simple way to do this. The @epoch macro can also be useful. Gradient descent works well for neural networks, but are is ideal for Lasso. Without further adjustment, gradient descent gets stuck in a cycle as jumps from one side of the other of the absolute value in the lasso penalty. Nonetheless, the results are near the true minimum, even though it never exactly gets there. Additional Resources \u00b6 Klok and Nazarathy ( 2019 ) Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence References [references] \u00b6 Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. \u201chdm: High-Dimensional Metrics.\u201d R Journal 8 (2): 185\u201399. https://journal.r-project.org/archive/2016/RJ-2016-040/index.html . Klok, Hayden, and Yoni Nazarathy. 2019. Statistics with Julia:Fundamentals for Data Science, Machinelearning and Artificial Intelligence . DRAFT. https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf . Tibshirani, Julie, Susan Athey, Stefan Wager, Rina Friedberg, Luke Miner, and Marvin Wright. 2018. Grf: Generalized Random Forests (Beta) . https://CRAN.R-project.org/package=grf .","title":"With Julia"},{"location":"ml-julia/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min}","title":"About this document"},{"location":"ml-julia/#introduction","text":"This document is a companion to my \u201cMachine learning in economics\u201d . Those notes discuss the recent use of machine learning in economics, with a focus on lasso and random forests. The code in those notes is written in R. This document will look at similar code in Julia.","title":"Introduction"},{"location":"ml-julia/#rcall","text":"If you want to use the methods of Chernozhukov and coauthors implements in the R packaga Chernozhukov, Hansen, and Spindler ( 2016 ) or the methods of Athey and coauthors implemented in the R package Tibshirani et al. ( 2018 ) , then it makes sense to use the R pacakge. You could simply write all your code in R. However, if you prefer using Julia, you can just call the necessary R functions with RCall.jl . Here, we load the pipeline data used in the machine learning methods notes , and do some cleaning in Julia. using RCall, DataFrames, Missings, Statistics R\"load(paste($(docdir),\\\"/rmd/pipelines.Rdata\\\",sep=\\\"\\\"))\" println(R\"ls()\") RObject{StrSxp} [1] \"#JL\" \"data\" data = @rget data # data on left is new Julia variable, data on right is the one in R println(R\"summary(data[,1:5])\") RObject{StrSxp} respondent_id report_yr report_prd major Min. : 1.0 Min. :1991 Min. :12 Mode :logical 1st Qu.: 64.0 1st Qu.:1997 1st Qu.:12 FALSE:1192 Median :148.0 Median :2003 Median :12 TRUE :2797 Mean :184.3 Mean :2003 Mean :12 NA's :2180 3rd Qu.:214.0 3rd Qu.:2010 3rd Qu.:12 Max. :745.0 Max. :2016 Max. :12 NA's :3371 respondent_name Centra Pipelines Minnesota Inc. : 22 Tuscarora Gas Transmission Company : 22 Eastern Shore Natural Gas Company : 22 Kern River Gas Transmission Company : 21 National Fuel Gas Supply Corporation: 21 (Other) :2938 NA's :3123 println(describe(data[:,1:5])) 5\u00d78 DataFrame \u2502 Row \u2502 variable \u2502 mean \u2502 min \u2502 m edian \u2502 max \u2502 nunique \u2502 nmissing \u2502 eltype \u2502 \u2502 \u2502 Symbol \u2502 Union\u2026 \u2502 Any \u2502 U nion\u2026 \u2502 Any \u2502 Union\u2026 \u2502 Union\u2026 \u2502 Type \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 1 \u2502 respondent_id \u2502 184.3 \u2502 1 \u2502 1 48.0 \u2502 745 \u2502 \u2502 \u2502 Int64 \u2502 \u2502 2 \u2502 report_yr \u2502 2003.49 \u2502 1991 \u2502 2 003.0 \u2502 2016 \u2502 \u2502 \u2502 Int64 \u2502 \u2502 3 \u2502 report_prd \u2502 12.0 \u2502 12 \u2502 1 2.0 \u2502 12 \u2502 \u2502 3371 \u2502 Union{Missing, Int64} \u2502 \u2502 4 \u2502 major \u2502 0.701178 \u2502 0 \u2502 1 .0 \u2502 1 \u2502 \u2502 2180 \u2502 Union{Missing, Bool} \u2502 \u2502 5 \u2502 respondent_name \u2502 \u2502 Algonquin Gas Transmission Company \u2502 \u2502 Southern Natural Gas Company \u2502 440 \u2502 3123 \u2502 Union{Missing, CategoricalString{UInt32}} \u2502 for c in 59:107 # columns of state mileage, want missing->0 replace!(x->(ismissing(x) || isnan(x)) ? 0.0 : x, data[!,c]) end println(describe(data[:,59:65])) 7\u00d78 DataFrame \u2502 Row \u2502 variable \u2502 mean \u2502 min \u2502 median \u2502 max \u2502 nuniqu e \u2502 nmissing \u2502 eltype \u2502 \u2502 \u2502 Symbol \u2502 Float64 \u2502 Float64 \u2502 Float64 \u2502 Float64 \u2502 Nothin g \u2502 Int64 \u2502 Union \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 1 \u2502 North Carolina \u2502 0.00358525 \u2502 0.0 \u2502 0.0 \u2502 1.0 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 2 \u2502 Tennessee \u2502 0.0061488 \u2502 0.0 \u2502 0.0 \u2502 0.635202 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 3 \u2502 Virginia \u2502 0.00552028 \u2502 0.0 \u2502 0.0 \u2502 1.0 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 4 \u2502 Illinois \u2502 0.0134891 \u2502 0.0 \u2502 0.0 \u2502 1.0 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 5 \u2502 Indiana \u2502 0.0058707 \u2502 0.0 \u2502 0.0 \u2502 0.550302 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 6 \u2502 Kentucky \u2502 0.0133474 \u2502 0.0 \u2502 0.0 \u2502 1.0 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 \u2502 7 \u2502 Gulf of Mexico \u2502 0.0140981 \u2502 0.0 \u2502 0.0 \u2502 0.825409 \u2502 \u2502 0 \u2502 Union{Missing, Float64} \u2502 Suppose we want to estimate the coefficient on transPlant (capital) in a partially linear model with transProfit (profit) as the outcome. This can be done with the R function hdm::rlassoEffects . R\"library(hdm)\" completedata = dropmissing(data,[1:10..., 59:122...], disallowmissing=true) y = completedata[:transProfit] inc = .!isnan.(y) y = y[inc] X = completedata[inc,[6:7..., 59:121...]] cols = [std(X[c])>0 for c in 1:ncol(X)] X = X[:,cols] est = R\"rlassoEffects($(X), $(y), index=c(1:2))\" R\"summary($est)\" RObject{VecSxp} [1] \"Estimates and significance testing of the effect of target variables\" Estimate. Std. Error t value Pr(>|t|) transPlant_bal_end_yr 0.034434 0.008878 3.879 0.000105 *** transPlant_bal_beg_yr 0.086580 0.009383 9.228 < 2e-16 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1","title":"RCall"},{"location":"ml-julia/#mljjl","text":"MLJ.jl is a machine learning framework for Julia. It gives a unified interface for many machine learning algorithms and tasks. Similar R packages include caret and MLR . scikit-learn is a similar Python package. For more information on MLJ see MLJ.jl docs MLJ tutorials You can see a list of models registered to work with MLJ.jl on github , or by calling MLJ::models() . using MLJ models() 109-element Array{NamedTuple,1}: (name = ARDRegressor, package_name = ScikitLearn, ... ) (name = AdaBoostClassifier, package_name = ScikitLearn, ... ) (name = AdaBoostRegressor, package_name = ScikitLearn, ... ) (name = BaggingClassifier, package_name = ScikitLearn, ... ) (name = BaggingRegressor, package_name = ScikitLearn, ... ) (name = BayesianLDA, package_name = MultivariateStats, ... ) (name = BayesianLDA, package_name = ScikitLearn, ... ) (name = BayesianQDA, package_name = ScikitLearn, ... ) (name = BayesianRidgeRegressor, package_name = ScikitLearn, ... ) (name = BernoulliNBClassifier, package_name = ScikitLearn, ... ) \u22ee (name = Standardizer, package_name = MLJModels, ... ) (name = StaticTransformer, package_name = MLJModels, ... ) (name = TheilSenRegressor, package_name = ScikitLearn, ... ) (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... ) (name = UnivariateDiscretizer, package_name = MLJModels, ... ) (name = UnivariateStandardizer, package_name = MLJModels, ... ) (name = XGBoostClassifier, package_name = XGBoost, ... ) (name = XGBoostCount, package_name = XGBoost, ... ) (name = XGBoostRegressor, package_name = XGBoost, ... ) To use these models, you need the corresponding package to be installed and loaded. The @load macro will load the needed package(s) for any model. lasso_model = @load LassoRegressor pkg=MLJLinearModels LassoRegressor(lambda = 1.0, fit_intercept = true, penalize_intercept = false, solver = nothing,) @ 9\u202676 Let\u2019s fit lasso to the same pipeline data as above. lasso_model.lambda = 1.0 lasso = machine(lasso_model, X, y) train,test = partition(eachindex(y), 0.6, shuffle=true) fit!(lasso, rows=train) yhat = predict(lasso, rows=test) println(yhat[1:10]) [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") MSE/var(y) = 1.4769678684316065 That doesn\u2019t look very good. All the predictions are zero. This could happen when the regularization parameter, lambda , is too large. However, in this case the problem is something else. The warning messages indicate numeric problems when minimizing the lasso objective function. This can happen when X is poorly scaled. The algorithm used to compute the lasso estimates works best when the coefficients are all roughly the same scale. The existing X \u2019s have wildly different scales, which causes problems. This situation is common, so MLJ.jl has functions to standardize variables. It is likely that the hdm package in R does something similar internally. lasso_stdx = @pipeline PipeLasso(std=Standardizer(), lasso=LassoRegressor(lambda=1.0*std(y), solver=MLJLinearModels.ISTA(max_iter=10000) ) ) m = machine(lasso_stdx, X, y) fit!(m, rows=train) yhat = predict(m , rows=test) println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") MSE/var(y) = 0.1376605360336052 # Get the fitted coefficients coef = fitted_params(m).fitted_params[1].coefs intercept = fitted_params(m).fitted_params[1].intercept sum(abs.(coef).>1e-8) # number non-zero 52 If we want to tune lambda using cross-validation, we can use the range and TunedModel functions. r = range(lasso_stdx, :(lasso.lambda), lower=1e1, upper=1e10, scale=:log) t=TunedModel(model=lasso_stdx, resampling=CV(nfolds=5), tuning=Grid(resolution=10), ranges=r, measure=rms) m = machine(t, X, y) fit!(m, rows=train, verbosity=1) yhat = predict(m , rows=test) println(\"MSE/var(y) = $(mean((y[test].-yhat).^2)/var(y[test]))\") MSE/var(y) = 0.1286305685499512 using Plots cvmse = m.report.measurements \u03bb = Float64.(m.report.parameter_values[:]) plot(\u03bb, cvmse, xlab=\"\u03bb\", ylab=\"CV(MSE)\")","title":"MLJ.jl"},{"location":"ml-julia/#fluxjl","text":"Flux.jl is another Julia package for machine learning. It seems to be emerging as the leading Julia package for neural networks and deep learning, but other machine learning models can also be implemented using Flux.jl . Let\u2019s create a lasso model in Flux.jl . using Flux, LinearAlgebra # Scale the variables Xstd = Flux.normalise(Matrix(X)) X_train = Xstd[train,:] X_test = Xstd[test,:] yscale = std(y) ymean = mean(y) ystd = (y .- ymean)./yscale y_train = ystd[train] y_test = ystd[test] # Set up the model parameters and initial values \u03b2ols = (X_train'*X_train) \\ (X_train'*(y_train .- mean(y_train))) \u03b2 = param(zeros(ncol(X))) #\u03b2ols) #zeros(ncol(X))) b = param([mean(y_train)]) \u03b8 = Tracker.Params([\u03b2,b]) # Define the loss function \u03c8 = ones(length(\u03b2)) \u03bb = 2.0 pred(x) = b .+ x*\u03b2 mse(x,y) = mean( (pred(x) .- y).^2 ) penalty(y) = \u03bb/length(y)*norm(\u03c8.*\u03b2,1) loss(x,y) = mse(x,y) + penalty(y) @show loss(X_train,y_train) loss(X_train, y_train) = 1.0694237073658825 (tracked) # minimize loss maxiter=2000 obj = zeros(maxiter) mse_train = zeros(maxiter) mse_test = zeros(maxiter) for i in 1:maxiter Flux.train!(loss, \u03b8, [(X_train, y_train)], Flux.AMSGrad()) mse_train[i] = Tracker.data(mse(X_train,y_train)) mse_test[i] = Tracker.data(mse(X_test, y_test)) obj[i] = Tracker.data(loss(X_train,y_train)) end lo = 1 hi = 250 plot(obj[lo:hi], ylab=\"Loss=MSE + \u03bb/n*||\u03b2||\u2081\", xlab=\"Iteration\") plot(lo:hi, [mse_train[lo:hi] mse_test[lo:hi]], ylab=\"MSE\", xaxis=(\"Iteration\") , lab=[\"Train\" \"Test\"]) The minimization methods in Flux.train! are all variants of gradient descent. Each call to Flux.train! runs one iteration of the specified solver. To find a locaol minimum, Flux.train! can be called repeatedly until progress stops. The above loop is a simple way to do this. The @epoch macro can also be useful. Gradient descent works well for neural networks, but are is ideal for Lasso. Without further adjustment, gradient descent gets stuck in a cycle as jumps from one side of the other of the absolute value in the lasso penalty. Nonetheless, the results are near the true minimum, even though it never exactly gets there.","title":"Flux.jl"},{"location":"ml-julia/#additional-resources","text":"Klok and Nazarathy ( 2019 ) Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence","title":"Additional Resources"},{"location":"ml-julia/#references-references","text":"Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. \u201chdm: High-Dimensional Metrics.\u201d R Journal 8 (2): 185\u201399. https://journal.r-project.org/archive/2016/RJ-2016-040/index.html . Klok, Hayden, and Yoni Nazarathy. 2019. Statistics with Julia:Fundamentals for Data Science, Machinelearning and Artificial Intelligence . DRAFT. https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf . Tibshirani, Julie, Susan Athey, Stefan Wager, Rina Friedberg, Luke Miner, and Marvin Wright. 2018. Grf: Generalized Random Forests (Beta) . https://CRAN.R-project.org/package=grf .","title":"References [references]"},{"location":"ml-methods/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction to machine learning \u00b6 Friedman, Hastie, and Tibshirani ( 2009 ) and James et al. ( 2013 ) are commonly recommended textbooks on machine learning. James et al. ( 2013 ) is less technical of the two, but neither book is especially difficult. Efron and Hastie ( 2016 ) covers similar material and is slightly more advanced. Some prediction examples \u00b6 Machine learning is tailored for prediction, let\u2019s look at some data and see how well it works Predicting house prices \u00b6 Example from Mullainathan and Spiess ( 2017 ) Training on 10000 observations from AHS Predict log house price using 150 variables Holdout sample of 41808 AHS variables [ahs-variables] \u00b6 ahs <- readRDS(\"ahs2011forjep.rdata\")$df print(summary(ahs[,1:20])) ## LOGVALUE REGION METRO METRO3 PHONE KITCHEN ## Min. : 0.00 1: 5773 1:10499 1:11928 -7: 1851 1:51513 ## 1st Qu.:11.56 2:13503 2: 1124 2:39037 1 :49353 2: 295 ## Median :12.10 3:15408 3: 202 9: 843 2 : 604 ## Mean :12.06 4:17124 4: 103 ## 3rd Qu.:12.61 7:39880 ## Max. :15.48 ## MOBILTYP WINTEROVEN WINTERKESP WINTERELSP WINTERWOOD WINTERNONE ## -1:49868 -8: 133 -8: 133 -8: 133 -8: 133 -8: 133 ## 1 : 927 -7: 50 -7: 50 -7: 50 -7: 50 -7: 50 ## 2 : 1013 1 : 446 1 : 813 1 : 8689 1 : 61 1 :41895 ## 2 :51179 2 :50812 2 :42936 2 :51564 2 : 9730 ## ## ## NEWC DISH WASH DRY NUNIT2 BURNER COOK ## -9:50485 1:42221 1:50456 1:49880 1:44922 -6:51567 1:51567 ## 1 : 1323 2: 9587 2: 1352 2: 1928 2: 2634 1 : 87 2: 241 ## 3: 2307 2 : 154 ## 4: 1945 ## ## ## OVEN ## -6:51654 ## 1 : 127 ## 2 : 27 ## ## ## library(GGally) ggpairs(ahs[,c(\"LOGVALUE\",\"ROOMS\", \"LOT\",\"UNITSF\",\"BUILT\")], lower=list(continuous=\"points\", combo=\"facethist\", discrete=\"facetbar\"), diag=list(continuous=\"barDiag\",discrete=\"barDiag\")) + theme_minimal() # use ms-reproduce.R from course git repo to download and run Mullainathon & Spiess data and code to # create jepfittedmodels-table1.csv. Be aware that this will take many hours. tbl <- read.csv(\"jepfittedmodels-table1.csv\") tab <- tbl[,3:ncol(tbl)] rownames(tab) <- tbl[,2] tab <- tab[1:5, c(1,2,3,5)] colnames(tab) <- c(\"in sample MSE\", \"in sample R^2\", \"out of sample MSE\", \"out of sample R^2\") library(kableExtra) kable_styling(kable(tab, caption=\"Performance of different algorithms in predicting housing values\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Performance of different algorithms in predicting housing values in sample MSE in sample R\\^2 out of sample MSE out of sample R\\^2 OLS 0.589 0.473 0.674 0.417 Tree 0.675 0.396 0.758 0.345 Lasso 0.603 0.460 0.656 0.433 Forest 0.166 0.851 0.632 0.454 Ensemble 0.216 0.807 0.625 0.460 library(ggplot2) load(file=\"jeperrorfig.RData\") print(fig) load(file=\"jeperrorfig.RData\") print(fig2) Predicting pipeline revenues \u00b6 Data on US natural gas pipelines Combination of FERC Form 2, EIA Form 176, and other sources, compiled by me 1996-2016, 236 pipeline companies, 1219 company-year observations Predict: $y =$ profits from transmission of natural gas Covariates: year, capital, discovered gas reserves, well head gas price, city gate gas price, heating degree days, state(s) that each pipeline operates in load(\"pipelines.Rdata\") # data has problems before 1996 due to format change data <- subset(data,report_yr>=1996) # replace NA state weights with 0's data[,59:107][is.na(data[,59:107])] <- 0 # spaces in variable names will create problems later names(data) <- gsub(\" \",\".\",names(data)) summary(data[,c(\"transProfit\",\"transPlant_bal_beg_yr\",\"cityPrice\",\"wellPrice\")]) ## transProfit transPlant_bal_beg_yr cityPrice ## Min. : -31622547 Min. :0.000e+00 Min. : 0.4068 ## 1st Qu.: 2586031 1st Qu.:2.404e+07 1st Qu.: 3.8666 ## Median : 23733170 Median :1.957e+08 Median : 5.1297 ## Mean : 93517513 Mean :7.772e+08 Mean : 5.3469 ## 3rd Qu.: 129629013 3rd Qu.:1.016e+09 3rd Qu.: 6.5600 ## Max. :1165050214 Max. :1.439e+10 Max. :12.4646 ## NA's :2817 NA's :2692 NA's :1340 ## wellPrice ## Min. :0.0008 ## 1st Qu.:2.1230 ## Median :3.4370 ## Mean :3.7856 ## 3rd Qu.:5.1795 ## Max. :9.6500 ## NA's :2637 library(GGally) ggpairs(data[,c(\"transProfit\",\"transPlant_bal_beg_yr\",\"cityPrice\",\"wellPrice\")], lower=list(continuous=\"smooth\")) + theme_minimal() Predicting pipeline revenues : methods \u00b6 OLS : 67 covariates (year dummies and state(s) create a lot) Lasso Random forests Randomly choose 75% of sample to fit the models, then look at prediction accuracy in remaining 25% We are focusing on Lasso and random forests because these are the two methods that econometricians have worked on the most. Other methods such as neural nets and support vector machines are also worth exploring. For now, you can think of Lasso and random forests these as black boxes that generate predictions from data. We will go into more detail soon. ## Create X matrix for OLS and random forests xnames <-c(\"transPlant_bal_beg_yr\", \"reserve\", \"wellPrice\", \"cityPrice\", \"plantArea\", \"heatDegDays\", names(data)[59:107] ) yname <- \"transProfit\" fmla <- paste(yname,\"~\",paste(xnames,collapse=\" + \"),\"+ as.factor(report_yr)\") ols <- lm(fmla,data=data,x=TRUE,y=TRUE) X <- ols$x[,!(colnames(ols$x) %in% c(\"(Intercept)\")) & !is.na(ols$coefficients)] y <- ols$y train <- runif(nrow(X))<0.75 # OLS prediction on training set y.t <- y[train] X.t <- X[train,] ols <- lm(y.t ~ X.t) y.hat.ols <- ols$coefficients[1] + X %*% ols$coefficients[2:(length(ols$coef))] df <- data.frame(y=y, y.hat=y.hat.ols, train=train, method=\"ols\") ## Lasso library(glmnet) # Create larger X matrix for lasso fmla.l <- paste(yname,\"~ (\", paste(xnames,collapse=\" + \"),\")*(report_yr + transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays) + \", paste(sprintf(\"I(%s^2)\",xnames[1:6],collapse=\" + \")) ) reg <- lm(fmla.l, data=data, x=TRUE,y=TRUE) Xl <- reg$x[,!(colnames(reg$x) %in% c(\"(Intercept)\")) & !is.na(reg$coefficients)] lasso <- cv.glmnet(Xl[train,],y[train],alpha=1,parallel=FALSE, standardize=TRUE, intercept=TRUE, nfolds = 50) y.hat.lasso <- predict(lasso, Xl, s=lasso$lambda.min, type=\"response\") df <- rbind(df, data.frame(y=y, y.hat=as.vector(y.hat.lasso), train=train, method=\"lasso\")) ## Random forest library(grf) rf <- regression_forest(X[train,],y[train],tune.parameters = TRUE) y.hat.rf <- predict(rf, X)$predictions df <- rbind(df, data.frame(y=y, y.hat=y.hat.rf, train=train, method=\"random forest\")) # Neural network library(RSNNS) n <- nrow(X[train,]) p <- ncol(X) rn <- floor(n^(1/(2*(1+1/(1+p))))/2) xn <- normalizeData(X) yn <- normalizeData(y) nn <- mlp(x=xn[train,], y=yn[train], linOut=TRUE, size=rn) yn.hat.nn <- predict(nn, xn) y.hat.nn <- denormalizeData(yn.hat.nn, getNormParameters(yn)) df <- rbind(df, data.frame(y=y, y.hat=y.hat.nn, train=train, method=\"neural network\")) ggplot(data=df,aes(x=y,y=y.hat,colour=method,shape=train)) + geom_point(alpha=0.5) + geom_line(aes(y=y)) + theme_minimal() df$trainf <- factor(df$train, levels=c(\"TRUE\", \"FALSE\")) df$error <- df$y - df$y.hat ggplot(data=df,aes(x=error,colour=method)) + geom_density() + theme_minimal() + xlim(quantile(df$error,c(0.01,0.99))) + facet_grid(trainf ~ .,labeller=label_both) library(kableExtra) fn <- function(df) with(df,c(mean((y.hat - y)^2)/var(y), mean(abs(y.hat - y))/mean(abs(y-mean(y))))) tab1 <-unlist(by(subset(df,train), df$method[train], FUN=fn)) tab1 <- (matrix(tab1,nrow=2)) rownames(tab1) <- c(\"relative MSE\",\"relative MAE\") colnames(tab1) <- c(\"OLS\",\"Lasso\",\"Random forest\",\"Neural Network\") tab2 <- unlist(by(subset(df,!train), df$method[!train], FUN=fn)) tab2 <- (matrix(tab2,nrow=2)) rownames(tab2) <- c(\"relative MSE\",\"relative MAE\") colnames(tab2) <- c(\"OLS\",\"Lasso\",\"Random forest\",\"Neural Network\") kable_styling(kable(tab1, caption=\"Training sample\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=F) Training sample OLS Lasso Random forest Neural Network relative MSE 0.110 0.031 0.063 0.012 relative MAE 0.265 0.137 0.179 0.111 kable_styling(kable(tab2, caption=\"Hold-out sample\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=F) Hold-out sample OLS Lasso Random forest Neural Network relative MSE 0.125 0.064 0.093 0.092 relative MAE 0.288 0.199 0.232 0.278 In this table, relative MSE is the mean squared error relative to the variance of $y$, that is \\text{relative MSE} = \\frac{\\En[(y_i - \\hat{y}_i)^2]} {\\En[ (y_i - \\bar{y})^2]}. It is equal to $1-R^2$. Similarly, relative MAE is \\text{relative MAE} = \\frac{\\En[|y_i - \\hat{y}_i|]} {\\En[|y_i - \\bar{y}|]}. $\\En$ denotes the empirical expectation, $\\En[y_i] = \\frac{1}{n}\\sum_{i=1}^n y_i$. Lasso \u00b6 Lasso solves a penalized (regularized) regression problem \\hat{\\beta} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] + \\frac{\\lambda}{n} \\norm{ \\hat{\\Psi} \\beta}_1 Penalty parameter $\\lambda$ Diagonal matrix $\\hat{\\Psi} = diag(\\hat{\\psi})$ Dimension of $x_i$ is $p$ and implicitly depends on $n$ can have $p >> n$ We are following the notation used in Chernozhukov, Hansen, and Spindler ( 2016 ). Note that this vignette has been updated since it was published in the R Journal. To obtain the most recent version, install the hdm package in R, load it, and then open the vignette. install.packages(\"hdm\") library(hdm) vignette(\"hdm_introduction\") The choice of penalty (or regularization) parameter, $\\lambda$, is important. When $\\lambda = 0$, Lasso is the same as OLS. As $\\lambda$ increases, the Lasso estimates will shrink toward 0. For large enough $\\lambda$, some components of $\\hat{\\beta}$ become exactly 0. As $\\lambda$ increases more, more and more components of $\\hat{\\beta}$ will be exactly $0$. For some intuition about why Lasso results in some coefficients being zero, note that \\hat{\\beta}^{lasso} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] + \\frac{\\lambda}{n} \\norm{\\beta}_1 is equivalent to \\hat{\\beta}^{lasso} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] \\text{ s.t. } \\norm{\\beta}_1 \\leq s for some $s$. In this problem, the boundary of the constraint set will be a diamond. The level sets of the objective will be elipses. Generically, the solution will lie on one of the corners of the $\\norm{\\beta}_1 = 1$ set. See Friedman, Hastie, and Tibshirani ( 2009 ) or James et al. ( 2013 ) for more details. Most machine learning methods involve some form of regularization with an associated regularization parameter. In choosing the regularization parameter, we face a bias-variance tradeoff. As $\\lambda$ increases, variance decreases, but bias increases. Machine learning algorithms typically choose regularization parameters through cross-validation. Although cross-validation leads to good predictive performance, the statistical properties are not always known. Chernozhukov, Hansen, and Spindler ( 2016 ) say, \u201cIn high dimensional settings cross-validation is very popular; but it lacks a theoretical justification for use in the present context.\u201d However, there has been some recent progress on convergence rates for Lasso with cross-validation, see Chetverikov, Liao, and Chernozhukov ( 2016 ). The diagonal matrix $\\hat{\\Psi}$ is used to make the estimator invariant to scaling of $x_i$, and to allow for heteroskedasticity. If reading about Lasso or using code from other authors, be careful some do not include $\\hat{\\Psi}$ and use $\\lambda$ instead of $\\frac{\\lambda}{n}$. load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) library(glmnet) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) # standardize everything so that coefficients are similar scale when plotted mod$y <- (mod$y - mean(mod$y))/sd(mod$y) for(c in 2:ncol(mod$x)) { mod$x[,c] <- (mod$x[,c] - mean(mod$x[,c]))/sd(mod$x[,c]) } lassoPath <- glmnet(mod$x, mod$y, alpha=1) plot(lassoPath, xvar=\"lambda\", label=TRUE) load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) library(glmnet) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) # standardize everything so that coefficients are similar scale when plotted mod$y <- (mod$y - mean(mod$y))/sd(mod$y) for(c in 2:ncol(mod$x)) { mod$x[,c] <- (mod$x[,c] - mean(mod$x[,c]))/sd(mod$x[,c]) } cv <- cv.glmnet(mod$x, mod$y, alpha=1) plot(cv) Statistical properties of Lasso \u00b6 Model : y_i = x_i'\\beta_0 + \\epsilon_i $\\Er[x_i \\epsilon_i] = 0$ $\\beta_0 \\in \\R^n$ $p$, $\\beta_0$, $x_i$, and $s$ implicitly depend on $n$ $\\log p = o(n^{1/3})$ $p$ may increase with $n$ and can have $p>n$ Sparsity $s$ Exact : $\\norm{\\beta_0}_0 = s = o(n)$ Approximate : $|\\beta_{0,j}| < Aj^{-a}$, $a > 1/2$, $s \\propto n^{1/(2a)}$ $\\norm{\\beta}_0$ is the number of non-zero components of $\\beta$. The approximate sparsity setting means if $|\\beta_{0,j}| < Aj^{-a}$, then, there exists a sparse approximation, say $\\beta_{a}$, with $s$ nonzero elements, such that the approximation error, \\En[(x_i'(\\beta_a - \\beta_0))^2] = c_s^2 will vanish quickly if $s \\propto n^{1/2a}$. Just how quickly will it vanish? An easy upper bound is \\begin{align*} c_s^2 \\leq & \\En\\left[\\left( \\sum_{j={s+1}}^p x_ij \\beta_{0,j} \\right)^2 \\right] \\\\ \\leq & \\En\\left[ \\left(\\sum_{j={s+1}}^p x_ij A j^{-a} \\right)^2 \\right] \\end{align*} To simplify the alegebra, let\u2019s assume $\\En[x_i x_i\u2019] = I_p$, then \\begin{align*} c_s^2 \\leq & \\sum_{j={s+1}}^p A^2 j^{-2a} \\\\ & \\leq \\sum_{j={s+1}}^\\infty A^2 j^{-2a} = A^2 s^{-2a} \\zeta(2a) \\\\ c_s^2 \\lesssim s^{-2a} \\end{align*} where $\\zeta(2a) = \\sum_{j=1}^\\infty j^{-2a}$ is the Riemann Zeta function (all that matters here is that $\\zeta(2a)$ is finite for $2a>1$). Then if $s \\propto n^{(1+\\delta)/2a}$, we would get $c_s \\lesssim n^{-(1+\\delta)/2}$. Importantly, $\\sqrt{n} c_s = o(1)$, so in the sort of expansions that we would do to show that $\\hat{\\theta}$ is $\\sqrt{n}$ asymptotically normal, the bias term would vanish. Rate of convergence \u00b6 With $\\lambda = 2c \\sqrt{n} \\Phi^{-1}(1-\\gamma/(2p))$ \\sqrt{\\En[(x_i'(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\sqrt{ (s/n) \\log (p) }, \\norm{\\hat{\\beta}^{lasso} - \\beta_0}_2 \\lesssim_P \\sqrt{ (s/n) \\log (p) }, and \\norm{\\hat{\\beta}^{lasso} - \\beta_0}_1 \\lesssim_P \\sqrt{ (s^2/n) \\log (p) } Constant $c>1$ Small $\\gamma \\to 0$ with $n$, and $\\log(1/\\gamma) \\lesssim \\log(p)$ Rank like condition on $x_i$ near-oracle rate In the semiparametric estimation problems that we\u2019re focused on, our object of interest is some finite dimensional parameter $\\theta$ that depends on our data some high dimensional parameter, like $\\beta_0$ in the Lasso. To analyze estimates of $\\hat{\\theta}(data, \\hat{\\beta})$, a key step will be to show that we can replace $\\hat{\\beta}$ with $\\beta_0$. The rate of convergence of $\\hat{\\beta}$ will be important for making this possible. Thus, the main thing that we care about for the Lasso and other machine learning estimators will be their rates of converagence. The notation $A_n \\lesssim_P B_n$ is read $A_n$ is bounded in probability by $B_n$ and means that for any $\\epsilon>0$, there exists $M$, $N$ such that $\\Pr(|A_n/B_n| > M) < \\epsilon$ for all $n > N$. This is also often denoted by $A_n = O(B_n)$. These rate results are from Belloni et al. ( 2012 ). Since this setup allows $p>n$, $x$ cannot be assumed to have full rank. Instead, an assumption about the eigenvalues of $X\u2019X$ restricted to the nonzero components of $\\beta_0$, plays a similar. See Belloni et al. ( 2012 ) for details. This convergence rate is called the near-oracle rate, because it is nearly as good as what we get if an oracle told us which components of $\\beta_0$ are nonzero. In that case OLS using just those $s$ components gives the fastest possible rate, which is \\sqrt{\\En[(x_i'(\\hat{\\beta}^{OLS} - \\beta_0))^2]} \\propto \\sqrt{s/n}. Rate of convergence [rate-of-convergence-1] \u00b6 Using cross-validation to choose $\\lambda$ known bounds are worse With Gaussian errors: $\\sqrt{\\En[(x_i\u2019(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\sqrt{ (s/n) \\log (p) } \\log(pn)^{7/8}$, Without Gaussian error $\\sqrt{\\En[(x_i\u2019(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\left( \\frac{s \\log(pn)^2}{n} \\right)^{1/4}$ Chetverikov, Liao, and Chernozhukov ( 2016 ) These results are for an exactly sparse setting. Do they hold under approximate sparsity? Other statistical properties \u00b6 Inference on $\\beta$: not the goal in our motivating examples Difficult, but some recent results See Lee et al. ( 2016 ), Taylor and Tibshirani ( 2017 ), Caner and Kock ( 2018 ) Model selection: not the goal in our motivating examples Under stronger conditions, Lasso correctly selects the nonzero components of $\\beta_0$ See Belloni and Chernozhukov ( 2011 ) In the statistics literature on high dimensional and nonparametric estimation, you will come across the terms \u201cadaptive\u201d and \u201chonest.\u201d Adaptivity of an estimator refers to the situation where the rate of convergence depends on some unknown parameter. In the case of Lasso, the sparsity index of the true model, $s$, is an unknown parameter affecting the rate of convergence. Without knowing or estimating $s$, Lasso attains the above rate of convergence for a wide range of admissable $s$. Thus, Lasso is adaptive to the unknown sparsity index. \u201cHonest\u201d is a property of an inference method. A confidence region is honest if it has correct coverage for a large class of true models. For the Lasso, an honest confidence region would be valid for a wide range of sparsity, $s$. An honest, adaptive confidence region would be one that is valide for a wide range of $s$ and whose size shrinks as quickly as if $s$ were known. Achieving both adaptivity and honesty is impossible in the most general setting. For example, although an $\\ell$ times differentiable function of a $p$ dimensional variable can be adaptively estimated at rate $n^{-\\ell}{2\\ell + p}$, Li ( 1989 ) showed that an honest confidence region can contract at most at rate $n^{-1/4}$ (not adaptive to $\\ell$). However, an adaptive confidence region can be constructed if further restrictions are placed on the set of possible models, see Nickl and Geer ( 2013 ) for such a result for Lasso.. Post-Lasso \u00b6 Two steps : Estimate $\\hat{\\beta}^{lasso}$ ${\\hat{\\beta}}^{post} =$ OLS regression of $y$ on components of $x$ with nonzero $\\hat{\\beta}^{lasso}$ Same rates of convergence as Lasso Under some conditions post-Lasso has lower bias If Lasso selects correct model, post-Lasso converges at the oracle rate Post-Lasso removes some of the regularizaton bias of Lasso. The rate of convergence of post-Lasso is always as fast as Lasso, and under conditions that allow perfect model selection, post-Lasso converges slightly faster (by a factor $\\log(p)$). See Belloni et al. ( 2012 ) for details. Random forests \u00b6 Regression trees \u00b6 $y_i \\in R$ on $x_i \\in \\R^p$ Want to estimate $\\Er[y | x]$ Locally constant estimate \\hat{t}(x) = \\sum_m^M c_m 1\\{x \\in R_m \\} Rectangular regions $R_m$ determined by tree Simulated data \u00b6 n <- 1000 x <- runif(n) y <- runif(n) f <- function(x,z) { 1/3*(sin(5*x)*sqrt(z)*exp(-(z-0.5)^2)) } f0 <- f(x,y) z <- f0 + rnorm(n)*0.1 tree.df <- data.frame(x=x,y=y,z=z) # plot true function and data x.g <- seq(0,1,length.out=100) y.g <- seq(0,1,length.out=100) f0.g <- t(outer(x.g,y.g,f)) library(plotly) fig <- plot_ly( colors=\"YlOrBr\") fig <- add_markers(fig,x=x,y=y,z=z, size=0.3, opacity=0.2) fig <- add_surface(fig, x=x.g, y=y.g, z=f0.g, opacity=1) fig Estimated tree \u00b6 # fit regression tree library(party) tree <- ctree(z ~ x + y, data=tree.df) # plot estimate x.g <- seq(0,1,length.out=100) y.g <- seq(0,1,length.out=100) df <- expand.grid(x.g,y.g) names(df) <- c(\"x\",\"y\") fhat.g <- matrix(predict(tree, newdata=df),nrow=length(x.g), byrow=TRUE) library(plotly) fig <- plot_ly(colors=\"YlOrBr\") fig <- add_markers(fig,x=x,y=y,z=z, size=0.3, opacity=0.2) fig <- add_surface(fig, x=x.g, y=y.g, z=fhat.g, opacity=1) fig Estimated tree \u00b6 plot(tree) Tree algorithm \u00b6 For each region, solve \\min_{j,s} \\left[ \\min_{c_1} \\sum_{i: x_{i,j} \\leq s, x_i \\in R} (y_i - c_1)^2 + \\min_{c_2} \\sum_{i: x_{i,j} > s, x_i \\in R} (y_i - c_2)^2 \\right] Repeat with $R = {x:x_{i,j} \\leq s^ } \\cap R$ and $R = {x:x_{i,j} \\leq s^ } \\cap R$ Stop when $|R| =$ some chosen minimum size Prune tree \\min_{tree \\subset T} \\sum (\\hat{f}(x)-y)^2 + \\alpha|\\text{terminal nodes in tree}| There are many variations on this tree building algorithm. They all share some rule to decide on which variable and where to split. They all have some kind of stopping rule, but not necessarily the same one. For example, some algorithms stop splitting into new branches when the improvement in $R^2$ becomes small. These trees don\u2019t need subsequent pruning, but also may fail to find later splits that might be important. As with lasso, regression trees involve some regularization. In the above description, both the minimum leaf size and $\\alpha$ in the pruning step serve as regularization parameters. A potential advantage of regression trees is that their output might be interpretable, especially if there are not many branches. Some disadvantages are that they often are not very good predictors, and small perturbations in data can lead to seemingly large changes in the tree. Random forests \u00b6 Average randomized regression trees Trees randomized by Bootstrap or subsampling Randomize branches: \\min_{j \\in S,s} \\left[ \\min_{c_1} \\sum_{i: x_{i,j} \\leq s, x_i \\in R} (y_i - c_1)^2 + \\min_{c_2} \\sum_{i: x_{i,j} > s, x_i \\in R} (y_i - c_2)^2 \\right] where $S$ is random subset of ${1, \u2026, p}$ Variance reduction Rate of convergence: regression tree \u00b6 $x \\in [0,1]^p$, $\\Er[y|x]$ Lipschitz in $x$ Crude calculation for single tree, let denote $R_i$ node that contains $x_i$ \\begin{align*} \\Er(\\hat{t}(x_i) - \\Er[y|x_i])^2 = & \\overbrace{\\Er(\\hat{t}(x_i) - \\Er[y|x\\in R_i])^2}^{variance} + \\overbrace{(\\Er[y|x \\in R_i] - \\Er[y|x])^2}^{bias^2} \\\\ = & O_p(1/m) + O\\left(L^2 \\left(\\frac{m}{n}\\right)^{2/p}\\right) \\end{align*} optimal $m = O(n^{2/(2+p)})$ gives \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2] = O_p(n^{\\frac{-2}{2+p}}) By a crude calculation, I mean lets treat the tree as fixed. The the variance term is simply from estimating a conditional mean. This analysis could be made more rigorous by assuming the tree was estimated by sample splitting \u2014 use half the data to construct the tree and the remaining half to estimate the mean of $y$ in each node. Athey and Wager, and others, refer to such trees as \u201chonest.\u201d I suppose that this is because sample splitting facilitates honest inference afterward. The order of the bias term comes from considering the width of the pieces of a $p$ dimensional cube split evenly into $n/m$ pieces. Remember that for our motivating semiparametric problems, we need $\\sqrt{n} \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2]$ to vanish. The above rate convergence is too slow for $p>2$. The calculation of the above was admittedly crude, and may not be exact. However, Stone ( 1982 ) showed that if $\\Er[y|x]$ is $\\ell$ times differentiable, the fastest possible rate of convergence for any estimator is $n^{\\frac{-\\ell}{2\\ell + p}}$. To have any hope of a fast enough rate, we need to assume the function we\u2019re estimating is very smooth (high $\\ell$), or place some other restriction on the class of functions we allow (like sparsity for the Lasso). Lipschitz continuity is slightly weaker than once differentiable on a compact set, so it should come as no surprise that the rate of convergence would be slow. Rate of convergence: random forest \u00b6 Result from Biau ( 2012 ) Assume $\\Er[y|x]=\\Er[y|x_{(s)}]$, $x_{(s)}$ subset of $s$ variables, then \\Er[(\\hat{r}(x_i) - \\Er[y|x_i])^2] = O_p\\left(\\frac{1}{m\\log(n/m)^{s/2p}}\\right) + O_p\\left(\\left(\\frac{m}{n}\\right)^{\\frac{0.75}{s\\log 2}} \\right) or with optimal $m$ \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2] = O_p(n^{\\frac{-0.75}{s\\log 2+0.75}}) This result from Biau ( 2012 ) assumes the forest is estimated with sample splitting. This avoids the difficult to analyze correlation between the nodes and $y$. Wager and Walther ( 2015 ) analyze what happens when the same data is used to construct the tree and average in each node. They get a slightly higher upper bound for the variance of $\\frac{\\log(p)\\log(n)}{m}$. Wager and Walther ( 2015 ) also allow $p$ to increase with $n$, whereas the previous analysis treated $p$ as fixed. These convergence rate results for random forests are not fast enough for our purpose. Does this mean that random forests should not be used in semiparametric estimation? Not necessarily. We\u2019re asking too much of random forests. There is no estimator for an arbitrary Lipschitz function that can have fast enough a rate of convergence. A restriction on the set of possible functions is needed to reduce the approximation bias. With Lasso, the assumption of (approximate) sparsity played that role. Chernozhukov et al. ( 2018 ) advise that random forests could be a good choice for semiparametric estimation when the function of interest is \u201cwell-approximated by a random forest.\u201d Unfortunately, there does not appear to be a clean mathematical way to describe the class of functions well-approximated by a forest. Other statistical properties \u00b6 Pointwise asymptotic normality : Wager and Athey ( 2018 ) Simulation study \u00b6 Partially linear model DGP : $x_i \\in \\R^p$ with $x_{ij} \\sim U(0,1)$ $d_i = m(x_i) + v_i$ $y_i = d_i\\theta + f(x_i) + \\epsilon_i$ $m()$, $f()$ either linear or step functions Estimate by OLS, Lasso, and random forest Lasso & random forest use orthogonal moments \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0 The point of this simulation is to see whether the slower convergence rate of random forests matters for the semiparametric problems we have in mind. Our theory results suggest that estimates of $\\theta$ using random forests with $p>2$ will be asymptotically biased. Specifically, the term d_n = \\En[(m(x_i) - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] will be $O_p(n^{\\frac{-2}{2+p}})$, so $\\sqrt{n} d_n = O_p(n^{\\frac{p-2}{2(2+p)}})$. However, this calculation is only an upper bound on $d_n$. For a given DGP, $d_n$ might be smaller. In this simulation exercise, when $m()$ and $f()$ are linear, they are not easy to approximate by a regression tree, so I expect the random forest estimator to behave relatively poorly. OLS and Lasso on the other hand will do very well, and are included mainly as benchmarks. When $m()$ and $f()$ are step functions (specifically, $f(x) = m(x) = \\sum_{j=1}^p 1(x_{j}>1/2)$), I thought they would be well approximated by a regression tree (and random forest). For OLS and Lasso, $x$ is still only included linearly in the estimation, so those estimators will do poorly in the step function DGP. Throughout the simulation $p$ is much less than $n$, so Lasso and OLS will generally give very similar results. rm(list=ls()) p <- 4 # number of x's mu.linear <- function(x) x%*%rep(1,p) m.linear <- function(x) x%*%rep(2,p) mu.step <- function(x) (x>0.5)%*%rep(1,p) m.step <- function(x) (x>0.5)%*%rep(2,p) theta <- 1 simulate <- function(n,p,mu,m) { theta <- 1 x <- matrix(runif(n*p), ncol=p) d <- m(x) + rnorm(n) y <- theta*d + mu(x) + rnorm(n) data.frame(y=y,d=d,x=x) } library(grf) library(hdm) df <- simulate(100,p,mu.linear,m.linear) mrfparams <- NULL murfparams <- NULL n.save <- NULL partial.linear.rf <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] if (is.null(mrfparams) || n.save!=nrow(df)) { # to save time, we only tune once per cluster worker and data set # size cat(\"tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=1000, tune.parameters=TRUE) mrfparams <<- m.rf$tunable.params mu.rf <- regression_forest(df[,x.names], df$y, num.trees=1000, tune.parameters=TRUE) n.save <<- nrow(df) murfparams <<- mu.rf$tunable.params } else { cat(\"not tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(mrfparams[\"min.node.size\"]), alpha = as.numeric(mrfparams[\"alpha\"]), imbalance.penalty=as.numeric(mrfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(mrfparams[\"sample.fraction\"]), mtry=as.numeric(mrfparams[\"mtry\"])) mu.rf <- regression_forest(df[,x.names], df$y, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(murfparams[\"min.node.size\"]), alpha = as.numeric(murfparams[\"alpha\"]), imbalance.penalty=as.numeric(murfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(murfparams[\"sample.fraction\"]), mtry=as.numeric(murfparams[\"mtry\"])) } vhat <- df$d - predict(m.rf)$predictions ehat <- df$y - predict(mu.rf)$predictions lm(ehat ~ vhat) } ## Manual sample splitting --- this turns out to be unneccessary. The ## default behavior of predict.regression_forest is to return ## predictions on the training data using only trees that were not fit ## on each observation. In other words, regression_forest already does ## the sample splitting for us. ## ## rf.tuneOnce <- function(x.names, y.name) { ## parms <- NULL ## function(df) { ## if (is.null(parms)) { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=500, ## tune.parameters=TRUE) ## parms <<- rf$tunable.params ## rf ## } else { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=200, ## tune.parameters=FALSE, ## honesty=FALSE, ## min.node.size = ## as.numeric(parms[\"min.node.size\"]), ## alpha = as.numeric(parms[\"alpha\"]), ## imbalance.penalty=as.numeric(parms[\"imbalance.penalty\"]), ## sample.fraction = as.numeric(parms[\"sample.fraction\"]), ## mtry=as.numeric(parms[\"mtry\"])) ## } ## } ## } ## n.save.split <- NULL ## m.hat.rf <- NULL ## mu.hat.rf <- NULL ## partial.linear.split.rf <- function(df , splits=3) { ## x.names <- names(df)[grep(\"x.\",names(df))] ## if (is.null(n.save.split) || n.save.split != nrow(df)) { ## n.save.split <<- nrow(df) ## m.hat.rf <<- rf.tuneOnce(x.names,\"d\") ## mu.hat.rf <<- rf.tuneOnce(x.names,\"y\") ## } ## df$group <- sample(1:splits, nrow(df), replace=TRUE) ## vhat <- df$d ## ehat <- df$y ## for(g in 1:splits) { ## sdf <- subset(df, group!=g) ## m <- m.hat.rf(sdf) ## mu <- mu.hat.rf(sdf) ## vhat[df$group==g] <- df$d[df$group==g] - ## predict(m, newx=df[df$group==g,x.names])$predictions ## ehat[df$group==g] <- df$y[df$group==g] - ## predict(mu, newx=df[df$group==g,x.names])$predictions ## } ## lm(ehat ~ vhat) ## } partial.linear.lasso <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) rlassoEffects(fmla, data=df, I = ~ d) } #summary(partial.linear.lasso(df)) # simulate a bunch of times in parallel simulations <- 500 # number of simulations library(parallel) cl <- makeCluster(detectCores()/2) # change as you see fit clusterEvalQ(cl,library(hdm)) clusterEvalQ(cl,library(grf)) # R Socket cluster spawns new R sessions with empty environments, we # need to make sure they load any needed libraries and have access to # things from the main environment that they use design <- c(\"linear\") #,\"step\") sim.df <- data.frame() start.time <- Sys.time() for (d in design) { if (d==\"linear\") { m <- m.linear mu <- mu.linear } else { m <- m.step mu <- mu.step } for (p in c(2,4,6,8)) { for (n in c(100, 200, 400, 800, 1600)) { clusterExport(cl,c(\"simulate\",\"partial.linear.lasso\", \"partial.linear.rf\",\"p\",\"mu\",\"m\", \"mrfparams\",\"murfparams\", \"n.save\")) # \"partial.linear.split.rf\", \"n.save.split\", # \"m.hat.rf\",\"mu.hat.rf\",\"rf.tuneOnce\")) thetas <- parSapply(cl, rep(n,simulations), function(n) { df <- simulate(n, p, mu, m) x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) c(lm(fmla,data=df)$coef[2], partial.linear.rf(df)$coef[2], #partial.linear.split.rf(df)$coef[2], partial.linear.lasso(df)$coefficients) } ) tmp <- (data.frame(t(thetas)) - 1)*sqrt(n) names(tmp) <- c(\"OLS\",\"Random.Forest\",\"Lasso\") tmp$n <- n tmp$p <- p tmp$design <- d sim.df <- rbind(sim.df, tmp) cat(\"finished sample size \",n,\"\\n\") cat(\"Elapsed time \", Sys.time()-start.time,\"\\n\") } cat(\"finished p = \",p,\"\\n\") } cat(\"finished design = \", d,\"\\n\") } stopCluster(cl) save(sim.df, file=\"partialLinearSim.Rdata\") library(ggplot2) library(reshape2) library(latex2exp) TeX <- latex2exp::TeX load(\"partialLinearSim.Rdata\") # see partialLinearSim.R for simulation # code df <- melt(sim.df, measure.vars=c(\"OLS\",\"Random.Forest\",\"Lasso\")) ggplot(subset(df,p==2), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(TeX('$\\\\sqrt{n}(\\\\hat{\\\\theta}-\\\\theta_0)$')) + ggtitle(\"p=2\") ggplot(subset(df,p==4), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=4\") ggplot(subset(df,p==6), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=6\") ggplot(subset(df,p==8), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=8\") Random forests do not seem to work very well in this context. Even when the functions being estimated are step functions, random forests do not produce a good estimate of $\\theta$. One caveat here is that I was not very careful about the tuning parameters for the random forests. It\u2019s possible that there exists a careful choice of tuning parameters that results in a better estimator. Research idea: create a generalization of random forests that is adaptive to the smoothness of the function being estimated. Two classic papers on adaptive regression estimators are Speckman ( 1985 ) and Donoho and Johnstone ( 1995 ). Friedberg et al. ( 2018 ) develop a local linear forest estimator. Combining their idea of using forests to form local neighborhoods with a smoothness adaptive variant of kernel or local polynomial regression should lead to a smoothness adaptive forest. Neural Networks \u00b6 Target function $f: \\R^p \\to \\R$ e.g. $f(x) = \\Er[y|x]$ Approximate with single hidden layer neural network : \\hat{f}(x) = \\sum_{j=1}^r \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j) Activation function $\\psi$ Examples: Sigmoid $\\psi(t) = 1/(1+e^{-t})$, Tanh $\\psi(t) = \\frac{e^t -e^{-t}}{e^t + e^{-t}}$, Heavyside $\\psi(t) = t 1(t\\geq 0)$ Weights $a_j$ Bias $b_j$ Able to approximate any $f$, Hornik, Stinchcombe, and White ( 1989 ) library(RSNNS) library(devtools) # download plot.nnet function from github source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r') load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) xn <- normalizeData(mod$x[,2:ncol(mod$x)]) yn <- normalizeData(mod$y) nn <- mlp(x=xn, y=yn, linOut=TRUE, size=c(10)) plot.nnet(nn, x.lab=colnames(mod$x)[2:ncol(mod$x)], y.lab=\"transProfit\") Deep Neural Networks \u00b6 Many hidden layers $x^{(0)} = x$ $x^{(\\ell)}_j = \\psi(a_j^{(\\ell)} x^{(\\ell-1)} + b_j^{(\\ell)})$ nn <- mlp(x=xn, y=yn, linOut=TRUE, size=c(5,10,3,5, 6)) plot.nnet(nn, x.lab=colnames(mod$x)[2:ncol(mod$x)], y.lab=\"transProfit\") Rate of convergence \u00b6 Chen and White ( 1999 ) $f(x) = \\Er[y|x]$ with Fourier representation f(x) = \\int e^{i a'x} d\\sigma_f(a) where $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$ Network sieve : \\begin{align*} \\mathcal{G}_n = \\{ & g: g(x) = \\sum_{j=1}^{r_n} \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j), \\\\ & \\norm{\\beta}_1 \\leq B_n \\} \\end{align*} The setup in Chen and White ( 1999 ) is more general. They consider estimating both $f$ and its first $m$ derivatives. Here, we focus on the case of just estimating $f$. Chen and White ( 1999 ) also consider estimation of functions other than conditional expectations. The restriction on $f$ in the second bullet is used to control approximation error. The second bullet says that $f$ is the inverse Fourier transform of measure $\\sigma_f$. The bite of the restriction on $f$ comes from the requirement that $\\sigma_f$ be absolutely integral, $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$. It would be a good exercise to check whether this restriction is satisfied by some familiar types of functions. Barron ( 1993 ) first showed that neural networks approximate this class of functions well, and compares the approximation rate of neural networks to other function approximation results. Rate of convergence [rate-of-convergence-3] \u00b6 Estimate \\hat{f} = \\argmin_{g \\in \\mathcal{G}_n} \\En [(y_i - g(x_i))^2] For fixed $p$, if $r_n^{2(1+1/(1+p))} \\log(r_n) = O(n)$, $B_n \\geq$ some constant \\Er[(\\hat{f}(x) - f(x))^2] = O\\left((n/\\log(n))^{\\frac{-(1 + 2/(p+1))} {2(1+1/(p+1))}}\\right) It is easy to see that regardless of $p$, $\\sqrt{n}\\Er[(\\hat{f}(x) - f(x))^2] \\to 0$. Therefore, neural networks would be suitable for estimating the nuisance functions in our examples above. There is a gap between applied use of neural networks and this statistical theory. These rate results are for networks with a single hidden layer. In prediction applications, the best performance is typically achieved by deep neural networks with many hidden layers. Intuitively, multiple hidden layers should do at least as well as a single hidden layer. There are some recent theoretical results that formalize this intuition. FIXME: ADD CITATIONS. Simulation Study \u00b6 Same setup as for random forests earlier Partially linear model DGP : $x_i \\in \\R^p$ with $x_{ij} \\sim U(0,1)$ $d_i = m(x_i) + v_i$ $y_i = d_i\\theta + f(x_i) + \\epsilon_i$ $m()$, $f()$ either linear or step functions Estimate by OLS, Neural network with & without cross-fitting Using orthogonal moments \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0 rm(list=ls()) p <- 4 # number of x's mu.linear <- function(x) x%*%rep(1,p) m.linear <- function(x) x%*%rep(2,p) mu.step <- function(x) (x>0.5)%*%rep(1,p) m.step <- function(x) (x>0.5)%*%rep(2,p) theta <- 1 simulate <- function(n,p,mu,m) { theta <- 1 x <- matrix(runif(n*p), ncol=p) d <- m(x) + rnorm(n) y <- theta*d + mu(x) + rnorm(n) data.frame(y=y,d=d,x=x) } library(grf) library(hdm) df <- simulate(100,p,mu.linear,m.linear) mrfparams <- NULL murfparams <- NULL n.save <- NULL partial.linear.rf <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] if (is.null(mrfparams) || n.save!=nrow(df)) { # to save time, we only tune once per cluster worker and data set # size cat(\"tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=1000, tune.parameters=TRUE) mrfparams <<- m.rf$tunable.params mu.rf <- regression_forest(df[,x.names], df$y, num.trees=1000, tune.parameters=TRUE) n.save <<- nrow(df) murfparams <<- mu.rf$tunable.params } else { cat(\"not tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(mrfparams[\"min.node.size\"]), alpha = as.numeric(mrfparams[\"alpha\"]), imbalance.penalty=as.numeric(mrfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(mrfparams[\"sample.fraction\"]), mtry=as.numeric(mrfparams[\"mtry\"])) mu.rf <- regression_forest(df[,x.names], df$y, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(murfparams[\"min.node.size\"]), alpha = as.numeric(murfparams[\"alpha\"]), imbalance.penalty=as.numeric(murfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(murfparams[\"sample.fraction\"]), mtry=as.numeric(murfparams[\"mtry\"])) } vhat <- df$d - predict(m.rf)$predictions ehat <- df$y - predict(mu.rf)$predictions lm(ehat ~ vhat) } ## Manual sample splitting --- this turns out to be unneccessary. The ## default behavior of predict.regression_forest is to return ## predictions on the training data using only trees that were not fit ## on each observation. In other words, regression_forest already does ## the sample splitting for us. ## ## rf.tuneOnce <- function(x.names, y.name) { ## parms <- NULL ## function(df) { ## if (is.null(parms)) { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=500, ## tune.parameters=TRUE) ## parms <<- rf$tunable.params ## rf ## } else { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=200, ## tune.parameters=FALSE, ## honesty=FALSE, ## min.node.size = ## as.numeric(parms[\"min.node.size\"]), ## alpha = as.numeric(parms[\"alpha\"]), ## imbalance.penalty=as.numeric(parms[\"imbalance.penalty\"]), ## sample.fraction = as.numeric(parms[\"sample.fraction\"]), ## mtry=as.numeric(parms[\"mtry\"])) ## } ## } ## } ## n.save.split <- NULL ## m.hat.rf <- NULL ## mu.hat.rf <- NULL ## partial.linear.split.rf <- function(df , splits=3) { ## x.names <- names(df)[grep(\"x.\",names(df))] ## if (is.null(n.save.split) || n.save.split != nrow(df)) { ## n.save.split <<- nrow(df) ## m.hat.rf <<- rf.tuneOnce(x.names,\"d\") ## mu.hat.rf <<- rf.tuneOnce(x.names,\"y\") ## } ## df$group <- sample(1:splits, nrow(df), replace=TRUE) ## vhat <- df$d ## ehat <- df$y ## for(g in 1:splits) { ## sdf <- subset(df, group!=g) ## m <- m.hat.rf(sdf) ## mu <- mu.hat.rf(sdf) ## vhat[df$group==g] <- df$d[df$group==g] - ## predict(m, newx=df[df$group==g,x.names])$predictions ## ehat[df$group==g] <- df$y[df$group==g] - ## predict(mu, newx=df[df$group==g,x.names])$predictions ## } ## lm(ehat ~ vhat) ## } partial.linear.lasso <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) rlassoEffects(fmla, data=df, I = ~ d) } #summary(partial.linear.lasso(df)) # simulate a bunch of times in parallel simulations <- 500 # number of simulations library(parallel) cl <- makeCluster(detectCores()/2) # change as you see fit clusterEvalQ(cl,library(hdm)) clusterEvalQ(cl,library(grf)) # R Socket cluster spawns new R sessions with empty environments, we # need to make sure they load any needed libraries and have access to # things from the main environment that they use design <- c(\"linear\") #,\"step\") sim.df <- data.frame() start.time <- Sys.time() for (d in design) { if (d==\"linear\") { m <- m.linear mu <- mu.linear } else { m <- m.step mu <- mu.step } for (p in c(2,4,6,8)) { for (n in c(100, 200, 400, 800, 1600)) { clusterExport(cl,c(\"simulate\",\"partial.linear.lasso\", \"partial.linear.rf\",\"p\",\"mu\",\"m\", \"mrfparams\",\"murfparams\", \"n.save\")) # \"partial.linear.split.rf\", \"n.save.split\", # \"m.hat.rf\",\"mu.hat.rf\",\"rf.tuneOnce\")) thetas <- parSapply(cl, rep(n,simulations), function(n) { df <- simulate(n, p, mu, m) x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) c(lm(fmla,data=df)$coef[2], partial.linear.rf(df)$coef[2], #partial.linear.split.rf(df)$coef[2], partial.linear.lasso(df)$coefficients) } ) tmp <- (data.frame(t(thetas)) - 1)*sqrt(n) names(tmp) <- c(\"OLS\",\"Random.Forest\",\"Lasso\") tmp$n <- n tmp$p <- p tmp$design <- d sim.df <- rbind(sim.df, tmp) cat(\"finished sample size \",n,\"\\n\") cat(\"Elapsed time \", Sys.time()-start.time,\"\\n\") } cat(\"finished p = \",p,\"\\n\") } cat(\"finished design = \", d,\"\\n\") } stopCluster(cl) save(sim.df, file=\"partialLinearSim.Rdata\") library(ggplot2) library(reshape2) library(latex2exp) TeX <- latex2exp::TeX load(\"partialLinearSimNet.Rdata\") # see partialLinearSim.R for simulation # code df <- melt(sim.df, measure.vars=names(sim.df)[1:3]) ggplot(subset(df,p==2), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(TeX('$\\\\sqrt{n}(\\\\hat{\\\\theta}-\\\\theta_0)$')) + ggtitle(\"p=2\") ggplot(subset(df,p==4), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=4\") ggplot(subset(df,p==6), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=6\") ggplot(subset(df,p==8), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=8\") The performance of the neural network estimator appears okay, but not outstanding in these simulations. In the linear model, the neural network estimator performs slightly worse than OLS. In the step function model, the neural network estimator performs slight better than the misspecified OLS, but neither appears to work well. In both cases, it appears that the neural network estimator produces occassional outliers. I believe that this is related to the fact that the minimization problem defining the neural network is actually very difficult to solve. In the simulation above, I suspect the outlying estimates are due to minimization problems. In the simulations, I simply set $r_n = n^{1/(2(1+1/(1+p)))}$. It\u2019s likely that a more careful choice of $r_n$, perhaps using cross-validation, would give better results. Bibliography \u00b6 Barron, A. R. 1993. \u201cUniversal Approximation Bounds for Superpositions of a Sigmoidal Function.\u201d IEEE Transactions on Information Theory 39 (3): 930\u201345. https://doi.org/10.1109/18.256500 . Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen. 2012. \u201cSparse Models and Methods for Optimal Instruments with an Application to Eminent Domain.\u201d Econometrica 80 (6): 2369\u20132429. https://doi.org/10.3982/ECTA9626 . Belloni, Alexandre, and Victor Chernozhukov. 2011. \u201cHigh Dimensional Sparse Econometric Models: An Introduction.\u201d In Inverse Problems and High-Dimensional Estimation: Stats in the Ch\u00e2teau Summer School, August 31 - September 4, 2009 , edited by Pierre Alquier, Eric Gautier, and Gilles Stoltz, 121\u201356. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-19989-9_3 . Biau, G\u00e9rard. 2012. \u201cAnalysis of a Random Forests Model.\u201d Journal of Machine Learning Research 13 (Apr): 1063\u201395. http://www.jmlr.org/papers/v13/biau12a.html . Caner, Mehmet, and Anders Bredahl Kock. 2018. \u201cAsymptotically Honest Confidence Regions for High Dimensional Parameters by the Desparsified Conservative Lasso.\u201d Journal of Econometrics 203 (1): 143\u201368. https://doi.org/https://doi.org/10.1016/j.jeconom.2017.11.005 . Chen, Xiaohong, and H. White. 1999. \u201cImproved Rates and Asymptotic Normality for Nonparametric Neural Network Estimators.\u201d IEEE Transactions on Information Theory 45 (2): 682\u201391. https://doi.org/10.1109/18.749011 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. \u201chdm: High-Dimensional Metrics.\u201d R Journal 8 (2): 185\u201399. https://journal.r-project.org/archive/2016/RJ-2016-040/index.html . Chetverikov, Denis, Zhipeng Liao, and Victor Chernozhukov. 2016. \u201cOn Cross-Validated Lasso.\u201d https://arxiv.org/abs/1605.02214 . Donoho, David L., and Iain M. Johnstone. 1995. \u201cAdapting to Unknown Smoothness via Wavelet Shrinkage.\u201d Journal of the American Statistical Association 90 (432): 1200\u20131224. http://www.jstor.org/stable/2291512 . Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference . Vol. 5. Cambridge University Press. https://web.stanford.edu/~hastie/CASI/ . Friedberg, Rina, Julie Tibshirani, Susan Athey, and Stefan Wager. 2018. \u201cLocal Linear Forests.\u201d https://arxiv.org/abs/1807.11408 . Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning . Springer series in statistics. https://web.stanford.edu/~hastie/ElemStatLearn/ . Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. \u201cMultilayer Feedforward Networks Are Universal Approximators.\u201d Neural Networks 2 (5): 359\u201366. https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8 . James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning . Vol. 112. Springer. http://www-bcf.usc.edu/%7Egareth/ISL/ . Lee, Jason D., Dennis L. Sun, Yuekai Sun, and Jonathan E. Taylor. 2016. \u201cExact Post-Selection Inference, with Application to the Lasso.\u201d Ann. Statist. 44 (3): 907\u201327. https://doi.org/10.1214/15-AOS1371 . Li, Ker-Chau. 1989. \u201cHonest Confidence Regions for Nonparametric Regression.\u201d Ann. Statist. 17 (3): 1001\u20138. https://doi.org/10.1214/aos/1176347253 . Mullainathan, Sendhil, and Jann Spiess. 2017. \u201cMachine Learning: An Applied Econometric Approach.\u201d Journal of Economic Perspectives 31 (2): 87\u2013106. https://doi.org/10.1257/jep.31.2.87 . Nickl, Richard, and Sara van de Geer. 2013. \u201cConfidence Sets in Sparse Regression.\u201d Ann. Statist. 41 (6): 2852\u201376. https://doi.org/10.1214/13-AOS1170 . Speckman, Paul. 1985. \u201cSpline Smoothing and Optimal Rates of Convergence in Nonparametric Regression Models.\u201d The Annals of Statistics 13 (3): 970\u201383. http://www.jstor.org/stable/2241119 . Stone, Charles J. 1982. \u201cOptimal Global Rates of Convergence for Nonparametric Regression.\u201d The Annals of Statistics 10 (4): 1040\u201353. http://www.jstor.org/stable/2240707 . Taylor, Jonathan, and Robert Tibshirani. 2017. \u201cPost-Selection Inference for -Penalized Likelihood Models.\u201d Canadian Journal of Statistics 46 (1): 41\u201361. https://doi.org/10.1002/cjs.11313 . Wager, Stefan, and Susan Athey. 2018. \u201cEstimation and Inference of Heterogeneous Treatment Effects Using Random Forests.\u201d Journal of the American Statistical Association 0 (0): 1\u201315. https://doi.org/10.1080/01621459.2017.1319839 . Wager, Stefan, and Guenther Walther. 2015. \u201cAdaptive Concentration of Regression Trees, with Application to Random Forests.\u201d https://arxiv.org/abs/1503.06388 .","title":"Methods"},{"location":"ml-methods/#introduction-to-machine-learning","text":"Friedman, Hastie, and Tibshirani ( 2009 ) and James et al. ( 2013 ) are commonly recommended textbooks on machine learning. James et al. ( 2013 ) is less technical of the two, but neither book is especially difficult. Efron and Hastie ( 2016 ) covers similar material and is slightly more advanced.","title":"Introduction to machine learning"},{"location":"ml-methods/#some-prediction-examples","text":"Machine learning is tailored for prediction, let\u2019s look at some data and see how well it works","title":"Some prediction examples"},{"location":"ml-methods/#predicting-house-prices","text":"Example from Mullainathan and Spiess ( 2017 ) Training on 10000 observations from AHS Predict log house price using 150 variables Holdout sample of 41808","title":"Predicting house prices"},{"location":"ml-methods/#ahs-variables-ahs-variables","text":"ahs <- readRDS(\"ahs2011forjep.rdata\")$df print(summary(ahs[,1:20])) ## LOGVALUE REGION METRO METRO3 PHONE KITCHEN ## Min. : 0.00 1: 5773 1:10499 1:11928 -7: 1851 1:51513 ## 1st Qu.:11.56 2:13503 2: 1124 2:39037 1 :49353 2: 295 ## Median :12.10 3:15408 3: 202 9: 843 2 : 604 ## Mean :12.06 4:17124 4: 103 ## 3rd Qu.:12.61 7:39880 ## Max. :15.48 ## MOBILTYP WINTEROVEN WINTERKESP WINTERELSP WINTERWOOD WINTERNONE ## -1:49868 -8: 133 -8: 133 -8: 133 -8: 133 -8: 133 ## 1 : 927 -7: 50 -7: 50 -7: 50 -7: 50 -7: 50 ## 2 : 1013 1 : 446 1 : 813 1 : 8689 1 : 61 1 :41895 ## 2 :51179 2 :50812 2 :42936 2 :51564 2 : 9730 ## ## ## NEWC DISH WASH DRY NUNIT2 BURNER COOK ## -9:50485 1:42221 1:50456 1:49880 1:44922 -6:51567 1:51567 ## 1 : 1323 2: 9587 2: 1352 2: 1928 2: 2634 1 : 87 2: 241 ## 3: 2307 2 : 154 ## 4: 1945 ## ## ## OVEN ## -6:51654 ## 1 : 127 ## 2 : 27 ## ## ## library(GGally) ggpairs(ahs[,c(\"LOGVALUE\",\"ROOMS\", \"LOT\",\"UNITSF\",\"BUILT\")], lower=list(continuous=\"points\", combo=\"facethist\", discrete=\"facetbar\"), diag=list(continuous=\"barDiag\",discrete=\"barDiag\")) + theme_minimal() # use ms-reproduce.R from course git repo to download and run Mullainathon & Spiess data and code to # create jepfittedmodels-table1.csv. Be aware that this will take many hours. tbl <- read.csv(\"jepfittedmodels-table1.csv\") tab <- tbl[,3:ncol(tbl)] rownames(tab) <- tbl[,2] tab <- tab[1:5, c(1,2,3,5)] colnames(tab) <- c(\"in sample MSE\", \"in sample R^2\", \"out of sample MSE\", \"out of sample R^2\") library(kableExtra) kable_styling(kable(tab, caption=\"Performance of different algorithms in predicting housing values\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Performance of different algorithms in predicting housing values in sample MSE in sample R\\^2 out of sample MSE out of sample R\\^2 OLS 0.589 0.473 0.674 0.417 Tree 0.675 0.396 0.758 0.345 Lasso 0.603 0.460 0.656 0.433 Forest 0.166 0.851 0.632 0.454 Ensemble 0.216 0.807 0.625 0.460 library(ggplot2) load(file=\"jeperrorfig.RData\") print(fig) load(file=\"jeperrorfig.RData\") print(fig2)","title":"AHS variables [ahs-variables]"},{"location":"ml-methods/#predicting-pipeline-revenues","text":"Data on US natural gas pipelines Combination of FERC Form 2, EIA Form 176, and other sources, compiled by me 1996-2016, 236 pipeline companies, 1219 company-year observations Predict: $y =$ profits from transmission of natural gas Covariates: year, capital, discovered gas reserves, well head gas price, city gate gas price, heating degree days, state(s) that each pipeline operates in load(\"pipelines.Rdata\") # data has problems before 1996 due to format change data <- subset(data,report_yr>=1996) # replace NA state weights with 0's data[,59:107][is.na(data[,59:107])] <- 0 # spaces in variable names will create problems later names(data) <- gsub(\" \",\".\",names(data)) summary(data[,c(\"transProfit\",\"transPlant_bal_beg_yr\",\"cityPrice\",\"wellPrice\")]) ## transProfit transPlant_bal_beg_yr cityPrice ## Min. : -31622547 Min. :0.000e+00 Min. : 0.4068 ## 1st Qu.: 2586031 1st Qu.:2.404e+07 1st Qu.: 3.8666 ## Median : 23733170 Median :1.957e+08 Median : 5.1297 ## Mean : 93517513 Mean :7.772e+08 Mean : 5.3469 ## 3rd Qu.: 129629013 3rd Qu.:1.016e+09 3rd Qu.: 6.5600 ## Max. :1165050214 Max. :1.439e+10 Max. :12.4646 ## NA's :2817 NA's :2692 NA's :1340 ## wellPrice ## Min. :0.0008 ## 1st Qu.:2.1230 ## Median :3.4370 ## Mean :3.7856 ## 3rd Qu.:5.1795 ## Max. :9.6500 ## NA's :2637 library(GGally) ggpairs(data[,c(\"transProfit\",\"transPlant_bal_beg_yr\",\"cityPrice\",\"wellPrice\")], lower=list(continuous=\"smooth\")) + theme_minimal()","title":"Predicting pipeline revenues"},{"location":"ml-methods/#predicting-pipeline-revenues-methods","text":"OLS : 67 covariates (year dummies and state(s) create a lot) Lasso Random forests Randomly choose 75% of sample to fit the models, then look at prediction accuracy in remaining 25% We are focusing on Lasso and random forests because these are the two methods that econometricians have worked on the most. Other methods such as neural nets and support vector machines are also worth exploring. For now, you can think of Lasso and random forests these as black boxes that generate predictions from data. We will go into more detail soon. ## Create X matrix for OLS and random forests xnames <-c(\"transPlant_bal_beg_yr\", \"reserve\", \"wellPrice\", \"cityPrice\", \"plantArea\", \"heatDegDays\", names(data)[59:107] ) yname <- \"transProfit\" fmla <- paste(yname,\"~\",paste(xnames,collapse=\" + \"),\"+ as.factor(report_yr)\") ols <- lm(fmla,data=data,x=TRUE,y=TRUE) X <- ols$x[,!(colnames(ols$x) %in% c(\"(Intercept)\")) & !is.na(ols$coefficients)] y <- ols$y train <- runif(nrow(X))<0.75 # OLS prediction on training set y.t <- y[train] X.t <- X[train,] ols <- lm(y.t ~ X.t) y.hat.ols <- ols$coefficients[1] + X %*% ols$coefficients[2:(length(ols$coef))] df <- data.frame(y=y, y.hat=y.hat.ols, train=train, method=\"ols\") ## Lasso library(glmnet) # Create larger X matrix for lasso fmla.l <- paste(yname,\"~ (\", paste(xnames,collapse=\" + \"),\")*(report_yr + transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays) + \", paste(sprintf(\"I(%s^2)\",xnames[1:6],collapse=\" + \")) ) reg <- lm(fmla.l, data=data, x=TRUE,y=TRUE) Xl <- reg$x[,!(colnames(reg$x) %in% c(\"(Intercept)\")) & !is.na(reg$coefficients)] lasso <- cv.glmnet(Xl[train,],y[train],alpha=1,parallel=FALSE, standardize=TRUE, intercept=TRUE, nfolds = 50) y.hat.lasso <- predict(lasso, Xl, s=lasso$lambda.min, type=\"response\") df <- rbind(df, data.frame(y=y, y.hat=as.vector(y.hat.lasso), train=train, method=\"lasso\")) ## Random forest library(grf) rf <- regression_forest(X[train,],y[train],tune.parameters = TRUE) y.hat.rf <- predict(rf, X)$predictions df <- rbind(df, data.frame(y=y, y.hat=y.hat.rf, train=train, method=\"random forest\")) # Neural network library(RSNNS) n <- nrow(X[train,]) p <- ncol(X) rn <- floor(n^(1/(2*(1+1/(1+p))))/2) xn <- normalizeData(X) yn <- normalizeData(y) nn <- mlp(x=xn[train,], y=yn[train], linOut=TRUE, size=rn) yn.hat.nn <- predict(nn, xn) y.hat.nn <- denormalizeData(yn.hat.nn, getNormParameters(yn)) df <- rbind(df, data.frame(y=y, y.hat=y.hat.nn, train=train, method=\"neural network\")) ggplot(data=df,aes(x=y,y=y.hat,colour=method,shape=train)) + geom_point(alpha=0.5) + geom_line(aes(y=y)) + theme_minimal() df$trainf <- factor(df$train, levels=c(\"TRUE\", \"FALSE\")) df$error <- df$y - df$y.hat ggplot(data=df,aes(x=error,colour=method)) + geom_density() + theme_minimal() + xlim(quantile(df$error,c(0.01,0.99))) + facet_grid(trainf ~ .,labeller=label_both) library(kableExtra) fn <- function(df) with(df,c(mean((y.hat - y)^2)/var(y), mean(abs(y.hat - y))/mean(abs(y-mean(y))))) tab1 <-unlist(by(subset(df,train), df$method[train], FUN=fn)) tab1 <- (matrix(tab1,nrow=2)) rownames(tab1) <- c(\"relative MSE\",\"relative MAE\") colnames(tab1) <- c(\"OLS\",\"Lasso\",\"Random forest\",\"Neural Network\") tab2 <- unlist(by(subset(df,!train), df$method[!train], FUN=fn)) tab2 <- (matrix(tab2,nrow=2)) rownames(tab2) <- c(\"relative MSE\",\"relative MAE\") colnames(tab2) <- c(\"OLS\",\"Lasso\",\"Random forest\",\"Neural Network\") kable_styling(kable(tab1, caption=\"Training sample\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=F) Training sample OLS Lasso Random forest Neural Network relative MSE 0.110 0.031 0.063 0.012 relative MAE 0.265 0.137 0.179 0.111 kable_styling(kable(tab2, caption=\"Hold-out sample\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=F) Hold-out sample OLS Lasso Random forest Neural Network relative MSE 0.125 0.064 0.093 0.092 relative MAE 0.288 0.199 0.232 0.278 In this table, relative MSE is the mean squared error relative to the variance of $y$, that is \\text{relative MSE} = \\frac{\\En[(y_i - \\hat{y}_i)^2]} {\\En[ (y_i - \\bar{y})^2]}. It is equal to $1-R^2$. Similarly, relative MAE is \\text{relative MAE} = \\frac{\\En[|y_i - \\hat{y}_i|]} {\\En[|y_i - \\bar{y}|]}. $\\En$ denotes the empirical expectation, $\\En[y_i] = \\frac{1}{n}\\sum_{i=1}^n y_i$.","title":"Predicting pipeline revenues : methods"},{"location":"ml-methods/#lasso","text":"Lasso solves a penalized (regularized) regression problem \\hat{\\beta} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] + \\frac{\\lambda}{n} \\norm{ \\hat{\\Psi} \\beta}_1 Penalty parameter $\\lambda$ Diagonal matrix $\\hat{\\Psi} = diag(\\hat{\\psi})$ Dimension of $x_i$ is $p$ and implicitly depends on $n$ can have $p >> n$ We are following the notation used in Chernozhukov, Hansen, and Spindler ( 2016 ). Note that this vignette has been updated since it was published in the R Journal. To obtain the most recent version, install the hdm package in R, load it, and then open the vignette. install.packages(\"hdm\") library(hdm) vignette(\"hdm_introduction\") The choice of penalty (or regularization) parameter, $\\lambda$, is important. When $\\lambda = 0$, Lasso is the same as OLS. As $\\lambda$ increases, the Lasso estimates will shrink toward 0. For large enough $\\lambda$, some components of $\\hat{\\beta}$ become exactly 0. As $\\lambda$ increases more, more and more components of $\\hat{\\beta}$ will be exactly $0$. For some intuition about why Lasso results in some coefficients being zero, note that \\hat{\\beta}^{lasso} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] + \\frac{\\lambda}{n} \\norm{\\beta}_1 is equivalent to \\hat{\\beta}^{lasso} = \\argmin_\\beta \\En [ (y_i - x_i'\\beta)^2 ] \\text{ s.t. } \\norm{\\beta}_1 \\leq s for some $s$. In this problem, the boundary of the constraint set will be a diamond. The level sets of the objective will be elipses. Generically, the solution will lie on one of the corners of the $\\norm{\\beta}_1 = 1$ set. See Friedman, Hastie, and Tibshirani ( 2009 ) or James et al. ( 2013 ) for more details. Most machine learning methods involve some form of regularization with an associated regularization parameter. In choosing the regularization parameter, we face a bias-variance tradeoff. As $\\lambda$ increases, variance decreases, but bias increases. Machine learning algorithms typically choose regularization parameters through cross-validation. Although cross-validation leads to good predictive performance, the statistical properties are not always known. Chernozhukov, Hansen, and Spindler ( 2016 ) say, \u201cIn high dimensional settings cross-validation is very popular; but it lacks a theoretical justification for use in the present context.\u201d However, there has been some recent progress on convergence rates for Lasso with cross-validation, see Chetverikov, Liao, and Chernozhukov ( 2016 ). The diagonal matrix $\\hat{\\Psi}$ is used to make the estimator invariant to scaling of $x_i$, and to allow for heteroskedasticity. If reading about Lasso or using code from other authors, be careful some do not include $\\hat{\\Psi}$ and use $\\lambda$ instead of $\\frac{\\lambda}{n}$. load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) library(glmnet) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) # standardize everything so that coefficients are similar scale when plotted mod$y <- (mod$y - mean(mod$y))/sd(mod$y) for(c in 2:ncol(mod$x)) { mod$x[,c] <- (mod$x[,c] - mean(mod$x[,c]))/sd(mod$x[,c]) } lassoPath <- glmnet(mod$x, mod$y, alpha=1) plot(lassoPath, xvar=\"lambda\", label=TRUE) load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) library(glmnet) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) # standardize everything so that coefficients are similar scale when plotted mod$y <- (mod$y - mean(mod$y))/sd(mod$y) for(c in 2:ncol(mod$x)) { mod$x[,c] <- (mod$x[,c] - mean(mod$x[,c]))/sd(mod$x[,c]) } cv <- cv.glmnet(mod$x, mod$y, alpha=1) plot(cv)","title":"Lasso"},{"location":"ml-methods/#statistical-properties-of-lasso","text":"Model : y_i = x_i'\\beta_0 + \\epsilon_i $\\Er[x_i \\epsilon_i] = 0$ $\\beta_0 \\in \\R^n$ $p$, $\\beta_0$, $x_i$, and $s$ implicitly depend on $n$ $\\log p = o(n^{1/3})$ $p$ may increase with $n$ and can have $p>n$ Sparsity $s$ Exact : $\\norm{\\beta_0}_0 = s = o(n)$ Approximate : $|\\beta_{0,j}| < Aj^{-a}$, $a > 1/2$, $s \\propto n^{1/(2a)}$ $\\norm{\\beta}_0$ is the number of non-zero components of $\\beta$. The approximate sparsity setting means if $|\\beta_{0,j}| < Aj^{-a}$, then, there exists a sparse approximation, say $\\beta_{a}$, with $s$ nonzero elements, such that the approximation error, \\En[(x_i'(\\beta_a - \\beta_0))^2] = c_s^2 will vanish quickly if $s \\propto n^{1/2a}$. Just how quickly will it vanish? An easy upper bound is \\begin{align*} c_s^2 \\leq & \\En\\left[\\left( \\sum_{j={s+1}}^p x_ij \\beta_{0,j} \\right)^2 \\right] \\\\ \\leq & \\En\\left[ \\left(\\sum_{j={s+1}}^p x_ij A j^{-a} \\right)^2 \\right] \\end{align*} To simplify the alegebra, let\u2019s assume $\\En[x_i x_i\u2019] = I_p$, then \\begin{align*} c_s^2 \\leq & \\sum_{j={s+1}}^p A^2 j^{-2a} \\\\ & \\leq \\sum_{j={s+1}}^\\infty A^2 j^{-2a} = A^2 s^{-2a} \\zeta(2a) \\\\ c_s^2 \\lesssim s^{-2a} \\end{align*} where $\\zeta(2a) = \\sum_{j=1}^\\infty j^{-2a}$ is the Riemann Zeta function (all that matters here is that $\\zeta(2a)$ is finite for $2a>1$). Then if $s \\propto n^{(1+\\delta)/2a}$, we would get $c_s \\lesssim n^{-(1+\\delta)/2}$. Importantly, $\\sqrt{n} c_s = o(1)$, so in the sort of expansions that we would do to show that $\\hat{\\theta}$ is $\\sqrt{n}$ asymptotically normal, the bias term would vanish.","title":"Statistical properties of Lasso"},{"location":"ml-methods/#rate-of-convergence","text":"With $\\lambda = 2c \\sqrt{n} \\Phi^{-1}(1-\\gamma/(2p))$ \\sqrt{\\En[(x_i'(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\sqrt{ (s/n) \\log (p) }, \\norm{\\hat{\\beta}^{lasso} - \\beta_0}_2 \\lesssim_P \\sqrt{ (s/n) \\log (p) }, and \\norm{\\hat{\\beta}^{lasso} - \\beta_0}_1 \\lesssim_P \\sqrt{ (s^2/n) \\log (p) } Constant $c>1$ Small $\\gamma \\to 0$ with $n$, and $\\log(1/\\gamma) \\lesssim \\log(p)$ Rank like condition on $x_i$ near-oracle rate In the semiparametric estimation problems that we\u2019re focused on, our object of interest is some finite dimensional parameter $\\theta$ that depends on our data some high dimensional parameter, like $\\beta_0$ in the Lasso. To analyze estimates of $\\hat{\\theta}(data, \\hat{\\beta})$, a key step will be to show that we can replace $\\hat{\\beta}$ with $\\beta_0$. The rate of convergence of $\\hat{\\beta}$ will be important for making this possible. Thus, the main thing that we care about for the Lasso and other machine learning estimators will be their rates of converagence. The notation $A_n \\lesssim_P B_n$ is read $A_n$ is bounded in probability by $B_n$ and means that for any $\\epsilon>0$, there exists $M$, $N$ such that $\\Pr(|A_n/B_n| > M) < \\epsilon$ for all $n > N$. This is also often denoted by $A_n = O(B_n)$. These rate results are from Belloni et al. ( 2012 ). Since this setup allows $p>n$, $x$ cannot be assumed to have full rank. Instead, an assumption about the eigenvalues of $X\u2019X$ restricted to the nonzero components of $\\beta_0$, plays a similar. See Belloni et al. ( 2012 ) for details. This convergence rate is called the near-oracle rate, because it is nearly as good as what we get if an oracle told us which components of $\\beta_0$ are nonzero. In that case OLS using just those $s$ components gives the fastest possible rate, which is \\sqrt{\\En[(x_i'(\\hat{\\beta}^{OLS} - \\beta_0))^2]} \\propto \\sqrt{s/n}.","title":"Rate of convergence"},{"location":"ml-methods/#rate-of-convergence-rate-of-convergence-1","text":"Using cross-validation to choose $\\lambda$ known bounds are worse With Gaussian errors: $\\sqrt{\\En[(x_i\u2019(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\sqrt{ (s/n) \\log (p) } \\log(pn)^{7/8}$, Without Gaussian error $\\sqrt{\\En[(x_i\u2019(\\hat{\\beta}^{lasso} - \\beta_0))^2 ] } \\lesssim_P \\left( \\frac{s \\log(pn)^2}{n} \\right)^{1/4}$ Chetverikov, Liao, and Chernozhukov ( 2016 ) These results are for an exactly sparse setting. Do they hold under approximate sparsity?","title":"Rate of convergence [rate-of-convergence-1]"},{"location":"ml-methods/#other-statistical-properties","text":"Inference on $\\beta$: not the goal in our motivating examples Difficult, but some recent results See Lee et al. ( 2016 ), Taylor and Tibshirani ( 2017 ), Caner and Kock ( 2018 ) Model selection: not the goal in our motivating examples Under stronger conditions, Lasso correctly selects the nonzero components of $\\beta_0$ See Belloni and Chernozhukov ( 2011 ) In the statistics literature on high dimensional and nonparametric estimation, you will come across the terms \u201cadaptive\u201d and \u201chonest.\u201d Adaptivity of an estimator refers to the situation where the rate of convergence depends on some unknown parameter. In the case of Lasso, the sparsity index of the true model, $s$, is an unknown parameter affecting the rate of convergence. Without knowing or estimating $s$, Lasso attains the above rate of convergence for a wide range of admissable $s$. Thus, Lasso is adaptive to the unknown sparsity index. \u201cHonest\u201d is a property of an inference method. A confidence region is honest if it has correct coverage for a large class of true models. For the Lasso, an honest confidence region would be valid for a wide range of sparsity, $s$. An honest, adaptive confidence region would be one that is valide for a wide range of $s$ and whose size shrinks as quickly as if $s$ were known. Achieving both adaptivity and honesty is impossible in the most general setting. For example, although an $\\ell$ times differentiable function of a $p$ dimensional variable can be adaptively estimated at rate $n^{-\\ell}{2\\ell + p}$, Li ( 1989 ) showed that an honest confidence region can contract at most at rate $n^{-1/4}$ (not adaptive to $\\ell$). However, an adaptive confidence region can be constructed if further restrictions are placed on the set of possible models, see Nickl and Geer ( 2013 ) for such a result for Lasso..","title":"Other statistical properties"},{"location":"ml-methods/#post-lasso","text":"Two steps : Estimate $\\hat{\\beta}^{lasso}$ ${\\hat{\\beta}}^{post} =$ OLS regression of $y$ on components of $x$ with nonzero $\\hat{\\beta}^{lasso}$ Same rates of convergence as Lasso Under some conditions post-Lasso has lower bias If Lasso selects correct model, post-Lasso converges at the oracle rate Post-Lasso removes some of the regularizaton bias of Lasso. The rate of convergence of post-Lasso is always as fast as Lasso, and under conditions that allow perfect model selection, post-Lasso converges slightly faster (by a factor $\\log(p)$). See Belloni et al. ( 2012 ) for details.","title":"Post-Lasso"},{"location":"ml-methods/#random-forests","text":"","title":"Random forests"},{"location":"ml-methods/#regression-trees","text":"$y_i \\in R$ on $x_i \\in \\R^p$ Want to estimate $\\Er[y | x]$ Locally constant estimate \\hat{t}(x) = \\sum_m^M c_m 1\\{x \\in R_m \\} Rectangular regions $R_m$ determined by tree","title":"Regression trees"},{"location":"ml-methods/#simulated-data","text":"n <- 1000 x <- runif(n) y <- runif(n) f <- function(x,z) { 1/3*(sin(5*x)*sqrt(z)*exp(-(z-0.5)^2)) } f0 <- f(x,y) z <- f0 + rnorm(n)*0.1 tree.df <- data.frame(x=x,y=y,z=z) # plot true function and data x.g <- seq(0,1,length.out=100) y.g <- seq(0,1,length.out=100) f0.g <- t(outer(x.g,y.g,f)) library(plotly) fig <- plot_ly( colors=\"YlOrBr\") fig <- add_markers(fig,x=x,y=y,z=z, size=0.3, opacity=0.2) fig <- add_surface(fig, x=x.g, y=y.g, z=f0.g, opacity=1) fig","title":"Simulated data"},{"location":"ml-methods/#estimated-tree","text":"# fit regression tree library(party) tree <- ctree(z ~ x + y, data=tree.df) # plot estimate x.g <- seq(0,1,length.out=100) y.g <- seq(0,1,length.out=100) df <- expand.grid(x.g,y.g) names(df) <- c(\"x\",\"y\") fhat.g <- matrix(predict(tree, newdata=df),nrow=length(x.g), byrow=TRUE) library(plotly) fig <- plot_ly(colors=\"YlOrBr\") fig <- add_markers(fig,x=x,y=y,z=z, size=0.3, opacity=0.2) fig <- add_surface(fig, x=x.g, y=y.g, z=fhat.g, opacity=1) fig","title":"Estimated tree"},{"location":"ml-methods/#estimated-tree_1","text":"plot(tree)","title":"Estimated tree"},{"location":"ml-methods/#tree-algorithm","text":"For each region, solve \\min_{j,s} \\left[ \\min_{c_1} \\sum_{i: x_{i,j} \\leq s, x_i \\in R} (y_i - c_1)^2 + \\min_{c_2} \\sum_{i: x_{i,j} > s, x_i \\in R} (y_i - c_2)^2 \\right] Repeat with $R = {x:x_{i,j} \\leq s^ } \\cap R$ and $R = {x:x_{i,j} \\leq s^ } \\cap R$ Stop when $|R| =$ some chosen minimum size Prune tree \\min_{tree \\subset T} \\sum (\\hat{f}(x)-y)^2 + \\alpha|\\text{terminal nodes in tree}| There are many variations on this tree building algorithm. They all share some rule to decide on which variable and where to split. They all have some kind of stopping rule, but not necessarily the same one. For example, some algorithms stop splitting into new branches when the improvement in $R^2$ becomes small. These trees don\u2019t need subsequent pruning, but also may fail to find later splits that might be important. As with lasso, regression trees involve some regularization. In the above description, both the minimum leaf size and $\\alpha$ in the pruning step serve as regularization parameters. A potential advantage of regression trees is that their output might be interpretable, especially if there are not many branches. Some disadvantages are that they often are not very good predictors, and small perturbations in data can lead to seemingly large changes in the tree.","title":"Tree algorithm"},{"location":"ml-methods/#random-forests_1","text":"Average randomized regression trees Trees randomized by Bootstrap or subsampling Randomize branches: \\min_{j \\in S,s} \\left[ \\min_{c_1} \\sum_{i: x_{i,j} \\leq s, x_i \\in R} (y_i - c_1)^2 + \\min_{c_2} \\sum_{i: x_{i,j} > s, x_i \\in R} (y_i - c_2)^2 \\right] where $S$ is random subset of ${1, \u2026, p}$ Variance reduction","title":"Random forests"},{"location":"ml-methods/#rate-of-convergence-regression-tree","text":"$x \\in [0,1]^p$, $\\Er[y|x]$ Lipschitz in $x$ Crude calculation for single tree, let denote $R_i$ node that contains $x_i$ \\begin{align*} \\Er(\\hat{t}(x_i) - \\Er[y|x_i])^2 = & \\overbrace{\\Er(\\hat{t}(x_i) - \\Er[y|x\\in R_i])^2}^{variance} + \\overbrace{(\\Er[y|x \\in R_i] - \\Er[y|x])^2}^{bias^2} \\\\ = & O_p(1/m) + O\\left(L^2 \\left(\\frac{m}{n}\\right)^{2/p}\\right) \\end{align*} optimal $m = O(n^{2/(2+p)})$ gives \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2] = O_p(n^{\\frac{-2}{2+p}}) By a crude calculation, I mean lets treat the tree as fixed. The the variance term is simply from estimating a conditional mean. This analysis could be made more rigorous by assuming the tree was estimated by sample splitting \u2014 use half the data to construct the tree and the remaining half to estimate the mean of $y$ in each node. Athey and Wager, and others, refer to such trees as \u201chonest.\u201d I suppose that this is because sample splitting facilitates honest inference afterward. The order of the bias term comes from considering the width of the pieces of a $p$ dimensional cube split evenly into $n/m$ pieces. Remember that for our motivating semiparametric problems, we need $\\sqrt{n} \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2]$ to vanish. The above rate convergence is too slow for $p>2$. The calculation of the above was admittedly crude, and may not be exact. However, Stone ( 1982 ) showed that if $\\Er[y|x]$ is $\\ell$ times differentiable, the fastest possible rate of convergence for any estimator is $n^{\\frac{-\\ell}{2\\ell + p}}$. To have any hope of a fast enough rate, we need to assume the function we\u2019re estimating is very smooth (high $\\ell$), or place some other restriction on the class of functions we allow (like sparsity for the Lasso). Lipschitz continuity is slightly weaker than once differentiable on a compact set, so it should come as no surprise that the rate of convergence would be slow.","title":"Rate of convergence: regression tree"},{"location":"ml-methods/#rate-of-convergence-random-forest","text":"Result from Biau ( 2012 ) Assume $\\Er[y|x]=\\Er[y|x_{(s)}]$, $x_{(s)}$ subset of $s$ variables, then \\Er[(\\hat{r}(x_i) - \\Er[y|x_i])^2] = O_p\\left(\\frac{1}{m\\log(n/m)^{s/2p}}\\right) + O_p\\left(\\left(\\frac{m}{n}\\right)^{\\frac{0.75}{s\\log 2}} \\right) or with optimal $m$ \\Er[(\\hat{t}(x_i) - \\Er[y|x_i])^2] = O_p(n^{\\frac{-0.75}{s\\log 2+0.75}}) This result from Biau ( 2012 ) assumes the forest is estimated with sample splitting. This avoids the difficult to analyze correlation between the nodes and $y$. Wager and Walther ( 2015 ) analyze what happens when the same data is used to construct the tree and average in each node. They get a slightly higher upper bound for the variance of $\\frac{\\log(p)\\log(n)}{m}$. Wager and Walther ( 2015 ) also allow $p$ to increase with $n$, whereas the previous analysis treated $p$ as fixed. These convergence rate results for random forests are not fast enough for our purpose. Does this mean that random forests should not be used in semiparametric estimation? Not necessarily. We\u2019re asking too much of random forests. There is no estimator for an arbitrary Lipschitz function that can have fast enough a rate of convergence. A restriction on the set of possible functions is needed to reduce the approximation bias. With Lasso, the assumption of (approximate) sparsity played that role. Chernozhukov et al. ( 2018 ) advise that random forests could be a good choice for semiparametric estimation when the function of interest is \u201cwell-approximated by a random forest.\u201d Unfortunately, there does not appear to be a clean mathematical way to describe the class of functions well-approximated by a forest.","title":"Rate of convergence: random forest"},{"location":"ml-methods/#other-statistical-properties_1","text":"Pointwise asymptotic normality : Wager and Athey ( 2018 )","title":"Other statistical properties"},{"location":"ml-methods/#simulation-study","text":"Partially linear model DGP : $x_i \\in \\R^p$ with $x_{ij} \\sim U(0,1)$ $d_i = m(x_i) + v_i$ $y_i = d_i\\theta + f(x_i) + \\epsilon_i$ $m()$, $f()$ either linear or step functions Estimate by OLS, Lasso, and random forest Lasso & random forest use orthogonal moments \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0 The point of this simulation is to see whether the slower convergence rate of random forests matters for the semiparametric problems we have in mind. Our theory results suggest that estimates of $\\theta$ using random forests with $p>2$ will be asymptotically biased. Specifically, the term d_n = \\En[(m(x_i) - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))] will be $O_p(n^{\\frac{-2}{2+p}})$, so $\\sqrt{n} d_n = O_p(n^{\\frac{p-2}{2(2+p)}})$. However, this calculation is only an upper bound on $d_n$. For a given DGP, $d_n$ might be smaller. In this simulation exercise, when $m()$ and $f()$ are linear, they are not easy to approximate by a regression tree, so I expect the random forest estimator to behave relatively poorly. OLS and Lasso on the other hand will do very well, and are included mainly as benchmarks. When $m()$ and $f()$ are step functions (specifically, $f(x) = m(x) = \\sum_{j=1}^p 1(x_{j}>1/2)$), I thought they would be well approximated by a regression tree (and random forest). For OLS and Lasso, $x$ is still only included linearly in the estimation, so those estimators will do poorly in the step function DGP. Throughout the simulation $p$ is much less than $n$, so Lasso and OLS will generally give very similar results. rm(list=ls()) p <- 4 # number of x's mu.linear <- function(x) x%*%rep(1,p) m.linear <- function(x) x%*%rep(2,p) mu.step <- function(x) (x>0.5)%*%rep(1,p) m.step <- function(x) (x>0.5)%*%rep(2,p) theta <- 1 simulate <- function(n,p,mu,m) { theta <- 1 x <- matrix(runif(n*p), ncol=p) d <- m(x) + rnorm(n) y <- theta*d + mu(x) + rnorm(n) data.frame(y=y,d=d,x=x) } library(grf) library(hdm) df <- simulate(100,p,mu.linear,m.linear) mrfparams <- NULL murfparams <- NULL n.save <- NULL partial.linear.rf <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] if (is.null(mrfparams) || n.save!=nrow(df)) { # to save time, we only tune once per cluster worker and data set # size cat(\"tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=1000, tune.parameters=TRUE) mrfparams <<- m.rf$tunable.params mu.rf <- regression_forest(df[,x.names], df$y, num.trees=1000, tune.parameters=TRUE) n.save <<- nrow(df) murfparams <<- mu.rf$tunable.params } else { cat(\"not tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(mrfparams[\"min.node.size\"]), alpha = as.numeric(mrfparams[\"alpha\"]), imbalance.penalty=as.numeric(mrfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(mrfparams[\"sample.fraction\"]), mtry=as.numeric(mrfparams[\"mtry\"])) mu.rf <- regression_forest(df[,x.names], df$y, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(murfparams[\"min.node.size\"]), alpha = as.numeric(murfparams[\"alpha\"]), imbalance.penalty=as.numeric(murfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(murfparams[\"sample.fraction\"]), mtry=as.numeric(murfparams[\"mtry\"])) } vhat <- df$d - predict(m.rf)$predictions ehat <- df$y - predict(mu.rf)$predictions lm(ehat ~ vhat) } ## Manual sample splitting --- this turns out to be unneccessary. The ## default behavior of predict.regression_forest is to return ## predictions on the training data using only trees that were not fit ## on each observation. In other words, regression_forest already does ## the sample splitting for us. ## ## rf.tuneOnce <- function(x.names, y.name) { ## parms <- NULL ## function(df) { ## if (is.null(parms)) { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=500, ## tune.parameters=TRUE) ## parms <<- rf$tunable.params ## rf ## } else { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=200, ## tune.parameters=FALSE, ## honesty=FALSE, ## min.node.size = ## as.numeric(parms[\"min.node.size\"]), ## alpha = as.numeric(parms[\"alpha\"]), ## imbalance.penalty=as.numeric(parms[\"imbalance.penalty\"]), ## sample.fraction = as.numeric(parms[\"sample.fraction\"]), ## mtry=as.numeric(parms[\"mtry\"])) ## } ## } ## } ## n.save.split <- NULL ## m.hat.rf <- NULL ## mu.hat.rf <- NULL ## partial.linear.split.rf <- function(df , splits=3) { ## x.names <- names(df)[grep(\"x.\",names(df))] ## if (is.null(n.save.split) || n.save.split != nrow(df)) { ## n.save.split <<- nrow(df) ## m.hat.rf <<- rf.tuneOnce(x.names,\"d\") ## mu.hat.rf <<- rf.tuneOnce(x.names,\"y\") ## } ## df$group <- sample(1:splits, nrow(df), replace=TRUE) ## vhat <- df$d ## ehat <- df$y ## for(g in 1:splits) { ## sdf <- subset(df, group!=g) ## m <- m.hat.rf(sdf) ## mu <- mu.hat.rf(sdf) ## vhat[df$group==g] <- df$d[df$group==g] - ## predict(m, newx=df[df$group==g,x.names])$predictions ## ehat[df$group==g] <- df$y[df$group==g] - ## predict(mu, newx=df[df$group==g,x.names])$predictions ## } ## lm(ehat ~ vhat) ## } partial.linear.lasso <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) rlassoEffects(fmla, data=df, I = ~ d) } #summary(partial.linear.lasso(df)) # simulate a bunch of times in parallel simulations <- 500 # number of simulations library(parallel) cl <- makeCluster(detectCores()/2) # change as you see fit clusterEvalQ(cl,library(hdm)) clusterEvalQ(cl,library(grf)) # R Socket cluster spawns new R sessions with empty environments, we # need to make sure they load any needed libraries and have access to # things from the main environment that they use design <- c(\"linear\") #,\"step\") sim.df <- data.frame() start.time <- Sys.time() for (d in design) { if (d==\"linear\") { m <- m.linear mu <- mu.linear } else { m <- m.step mu <- mu.step } for (p in c(2,4,6,8)) { for (n in c(100, 200, 400, 800, 1600)) { clusterExport(cl,c(\"simulate\",\"partial.linear.lasso\", \"partial.linear.rf\",\"p\",\"mu\",\"m\", \"mrfparams\",\"murfparams\", \"n.save\")) # \"partial.linear.split.rf\", \"n.save.split\", # \"m.hat.rf\",\"mu.hat.rf\",\"rf.tuneOnce\")) thetas <- parSapply(cl, rep(n,simulations), function(n) { df <- simulate(n, p, mu, m) x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) c(lm(fmla,data=df)$coef[2], partial.linear.rf(df)$coef[2], #partial.linear.split.rf(df)$coef[2], partial.linear.lasso(df)$coefficients) } ) tmp <- (data.frame(t(thetas)) - 1)*sqrt(n) names(tmp) <- c(\"OLS\",\"Random.Forest\",\"Lasso\") tmp$n <- n tmp$p <- p tmp$design <- d sim.df <- rbind(sim.df, tmp) cat(\"finished sample size \",n,\"\\n\") cat(\"Elapsed time \", Sys.time()-start.time,\"\\n\") } cat(\"finished p = \",p,\"\\n\") } cat(\"finished design = \", d,\"\\n\") } stopCluster(cl) save(sim.df, file=\"partialLinearSim.Rdata\") library(ggplot2) library(reshape2) library(latex2exp) TeX <- latex2exp::TeX load(\"partialLinearSim.Rdata\") # see partialLinearSim.R for simulation # code df <- melt(sim.df, measure.vars=c(\"OLS\",\"Random.Forest\",\"Lasso\")) ggplot(subset(df,p==2), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(TeX('$\\\\sqrt{n}(\\\\hat{\\\\theta}-\\\\theta_0)$')) + ggtitle(\"p=2\") ggplot(subset(df,p==4), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=4\") ggplot(subset(df,p==6), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=6\") ggplot(subset(df,p==8), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=8\") Random forests do not seem to work very well in this context. Even when the functions being estimated are step functions, random forests do not produce a good estimate of $\\theta$. One caveat here is that I was not very careful about the tuning parameters for the random forests. It\u2019s possible that there exists a careful choice of tuning parameters that results in a better estimator. Research idea: create a generalization of random forests that is adaptive to the smoothness of the function being estimated. Two classic papers on adaptive regression estimators are Speckman ( 1985 ) and Donoho and Johnstone ( 1995 ). Friedberg et al. ( 2018 ) develop a local linear forest estimator. Combining their idea of using forests to form local neighborhoods with a smoothness adaptive variant of kernel or local polynomial regression should lead to a smoothness adaptive forest.","title":"Simulation study"},{"location":"ml-methods/#neural-networks","text":"Target function $f: \\R^p \\to \\R$ e.g. $f(x) = \\Er[y|x]$ Approximate with single hidden layer neural network : \\hat{f}(x) = \\sum_{j=1}^r \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j) Activation function $\\psi$ Examples: Sigmoid $\\psi(t) = 1/(1+e^{-t})$, Tanh $\\psi(t) = \\frac{e^t -e^{-t}}{e^t + e^{-t}}$, Heavyside $\\psi(t) = t 1(t\\geq 0)$ Weights $a_j$ Bias $b_j$ Able to approximate any $f$, Hornik, Stinchcombe, and White ( 1989 ) library(RSNNS) library(devtools) # download plot.nnet function from github source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r') load(\"~/natural-gas-pipelines/dataAndCode/pipelines.Rdata\") data <- subset(data,report_yr>=1996) mod <- lm(transProfit ~ transPlant_bal_beg_yr + reserve + wellPrice + cityPrice + plantArea + heatDegDays, data=data, x=T, y=T) xn <- normalizeData(mod$x[,2:ncol(mod$x)]) yn <- normalizeData(mod$y) nn <- mlp(x=xn, y=yn, linOut=TRUE, size=c(10)) plot.nnet(nn, x.lab=colnames(mod$x)[2:ncol(mod$x)], y.lab=\"transProfit\")","title":"Neural Networks"},{"location":"ml-methods/#deep-neural-networks","text":"Many hidden layers $x^{(0)} = x$ $x^{(\\ell)}_j = \\psi(a_j^{(\\ell)} x^{(\\ell-1)} + b_j^{(\\ell)})$ nn <- mlp(x=xn, y=yn, linOut=TRUE, size=c(5,10,3,5, 6)) plot.nnet(nn, x.lab=colnames(mod$x)[2:ncol(mod$x)], y.lab=\"transProfit\")","title":"Deep Neural Networks"},{"location":"ml-methods/#rate-of-convergence_1","text":"Chen and White ( 1999 ) $f(x) = \\Er[y|x]$ with Fourier representation f(x) = \\int e^{i a'x} d\\sigma_f(a) where $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$ Network sieve : \\begin{align*} \\mathcal{G}_n = \\{ & g: g(x) = \\sum_{j=1}^{r_n} \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j), \\\\ & \\norm{\\beta}_1 \\leq B_n \\} \\end{align*} The setup in Chen and White ( 1999 ) is more general. They consider estimating both $f$ and its first $m$ derivatives. Here, we focus on the case of just estimating $f$. Chen and White ( 1999 ) also consider estimation of functions other than conditional expectations. The restriction on $f$ in the second bullet is used to control approximation error. The second bullet says that $f$ is the inverse Fourier transform of measure $\\sigma_f$. The bite of the restriction on $f$ comes from the requirement that $\\sigma_f$ be absolutely integral, $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$. It would be a good exercise to check whether this restriction is satisfied by some familiar types of functions. Barron ( 1993 ) first showed that neural networks approximate this class of functions well, and compares the approximation rate of neural networks to other function approximation results.","title":"Rate of convergence"},{"location":"ml-methods/#rate-of-convergence-rate-of-convergence-3","text":"Estimate \\hat{f} = \\argmin_{g \\in \\mathcal{G}_n} \\En [(y_i - g(x_i))^2] For fixed $p$, if $r_n^{2(1+1/(1+p))} \\log(r_n) = O(n)$, $B_n \\geq$ some constant \\Er[(\\hat{f}(x) - f(x))^2] = O\\left((n/\\log(n))^{\\frac{-(1 + 2/(p+1))} {2(1+1/(p+1))}}\\right) It is easy to see that regardless of $p$, $\\sqrt{n}\\Er[(\\hat{f}(x) - f(x))^2] \\to 0$. Therefore, neural networks would be suitable for estimating the nuisance functions in our examples above. There is a gap between applied use of neural networks and this statistical theory. These rate results are for networks with a single hidden layer. In prediction applications, the best performance is typically achieved by deep neural networks with many hidden layers. Intuitively, multiple hidden layers should do at least as well as a single hidden layer. There are some recent theoretical results that formalize this intuition. FIXME: ADD CITATIONS.","title":"Rate of convergence [rate-of-convergence-3]"},{"location":"ml-methods/#simulation-study_1","text":"Same setup as for random forests earlier Partially linear model DGP : $x_i \\in \\R^p$ with $x_{ij} \\sim U(0,1)$ $d_i = m(x_i) + v_i$ $y_i = d_i\\theta + f(x_i) + \\epsilon_i$ $m()$, $f()$ either linear or step functions Estimate by OLS, Neural network with & without cross-fitting Using orthogonal moments \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) - \\theta (d_i - \\hat{m}(x_i)))] = 0 rm(list=ls()) p <- 4 # number of x's mu.linear <- function(x) x%*%rep(1,p) m.linear <- function(x) x%*%rep(2,p) mu.step <- function(x) (x>0.5)%*%rep(1,p) m.step <- function(x) (x>0.5)%*%rep(2,p) theta <- 1 simulate <- function(n,p,mu,m) { theta <- 1 x <- matrix(runif(n*p), ncol=p) d <- m(x) + rnorm(n) y <- theta*d + mu(x) + rnorm(n) data.frame(y=y,d=d,x=x) } library(grf) library(hdm) df <- simulate(100,p,mu.linear,m.linear) mrfparams <- NULL murfparams <- NULL n.save <- NULL partial.linear.rf <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] if (is.null(mrfparams) || n.save!=nrow(df)) { # to save time, we only tune once per cluster worker and data set # size cat(\"tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=1000, tune.parameters=TRUE) mrfparams <<- m.rf$tunable.params mu.rf <- regression_forest(df[,x.names], df$y, num.trees=1000, tune.parameters=TRUE) n.save <<- nrow(df) murfparams <<- mu.rf$tunable.params } else { cat(\"not tuning\") m.rf <- regression_forest(df[,x.names], df$d, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(mrfparams[\"min.node.size\"]), alpha = as.numeric(mrfparams[\"alpha\"]), imbalance.penalty=as.numeric(mrfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(mrfparams[\"sample.fraction\"]), mtry=as.numeric(mrfparams[\"mtry\"])) mu.rf <- regression_forest(df[,x.names], df$y, num.trees=200, tune.parameters=FALSE, min.node.size = as.numeric(murfparams[\"min.node.size\"]), alpha = as.numeric(murfparams[\"alpha\"]), imbalance.penalty=as.numeric(murfparams[\"imbalance.penalty\"]), sample.fraction = as.numeric(murfparams[\"sample.fraction\"]), mtry=as.numeric(murfparams[\"mtry\"])) } vhat <- df$d - predict(m.rf)$predictions ehat <- df$y - predict(mu.rf)$predictions lm(ehat ~ vhat) } ## Manual sample splitting --- this turns out to be unneccessary. The ## default behavior of predict.regression_forest is to return ## predictions on the training data using only trees that were not fit ## on each observation. In other words, regression_forest already does ## the sample splitting for us. ## ## rf.tuneOnce <- function(x.names, y.name) { ## parms <- NULL ## function(df) { ## if (is.null(parms)) { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=500, ## tune.parameters=TRUE) ## parms <<- rf$tunable.params ## rf ## } else { ## rf <- regression_forest(df[,x.names], df[,y.name], num.trees=200, ## tune.parameters=FALSE, ## honesty=FALSE, ## min.node.size = ## as.numeric(parms[\"min.node.size\"]), ## alpha = as.numeric(parms[\"alpha\"]), ## imbalance.penalty=as.numeric(parms[\"imbalance.penalty\"]), ## sample.fraction = as.numeric(parms[\"sample.fraction\"]), ## mtry=as.numeric(parms[\"mtry\"])) ## } ## } ## } ## n.save.split <- NULL ## m.hat.rf <- NULL ## mu.hat.rf <- NULL ## partial.linear.split.rf <- function(df , splits=3) { ## x.names <- names(df)[grep(\"x.\",names(df))] ## if (is.null(n.save.split) || n.save.split != nrow(df)) { ## n.save.split <<- nrow(df) ## m.hat.rf <<- rf.tuneOnce(x.names,\"d\") ## mu.hat.rf <<- rf.tuneOnce(x.names,\"y\") ## } ## df$group <- sample(1:splits, nrow(df), replace=TRUE) ## vhat <- df$d ## ehat <- df$y ## for(g in 1:splits) { ## sdf <- subset(df, group!=g) ## m <- m.hat.rf(sdf) ## mu <- mu.hat.rf(sdf) ## vhat[df$group==g] <- df$d[df$group==g] - ## predict(m, newx=df[df$group==g,x.names])$predictions ## ehat[df$group==g] <- df$y[df$group==g] - ## predict(mu, newx=df[df$group==g,x.names])$predictions ## } ## lm(ehat ~ vhat) ## } partial.linear.lasso <- function(df) { x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) rlassoEffects(fmla, data=df, I = ~ d) } #summary(partial.linear.lasso(df)) # simulate a bunch of times in parallel simulations <- 500 # number of simulations library(parallel) cl <- makeCluster(detectCores()/2) # change as you see fit clusterEvalQ(cl,library(hdm)) clusterEvalQ(cl,library(grf)) # R Socket cluster spawns new R sessions with empty environments, we # need to make sure they load any needed libraries and have access to # things from the main environment that they use design <- c(\"linear\") #,\"step\") sim.df <- data.frame() start.time <- Sys.time() for (d in design) { if (d==\"linear\") { m <- m.linear mu <- mu.linear } else { m <- m.step mu <- mu.step } for (p in c(2,4,6,8)) { for (n in c(100, 200, 400, 800, 1600)) { clusterExport(cl,c(\"simulate\",\"partial.linear.lasso\", \"partial.linear.rf\",\"p\",\"mu\",\"m\", \"mrfparams\",\"murfparams\", \"n.save\")) # \"partial.linear.split.rf\", \"n.save.split\", # \"m.hat.rf\",\"mu.hat.rf\",\"rf.tuneOnce\")) thetas <- parSapply(cl, rep(n,simulations), function(n) { df <- simulate(n, p, mu, m) x.names <- names(df)[grep(\"x.\",names(df))] fmla <- as.formula(paste(c(\"y ~ d\",x.names), collapse=\" + \")) c(lm(fmla,data=df)$coef[2], partial.linear.rf(df)$coef[2], #partial.linear.split.rf(df)$coef[2], partial.linear.lasso(df)$coefficients) } ) tmp <- (data.frame(t(thetas)) - 1)*sqrt(n) names(tmp) <- c(\"OLS\",\"Random.Forest\",\"Lasso\") tmp$n <- n tmp$p <- p tmp$design <- d sim.df <- rbind(sim.df, tmp) cat(\"finished sample size \",n,\"\\n\") cat(\"Elapsed time \", Sys.time()-start.time,\"\\n\") } cat(\"finished p = \",p,\"\\n\") } cat(\"finished design = \", d,\"\\n\") } stopCluster(cl) save(sim.df, file=\"partialLinearSim.Rdata\") library(ggplot2) library(reshape2) library(latex2exp) TeX <- latex2exp::TeX load(\"partialLinearSimNet.Rdata\") # see partialLinearSim.R for simulation # code df <- melt(sim.df, measure.vars=names(sim.df)[1:3]) ggplot(subset(df,p==2), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(TeX('$\\\\sqrt{n}(\\\\hat{\\\\theta}-\\\\theta_0)$')) + ggtitle(\"p=2\") ggplot(subset(df,p==4), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=4\") ggplot(subset(df,p==6), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=6\") ggplot(subset(df,p==8), aes(x=value, colour=variable)) + facet_grid(n ~ design) + geom_density() + theme_minimal() + xlab(unname(TeX(\"$\\\\sqrt{n}(\\\\hat{\\\\theta} - \\\\theta_0)$\"))) + ggtitle(\"p=8\") The performance of the neural network estimator appears okay, but not outstanding in these simulations. In the linear model, the neural network estimator performs slightly worse than OLS. In the step function model, the neural network estimator performs slight better than the misspecified OLS, but neither appears to work well. In both cases, it appears that the neural network estimator produces occassional outliers. I believe that this is related to the fact that the minimization problem defining the neural network is actually very difficult to solve. In the simulation above, I suspect the outlying estimates are due to minimization problems. In the simulations, I simply set $r_n = n^{1/(2(1+1/(1+p)))}$. It\u2019s likely that a more careful choice of $r_n$, perhaps using cross-validation, would give better results.","title":"Simulation Study"},{"location":"ml-methods/#bibliography","text":"Barron, A. R. 1993. \u201cUniversal Approximation Bounds for Superpositions of a Sigmoidal Function.\u201d IEEE Transactions on Information Theory 39 (3): 930\u201345. https://doi.org/10.1109/18.256500 . Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen. 2012. \u201cSparse Models and Methods for Optimal Instruments with an Application to Eminent Domain.\u201d Econometrica 80 (6): 2369\u20132429. https://doi.org/10.3982/ECTA9626 . Belloni, Alexandre, and Victor Chernozhukov. 2011. \u201cHigh Dimensional Sparse Econometric Models: An Introduction.\u201d In Inverse Problems and High-Dimensional Estimation: Stats in the Ch\u00e2teau Summer School, August 31 - September 4, 2009 , edited by Pierre Alquier, Eric Gautier, and Gilles Stoltz, 121\u201356. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-19989-9_3 . Biau, G\u00e9rard. 2012. \u201cAnalysis of a Random Forests Model.\u201d Journal of Machine Learning Research 13 (Apr): 1063\u201395. http://www.jmlr.org/papers/v13/biau12a.html . Caner, Mehmet, and Anders Bredahl Kock. 2018. \u201cAsymptotically Honest Confidence Regions for High Dimensional Parameters by the Desparsified Conservative Lasso.\u201d Journal of Econometrics 203 (1): 143\u201368. https://doi.org/https://doi.org/10.1016/j.jeconom.2017.11.005 . Chen, Xiaohong, and H. White. 1999. \u201cImproved Rates and Asymptotic Normality for Nonparametric Neural Network Estimators.\u201d IEEE Transactions on Information Theory 45 (2): 682\u201391. https://doi.org/10.1109/18.749011 . Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. \u201chdm: High-Dimensional Metrics.\u201d R Journal 8 (2): 185\u201399. https://journal.r-project.org/archive/2016/RJ-2016-040/index.html . Chetverikov, Denis, Zhipeng Liao, and Victor Chernozhukov. 2016. \u201cOn Cross-Validated Lasso.\u201d https://arxiv.org/abs/1605.02214 . Donoho, David L., and Iain M. Johnstone. 1995. \u201cAdapting to Unknown Smoothness via Wavelet Shrinkage.\u201d Journal of the American Statistical Association 90 (432): 1200\u20131224. http://www.jstor.org/stable/2291512 . Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference . Vol. 5. Cambridge University Press. https://web.stanford.edu/~hastie/CASI/ . Friedberg, Rina, Julie Tibshirani, Susan Athey, and Stefan Wager. 2018. \u201cLocal Linear Forests.\u201d https://arxiv.org/abs/1807.11408 . Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of Statistical Learning . Springer series in statistics. https://web.stanford.edu/~hastie/ElemStatLearn/ . Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. \u201cMultilayer Feedforward Networks Are Universal Approximators.\u201d Neural Networks 2 (5): 359\u201366. https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8 . James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning . Vol. 112. Springer. http://www-bcf.usc.edu/%7Egareth/ISL/ . Lee, Jason D., Dennis L. Sun, Yuekai Sun, and Jonathan E. Taylor. 2016. \u201cExact Post-Selection Inference, with Application to the Lasso.\u201d Ann. Statist. 44 (3): 907\u201327. https://doi.org/10.1214/15-AOS1371 . Li, Ker-Chau. 1989. \u201cHonest Confidence Regions for Nonparametric Regression.\u201d Ann. Statist. 17 (3): 1001\u20138. https://doi.org/10.1214/aos/1176347253 . Mullainathan, Sendhil, and Jann Spiess. 2017. \u201cMachine Learning: An Applied Econometric Approach.\u201d Journal of Economic Perspectives 31 (2): 87\u2013106. https://doi.org/10.1257/jep.31.2.87 . Nickl, Richard, and Sara van de Geer. 2013. \u201cConfidence Sets in Sparse Regression.\u201d Ann. Statist. 41 (6): 2852\u201376. https://doi.org/10.1214/13-AOS1170 . Speckman, Paul. 1985. \u201cSpline Smoothing and Optimal Rates of Convergence in Nonparametric Regression Models.\u201d The Annals of Statistics 13 (3): 970\u201383. http://www.jstor.org/stable/2241119 . Stone, Charles J. 1982. \u201cOptimal Global Rates of Convergence for Nonparametric Regression.\u201d The Annals of Statistics 10 (4): 1040\u201353. http://www.jstor.org/stable/2240707 . Taylor, Jonathan, and Robert Tibshirani. 2017. \u201cPost-Selection Inference for -Penalized Likelihood Models.\u201d Canadian Journal of Statistics 46 (1): 41\u201361. https://doi.org/10.1002/cjs.11313 . Wager, Stefan, and Susan Athey. 2018. \u201cEstimation and Inference of Heterogeneous Treatment Effects Using Random Forests.\u201d Journal of the American Statistical Association 0 (0): 1\u201315. https://doi.org/10.1080/01621459.2017.1319839 . Wager, Stefan, and Guenther Walther. 2015. \u201cAdaptive Concentration of Regression Trees, with Application to Random Forests.\u201d https://arxiv.org/abs/1503.06388 .","title":"Bibliography"},{"location":"mlExamplePKH/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 Background \u00b6 Program Keluarga Harapan : pilot conditional cash transfer program in Indonesia Alatas et al. ( 2011 ), Triyana ( 2016 ) Conditional cash transfer: receive cash if Expectant women: 4 prenatal visits, iron supplement, delivery by doctor or midwife, 2 postnatal visits Children under 5: weighed monthly, vaccinated, vitamin A Quarterly transfer of 600,000-2,200,000 rupiah (\\$60 - \\$220) depending on household composition (15-20% of quaterly consumption) Randomized at subdistrict level : want to capture supply side effects that would occur if policy implemented everywhere Baseline characteristics \u00b6 if (!file.exists(\"Data_AEJPol-2014-0048/data_final/price_women.dta\")) { stop(\"Please download the data from https://www.aeaweb.org/articles?id=10.1257/pol.20140048 and unzip it in the current directory\") } if (!require(foreign)) install.packages(\"foreign\") library(foreign) all.data <- read.dta(\"Data_AEJPol-2014-0048/data_final/price_women.dta\") # Variables we will use vars <- c(\"rid_panel\",\"prov\",\"Location_ID\",\"dist\", \"wave\", \"edu\",\"agecat\",\"log_xp_percap\",#\"hh_land\",\"hh_home\", \"rhr031\", \"rhr032\", \"rhr034\", \"rhr036\",# \"hh_xp\", \"death_ch\",\"bw\",\"bw_low\", \"birthfac\", \"good_assisted_delivery\", \"dummy_A\", \"dummy_B\", \"dummy_D\", \"delivery_fees\", \"delivery_fees_prof\", \"delivery_fees_doc\", \"delivery_fees_midw\", \"delivery_fees_trad\", \"control_pkh\", \"pkh_kec_ever\", \"pkh_ever\", \"bw_kec\", \"bw_low_kec\", \"death_ch_kec\", \"dummy_A_base\", \"dummy_B_base\", \"dummy_D_base\", \"bid_pkh1\", \"bid_pkh1_base\" , \"delivery_fees_top\", \"delivery_fees_prof_top\", \"delivery_fees_doc_top\", \"delivery_fees_midw_top\", \"delivery_fees_trad_top\", \"delivery_fees_top_base\", \"delivery_fees_prof_top_base\", \"delivery_fees_doc_top_base\", \"delivery_fees_midw_top_base\", \"delivery_fees_trad_top_base\", names(all.data)[grep(\"^hh_\",names(all.data))], \"tv\", \"parabola\" , \"fridge\" , \"motorbike\", \"car\" , \"pig\" , \"goat\" , \"cow\" , \"horse\" ) pw <- all.data[,vars] ## Rename some variables names(pw)[names(pw)==\"dummy_A\"] <- \"doctor_birth\" names(pw)[names(pw)==\"dummy_A_base\"] <- \"doctor_birth_base\" names(pw)[names(pw)==\"dummy_B\"] <- \"midwife_birth\" names(pw)[names(pw)==\"dummy_B_base\"] <- \"midwife_birth_base\" names(pw)[names(pw)==\"dummy_D\"] <- \"traditional_birth\" names(pw)[names(pw)==\"dummy_D_base\"] <- \"traditional_birth_base\" # Set household variables to their pretreatment (wave=1) values for (v in c(\"edu\",\"agecat\",\"log_xp_percap\", \"rhr031\", \"rhr032\", \"rhr034\", \"rhr036\", names(pw)[grep(\"^hh_\",names(pw))], \"tv\",\"parabola\",\"fridge\",\"motorbike\",\"car\",\"pig\",\"goat\",\"cow\",\"horse\")) { temp <- ifelse(pw$wave==1,pw[,v],NA) temp0 <- ave(temp, pw$rid_panel, FUN=function(x) max(x,na.rm=TRUE)) pw[,v] <- ifelse(is.finite(temp0), temp0, pw[,v]) } pw$delivery_fees_trad_top[is.na(pw$delivery_fees_trad_top) & !is.na(pw$delivery_fees_midw_top)] <- 0 adf <- aggregate(subset(pw,wave==1), by=list(subset(pw,wave==1)$control_pkh), FUN=function(x) mean(x,na.rm=TRUE)) adf ## Group.1 rid_panel prov Location_ID dist wave edu agecat ## 1 0 NA NA NA NA 1 15.50180 3.405191 ## 2 1 NA NA NA NA 1 14.59808 3.428927 ## log_xp_percap rhr031 rhr032 rhr034 rhr036 death_ch ## 1 11.98384 0.3907714 0.5313627 0.4354722 0.4826965 0.01333814 ## 2 12.00990 0.4068941 0.5458422 0.4427861 0.4765458 0.01137171 ## bw bw_low birthfac good_assisted_delivery doctor_birth ## 1 3166.346 0.08163265 0.4353201 0.6392936 0.07902870 ## 2 3180.763 0.07678883 0.4402434 0.6232073 0.08778792 ## midwife_birth traditional_birth delivery_fees delivery_fees_prof ## 1 0.5876380 0.4189845 284483.6 243728.8 ## 2 0.5619296 0.4315515 298738.9 256254.9 ## delivery_fees_doc delivery_fees_midw delivery_fees_trad control_pkh ## 1 82076.82 178308.9 39173.57 0 ## 2 102721.64 176275.7 40711.49 1 ## pkh_kec_ever pkh_ever bw_kec bw_low_kec death_ch_kec doctor_birth_base ## 1 0 0 3165.103 0.08319263 0.01333814 0.07897035 ## 2 0 0 3180.449 0.08287012 0.01137171 0.08718138 ## midwife_birth_base traditional_birth_base bid_pkh1 bid_pkh1_base ## 1 0.5848753 0.4195088 3.574443e-02 3.574443e-02 ## 2 0.5585186 0.4353818 -1.469066e-17 -2.999183e-18 ## delivery_fees_top delivery_fees_prof_top delivery_fees_doc_top ## 1 274278.3 233523.5 64174.41 ## 2 278295.7 235811.7 72279.24 ## delivery_fees_midw_top delivery_fees_trad_top delivery_fees_top_base ## 1 167942.7 36862.44 273320.4 ## 2 162837.3 37162.07 275470.0 ## delivery_fees_prof_top_base delivery_fees_doc_top_base ## 1 232340.5 63631.20 ## 2 232639.0 70302.13 ## delivery_fees_midw_top_base delivery_fees_trad_top_base hh_serial_no_w1 ## 1 166914.5 37767.86 2.898702 ## 2 162082.5 38660.42 2.871713 ## hh_survey_date_w1 hh_adult hh_5_15 hh_u5 hh_serial_no_w2 ## 1 17368.93 3.426614 1.858130 1.631684 NaN ## 2 17367.48 3.363539 1.887016 1.638240 NaN ## hh_serial_no_w3 hh_u2 hh_3_6 hh_7_15 hh_phone hh_rf_tile ## 1 NaN 1.231982 1.496021 2.336163 0.1167988 0.7119683 ## 2 NaN 1.232099 1.467532 2.340625 0.1122957 0.7157072 ## hh_rf_shingle hh_rf_fiber hh_rf_oth hh_wall_plaster hh_wall_brick ## 1 0.2025955 0.1304975 0.04578226 0.3183129 0.1503244 ## 2 0.1943852 0.1339730 0.04442075 0.3471926 0.1368159 ## hh_wall_wood hh_wall_fiber hh_wall_oth hh_fl_tile hh_fl_plaster ## 1 0.2599135 0.3604903 0.03208363 0.2058399 0.3713050 ## 2 0.2356077 0.3681592 0.03020611 0.2160625 0.3901919 ## hh_fl_wood hh_fl_dirt hh_fl_oth hh_drink_pam hh_drink_mechwell ## 1 0.1398702 0.3738284 0.000000000 0.1528479 0.2025955 ## 2 0.1346837 0.3471926 0.000355366 0.1464108 0.2174840 ## hh_drink_well hh_drink_spring hh_drink_oth hh_drinkhome hh_drinkdist ## 1 0.4913482 0.1870944 0.08723864 0.2826244 1052.188 ## 2 0.4626866 0.1915423 0.09985785 0.2782516 1064.803 ## hh_waterdrink hh_water_pam hh_water_mechwell hh_water_well ## 1 0.8280461 0.1171593 0.2004326 0.4783706 ## 2 0.8312011 0.1055437 0.2235252 0.4552239 ## hh_water_spring hh_water_river hh_water_oth hh_waterhome hh_waterdist ## 1 0.1856525 0.09841384 0.04109589 0.05839942 610.1139 ## 2 0.1847903 0.09630419 0.04157783 0.05934613 634.7447 ## hh_toilet_own hh_toilet_pub hh_toilet_none hh_toilet_1 hh_toilet_2 ## 1 0.4340303 0.1784427 0.4480894 0.2811824 0.1049027 ## 2 0.4648188 0.1844350 0.4097370 0.3280028 0.1080313 ## hh_toilet_3 hh_waste_tank hh_waste_hole hh_waste_river hh_waste_field ## 1 0.2505407 0.2606345 0.2487383 0.3695025 0.2119683 ## 2 0.2313433 0.3120114 0.2341862 0.3454158 0.1968728 ## hh_waste_oth hh_listrik hh_pln hh_kitchen hh_cook_wood ## 1 0 0.8601298 0.8417448 0.8936554 0.8136265 ## 2 0 0.8681592 0.8464819 0.9033404 0.7782516 ## hh_cook_kerosene hh_cook_gas hh_cook_oth hh_kec hh_land hh_home ## 1 0.2105263 0.03640952 0 9400.184 0.3734679 0.8846431 ## 2 0.2469794 0.03375977 0 9618.802 0.3848614 0.8749112 ## hh_land_miss hh_home_miss hh_xp hh_xp_all tv parabola ## 1 0 0.03028118 3.024874 2.601298 0.5313627 0.03893295 ## 2 0 0.02949538 3.096304 2.676617 0.5458422 0.04015636 ## fridge motorbike car pig goat cow horse ## 1 0.05803893 0.2011536 0.03496756 0.1196828 0.1456381 0.1121125 0.03640952 ## 2 0.05685856 0.1986496 0.03269367 0.1364606 0.1552950 0.1186923 0.03802416 Main results \u00b6 Delivery attendant usage \u00b6 library(lfe) lhs <- \"pkh_kec_ever + doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | 0 | Location_ID\" itt <- list(felm(as.formula(paste(\"doctor_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"midwife_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"traditional_birth ~\",lhs)),data=pw)) lhs <- \"doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | (pkh_ever ~ pkh_kec_ever) | Location_ID\" iv <- list(felm(as.formula(paste(\"doctor_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"midwife_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"traditional_birth ~\",lhs)),data=pw)) library(stargazer) stargazer(itt, type=\"html\", title=\"ITT estimates\", dep.var.labels = c(\"Doctor\",\"Midwife\",\"Traditional\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) ITT estimates Dependent variable: Doctor Midwife Traditional (1) (2) (3) pkh_kec_ever 0.043 *** 0.094 *** -0.090 *** (0.012) (0.017) (0.016) Observations 6,629 6,629 6,629 Delivery attendant usage [delivery-attendant-usage-1] \u00b6 stargazer(iv, type=\"html\", title=\"IV effect of program participation\", dep.var.labels = c(\"Doctor\",\"Midwife\",\"Traditional\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) IV effect of program participation Dependent variable: Doctor Midwife Traditional (1) (2) (3) pkh_ever(fit) 0.091 *** 0.198 *** -0.189 *** (0.025) (0.036) (0.035) Observations 6,629 6,629 6,629 Health outcomes \u00b6 library(lfe) lhs <- \"pkh_kec_ever + doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | 0 | Location_ID\" itt <- list(felm(as.formula(paste(\"death_ch ~\",lhs)),data=pw), felm(as.formula(paste(\"bw ~\",lhs)),data=pw), felm(as.formula(paste(\"bw_low ~\",lhs)),data=pw)) lhs <- \"doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | (pkh_ever ~ pkh_kec_ever) | Location_ID\" iv <- list(felm(as.formula(paste(\"death_ch ~\",lhs)),data=pw), felm(as.formula(paste(\"bw ~\",lhs)),data=pw), felm(as.formula(paste(\"bw_low ~\",lhs)),data=pw)) library(stargazer) stargazer(itt, type=\"html\", title=\"ITT estimates\", dep.var.labels = c(\"Infant mortality\",\"Birthweight\",\"Low birthweight\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) ITT estimates Dependent variable: Infant mortality Birthweight Low birthweight (1) (2) (3) pkh_kec_ever 0.005 -5.559 0.017 (0.004) (23.805) (0.012) Observations 8,303 4,988 4,988 Health outcomes [health-outcomes-1] \u00b6 stargazer(iv, type=\"html\", title=\"IV effect of program participation\", dep.var.labels = c(\"Infant mortality\",\"Birthweight\",\"Lowbirthweigt\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) IV effect of program participation Dependent variable: Infant mortality Birthweight Lowbirthweigt (1) (2) (3) pkh_ever(fit) 0.011 -12.674 0.039 (0.008) (54.269) (0.026) Observations 8,303 4,988 4,988 Exploring heterogeneity \u00b6 Machine learning as proxy \u00b6 Generic machine learning approach of Chernozhukov et al. ( 2018 ) Estimate machine learning proxies for $B(x) = \\Er[y(0)|x]$ and $S(x) = \\Er[y(1) - y(0) |x]$ Use proxies to : Estimate best linear projection on true $\\Er[y(1) - y(0)|x]$ Estimate $\\Er[y(1) - y(0) | groups]$ Heterogeneity in CATE for Birthweight \u00b6 ## Function for Generic machine learning of Chernozhukov, Demirer, Duflo, & Fernandez-Val (2018) genericML <- function(x,y,treat, fit.function, predict.function, n.split=10, n.group=5, clusterid=NULL) { if (!is.null(clusterid)) require(sandwich) blp <- matrix(NA, nrow=n.split, ncol=2) blp.se <- blp gate <- matrix(NA, nrow=n.split, ncol=n.group) gate.se <- gate baseline <- matrix(NA, nrow=nrow(x), ncol=n.split) cate <- matrix(NA, nrow=nrow(x), ncol=n.split) Lambda <- matrix(NA, nrow=n.split, ncol=2) for(i in 1:n.split) { main <- runif(nrow(x))>0.5 fit1 <- fit.function(x[!main & treat==1,], y[!main & treat==1]) fit0 <- fit.function(x[!main & treat==0,], y[!main & treat==0]) B <- as.vector(predict.function(fit0,x)) S <- as.vector(predict.function(fit1,x)) - B baseline[,i] <- B cate[,i] <- S ES <- mean(S) ## BLP # assume P(treat|x) = P(treat) = mean(treat) p <- mean(treat) df <- data.frame(y, B, treat, S, main) reg <- lm(y ~ B + I(treat-p) + I((treat-p)*(S-ES)), data=subset(df, main)) blp[i,] <- reg$coef[3:4] if (is.null(clusterid)) blp.se[i,] <- sqrt(diag(vcovHC(reg))[3:4]) else blp.se[i,] <- sqrt(diag(vcovCL(reg, clusterid[main]))[3:4]) Lambda[i,1] <- reg$coefficient[4]^2*var(S) ## GATES cut <- quantile(S, probs=seq(0,1,length.out=(n.group+1))) cut[n.group+1] <- cut[n.group+1] + 1 for(k in 1:n.group) { df[,sprintf(\"G.%d\",k)] <- (cut[k]<=S & S<cut[k+1]) } greg <- lm(as.formula(paste(c(\"y ~ B \", sprintf(\"I((treat-p)*G.%d)\",1:n.group)), collapse=\" + \")), data=subset(df,main)) gc <- grep(\"G\", names(greg$coefficients)) gate[i,] <- greg$coefficients[gc] if (is.null(clusterid)) gate.se[i,] <- sqrt(diag(vcovHC(greg))[gc]) else gate.se[i,] <- sqrt(diag(vcovCL(greg, clusterid[main]))[gc]) Lambda[i,2] <- sum(gate[i,]^2)/n.group } out <- list( gate=gate, gate.se=gate.se, blp=blp, blp.se=blp.se, Lambda=Lambda, baseline=baseline, cate=cate) } genericML.summary <- function(gml) { blp <- apply(gml$blp, 2, function(x) median(x, na.rm=TRUE)) blp.se <- apply(gml$blp.se, 2, function(x) median(x, na.rm=TRUE)) gate <- apply(gml$gate, 2, function(x) median(x, na.rm=TRUE)) gate.se <- apply(gml$gate.se, 2, function(x) median(x, na.rm=TRUE)) Lambda <- apply(gml$Lambda, 2, function(x) median(x, na.rm=TRUE)) return(list(blp=blp, blp.se=blp.se, gate=gate, gate.se=gate.se, Lambda=Lambda)) } library(glmnet) # create x matrix fmla.l <- bw ~ pkh_kec_ever + as.factor(edu)*as.factor(agecat) + log_xp_percap + hh_land + hh_home + as.factor(dist) + hh_phone + hh_rf_tile + hh_rf_shingle + hh_rf_fiber + hh_wall_plaster + hh_wall_brick + hh_wall_wood + hh_wall_fiber + hh_fl_tile + hh_fl_plaster + hh_fl_wood + hh_fl_dirt + hh_water_pam + hh_water_mechwell + hh_water_well + hh_water_spring + hh_water_river + hh_waterhome + hh_toilet_own + hh_toilet_pub + hh_toilet_none + hh_waste_tank + hh_waste_hole + hh_waste_river + hh_waste_field + hh_kitchen + hh_cook_wood + hh_cook_kerosene + hh_cook_gas + tv + fridge + motorbike + car + goat + cow + horse m <- lm(fmla.l, data=pw, x=TRUE, y=TRUE) treat <- m$x[,2] Xl <- m$x[,3:ncol(m$x)] scale <- sd(m$y) center <- mean(m$y) yl <- (m$y-center)/scale lid <- as.factor(pw[as.numeric(rownames(m$x)),]$Location_ID) gml.lasso <- genericML(Xl,m$y, treat, function(x,y) cv.glmnet(x,(y-center)/scale,alpha=1,parallel=FALSE, intercept=TRUE, nfolds=20), function(model, x) { predict(model, x, s=model$lambda.min, type=\"response\")*scale + center }, n.split=11,n.group=5, clusterid=lid) library(grf) gml.rf <- genericML(Xl,m$y, treat, function(x,y) regression_forest(x, (y-center)/scale, tune.parameters=TRUE), function(model, x) { predict(model,x)$predictions*scale + center}, n.split=11,n.group=5, clusterid=lid) library(RSNNS) gml.nn <- genericML(Xl,m$y, treat, function(x,y) mlp(x,(y-center)/scale,linOut=TRUE, size=c(10,10), learnFunc=\"SCG\"), function(model, x) { predict(model,x)*scale + center}, n.split=11,n.group=5, clusterid=lid) library(GGally) df <- data.frame(Lasso=apply(gml.lasso$cate,1,median), Forest=apply(gml.rf$cate,1,median), Neural=apply(gml.nn$cate,1,median)) ggpairs(df, lower=list(continuous=\"smooth\")) + theme_minimal() Best linear projection of CATE \u00b6 Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]} Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\beta_0 (d-P(x)) + \\beta_1 (d-P(x))(S(x) - \\Er[S(x)]) + \\epsilon $\\hat{\\beta} 0, \\hat{\\beta}_1 \\to_p \\argmin \\Er[(s_0(x) - b_0 - b_1 (S(x)-E[S(x)]))^2]$ $\\Lambda = \\beta_1^2 Var(S(x)) = corr(s_0(x),S(X))^2 Var(s_0(x))$ library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$blp[1], s$blp.se[1], s$blp[2], s$blp.se[2], s$Lambda[1]) } tbl <- cbind(colfn(gml.lasso), colfn(gml.rf), colfn(gml.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(\"ATE=b0\",\"se\",\"b1\",\"se\",\"Lambda\") kable_styling(kable(tbl, caption=\"Machine learning proxies as BLP of CATE on Birthweight\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Machine learning proxies as BLP of CATE on Birthweight Lasso Regression forest Neural network ATE=b0 -11.381 -1.751 6.185 se 31.414 32.285 32.309 b1 0.371 0.369 0.044 se 0.547 0.491 0.115 Lambda 601.155 683.726 372.192 Group average treatment effects [group-average-treatment-effects] \u00b6 Define $G_k = 1{\\ell_{k-1} \\leq S(x) \\leq \\ell_k}$ with $\\ell_k = k/5$ quantile of $S(x)$ Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\sum_k \\gamma_k (d-P(X)) 1(G_k) + \\epsilon $\\hat{\\gamma}_k \\to_p \\Er[y(1) - y(0) | G_k]$ $\\bar{\\Lambda} = \\frac{1}{K} \\sum_k \\gamma_k^2$ library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$gate[1], s$gate.se[1], s$gate[2], s$gate.se[2], s$gate[3], s$gate.se[3], s$gate[4], s$gate.se[4], s$gate[5], s$gate.se[5], s$Lambda[2]) } tbl <- cbind(colfn(gml.lasso), colfn(gml.rf), colfn(gml.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(as.vector(sapply(1:5, function(x) c(sprintf(\"GATE %d\",x),\"se\"))),\"Lambda\") kable_styling(kable(tbl, caption=\"GATE on Birthweight\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) GATE on Birthweight Lasso Regression forest Neural network GATE 1 -48.985 -3.950 19.586 se 69.276 69.646 62.126 GATE 2 21.303 -21.106 -30.948 se 74.528 60.761 67.161 GATE 3 -9.369 -44.090 -11.807 se 65.917 66.654 66.981 GATE 4 1.730 10.772 -13.668 se 66.391 67.695 66.693 GATE 5 -22.329 5.736 52.466 se 66.335 77.828 77.496 Lambda 5998.909 4268.074 3912.912 Heterogeneity in CATE on Midwife utilization \u00b6 # create x matrix mwb <- pw[as.numeric(rownames(m$x)),]$midwife_birth mw.lasso <- genericML(Xl,mwb, treat, function(x,y) cv.glmnet(x,y,family=\"binomial\", alpha=1,parallel=FALSE, intercept=TRUE, nfolds=20), function(model, x) { predict(model, x, s=model$lambda.min, type=\"response\") }, n.split=11,n.group=5, clusterid=lid) library(grf) mw.rf <- genericML(Xl,mwb, treat, function(x,y) regression_forest(x, y, tune.parameters=TRUE), function(model, x) { predict(model,x)$predictions }, n.split=11,n.group=5, clusterid=lid) library(RSNNS) mw.nn <- genericML(Xl,mwb, treat, function(x,y) mlp(x, y, linOut=FALSE, size=c(10,10), learnFunc=\"SCG\"), function(model, x) { predict(model,x) }, n.split=11,n.group=5, clusterid=lid) library(GGally) df <- data.frame(Lasso=apply(mw.lasso$cate,1,median), Forest=apply(mw.rf$cate,1,median), Neural=apply(mw.nn$cate,1,median)) ggpairs(df, lower=list(continuous=\"smooth\")) + theme_minimal() library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$blp[1], s$blp.se[1], s$blp[2], s$blp.se[2], s$Lambda[1]) } tbl <- cbind(colfn(mw.lasso), colfn(mw.rf), colfn(mw.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(\"ATE=b0\",\"se\",\"b1\",\"se\",\"Lambda\") kable_styling(kable(tbl, caption=\"Machine learning proxies as BLP of CATE on Midwife Use\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Machine learning proxies as BLP of CATE on Midwife Use Lasso Regression forest Neural network ATE=b0 0.048 0.056 0.043 se 0.023 0.023 0.024 b1 0.466 0.625 0.207 se 0.189 0.240 0.089 Lambda 0.003 0.003 0.003 library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$gate[1], s$gate.se[1], s$gate[2], s$gate.se[2], s$gate[3], s$gate.se[3], s$gate[4], s$gate.se[4], s$gate[5], s$gate.se[5], s$Lambda[2]) } tbl <- cbind(colfn(mw.lasso), colfn(mw.rf), colfn(mw.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(as.vector(sapply(1:5, function(x) c(sprintf(\"GATE %d\",x),\"se\"))),\"Lambda\") kable_styling(kable(tbl, caption=\"GATE on Midwife Use\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) GATE on Midwife Use Lasso Regression forest Neural network GATE 1 -0.002 0.030 -0.027 se 0.045 0.054 0.060 GATE 2 0.017 -0.024 0.035 se 0.050 0.054 0.052 GATE 3 0.010 0.038 0.003 se 0.054 0.050 0.050 GATE 4 0.089 0.080 0.058 se 0.053 0.046 0.050 GATE 5 0.166 0.189 0.136 se 0.053 0.052 0.052 Lambda 0.007 0.011 0.006 Covariate means by group [covariate-means-by-group] \u00b6 df <- pw[as.numeric(rownames(m$x)),] df$edu99 <- df$edu==99 df$educ <- df$edu df$educ[df$educ==99] <- NA vars <- c(\"log_xp_percap\",\"agecat\",\"educ\",\"tv\",\"goat\", \"hh_toilet_own\",\"motorbike\",\"hh_cook_wood\",\"pkh_ever\") tmp <- data.frame() groupMeans <- function(var, gml, clusterid) { n.group <- ncol(gml$gate) gate <- matrix(NA, nrow=nrow(gml$gate), ncol=ncol(gml$gate)) gate.se <- gate dat <- data.frame(y=var) for (i in 1:ncol(gml$cate)) { S <- gml$cate[,i] cut <- quantile(S, probs=seq(0,1,length.out=(n.group+1))) cut[n.group+1] <- cut[n.group+1] + 1 for(k in 1:n.group) { dat[,sprintf(\"G.%d\",k)] <- 1*(cut[k]<=S & S<cut[k+1]) } greg <- lm(as.formula(paste(c(\"y ~ -1\", sprintf(\"G.%d\",1:n.group)), collapse=\" + \")), data=dat) gc <- grep(\"G\", names(greg$coefficients)) gate[i,] <- greg$coefficients[gc] if (is.null(clusterid)) gate.se[i,] <- sqrt(diag(vcovHC(greg))[gc]) else gate.se[i,] <- sqrt(diag(vcovCL(greg,clusterid))[gc]) } return(list(mean=apply(gate, 2, function(x) median(x,na.rm=TRUE)), se = apply(gate.se, 2, function(x) median(x,na.rm=TRUE)))) } methods <- c(\"Lasso\",\"Forest\",\"Neural\") gmls <- list(mw.lasso,mw.rf,mw.nn) for(v in vars) { for (m in 1:length(methods)) { gm <- groupMeans(df[,v], gmls[[m]], lid) tmp <- rbind(tmp, data.frame(group=1:length(gm$mean),variable=v, method=methods[m], mean=gm$mean, se=gm$se)) } } library(ggplot2) fig <- ggplot(data=tmp, aes(x=group, y=mean, colour=method)) + geom_line() + geom_line(aes(y=(mean+1.96*se), colour=method), linetype=2) + geom_line(aes(y=(mean-1.96*se), colour=method), linetype=2) + facet_wrap(~ variable,scales=\"free_y\") + theme_minimal() print(fig) References \u00b6 Alatas, Vivi, Nur Cahyadi, Elisabeth Ekasari, Sarah Harmoun, Budi Hidayat, Edgar Janz, Jon Jellema, H Tuhiman, and M Wai-Poi. 2011. \u201cProgram Keluarga Harapan : Impact Evaluation of Indonesia\u2019s Pilot Household Conditional Cash Transfer Program.\u201d World Bank. http://documents.worldbank.org/curated/en/589171468266179965/Program-Keluarga-Harapan-impact-evaluation-of-Indonesias-Pilot-Household-Conditional-Cash-Transfer-Program . Chernozhukov, Victor, Mert Demirer, Esther Duflo, and Iv\u00e1n Fern\u00e1ndez-Val. 2018. \u201cGeneric Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experimentsxo.\u201d Working Paper 24678. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w24678 . Triyana, Margaret. 2016. \u201cDo Health Care Providers Respond to Demand-Side Incentives? Evidence from Indonesia.\u201d American Economic Journal: Economic Policy 8 (4): 255\u201388. https://doi.org/10.1257/pol.20140048 .","title":"Detecting heterogeneity"},{"location":"mlExamplePKH/#introduction","text":"","title":"Introduction"},{"location":"mlExamplePKH/#background","text":"Program Keluarga Harapan : pilot conditional cash transfer program in Indonesia Alatas et al. ( 2011 ), Triyana ( 2016 ) Conditional cash transfer: receive cash if Expectant women: 4 prenatal visits, iron supplement, delivery by doctor or midwife, 2 postnatal visits Children under 5: weighed monthly, vaccinated, vitamin A Quarterly transfer of 600,000-2,200,000 rupiah (\\$60 - \\$220) depending on household composition (15-20% of quaterly consumption) Randomized at subdistrict level : want to capture supply side effects that would occur if policy implemented everywhere","title":"Background"},{"location":"mlExamplePKH/#baseline-characteristics","text":"if (!file.exists(\"Data_AEJPol-2014-0048/data_final/price_women.dta\")) { stop(\"Please download the data from https://www.aeaweb.org/articles?id=10.1257/pol.20140048 and unzip it in the current directory\") } if (!require(foreign)) install.packages(\"foreign\") library(foreign) all.data <- read.dta(\"Data_AEJPol-2014-0048/data_final/price_women.dta\") # Variables we will use vars <- c(\"rid_panel\",\"prov\",\"Location_ID\",\"dist\", \"wave\", \"edu\",\"agecat\",\"log_xp_percap\",#\"hh_land\",\"hh_home\", \"rhr031\", \"rhr032\", \"rhr034\", \"rhr036\",# \"hh_xp\", \"death_ch\",\"bw\",\"bw_low\", \"birthfac\", \"good_assisted_delivery\", \"dummy_A\", \"dummy_B\", \"dummy_D\", \"delivery_fees\", \"delivery_fees_prof\", \"delivery_fees_doc\", \"delivery_fees_midw\", \"delivery_fees_trad\", \"control_pkh\", \"pkh_kec_ever\", \"pkh_ever\", \"bw_kec\", \"bw_low_kec\", \"death_ch_kec\", \"dummy_A_base\", \"dummy_B_base\", \"dummy_D_base\", \"bid_pkh1\", \"bid_pkh1_base\" , \"delivery_fees_top\", \"delivery_fees_prof_top\", \"delivery_fees_doc_top\", \"delivery_fees_midw_top\", \"delivery_fees_trad_top\", \"delivery_fees_top_base\", \"delivery_fees_prof_top_base\", \"delivery_fees_doc_top_base\", \"delivery_fees_midw_top_base\", \"delivery_fees_trad_top_base\", names(all.data)[grep(\"^hh_\",names(all.data))], \"tv\", \"parabola\" , \"fridge\" , \"motorbike\", \"car\" , \"pig\" , \"goat\" , \"cow\" , \"horse\" ) pw <- all.data[,vars] ## Rename some variables names(pw)[names(pw)==\"dummy_A\"] <- \"doctor_birth\" names(pw)[names(pw)==\"dummy_A_base\"] <- \"doctor_birth_base\" names(pw)[names(pw)==\"dummy_B\"] <- \"midwife_birth\" names(pw)[names(pw)==\"dummy_B_base\"] <- \"midwife_birth_base\" names(pw)[names(pw)==\"dummy_D\"] <- \"traditional_birth\" names(pw)[names(pw)==\"dummy_D_base\"] <- \"traditional_birth_base\" # Set household variables to their pretreatment (wave=1) values for (v in c(\"edu\",\"agecat\",\"log_xp_percap\", \"rhr031\", \"rhr032\", \"rhr034\", \"rhr036\", names(pw)[grep(\"^hh_\",names(pw))], \"tv\",\"parabola\",\"fridge\",\"motorbike\",\"car\",\"pig\",\"goat\",\"cow\",\"horse\")) { temp <- ifelse(pw$wave==1,pw[,v],NA) temp0 <- ave(temp, pw$rid_panel, FUN=function(x) max(x,na.rm=TRUE)) pw[,v] <- ifelse(is.finite(temp0), temp0, pw[,v]) } pw$delivery_fees_trad_top[is.na(pw$delivery_fees_trad_top) & !is.na(pw$delivery_fees_midw_top)] <- 0 adf <- aggregate(subset(pw,wave==1), by=list(subset(pw,wave==1)$control_pkh), FUN=function(x) mean(x,na.rm=TRUE)) adf ## Group.1 rid_panel prov Location_ID dist wave edu agecat ## 1 0 NA NA NA NA 1 15.50180 3.405191 ## 2 1 NA NA NA NA 1 14.59808 3.428927 ## log_xp_percap rhr031 rhr032 rhr034 rhr036 death_ch ## 1 11.98384 0.3907714 0.5313627 0.4354722 0.4826965 0.01333814 ## 2 12.00990 0.4068941 0.5458422 0.4427861 0.4765458 0.01137171 ## bw bw_low birthfac good_assisted_delivery doctor_birth ## 1 3166.346 0.08163265 0.4353201 0.6392936 0.07902870 ## 2 3180.763 0.07678883 0.4402434 0.6232073 0.08778792 ## midwife_birth traditional_birth delivery_fees delivery_fees_prof ## 1 0.5876380 0.4189845 284483.6 243728.8 ## 2 0.5619296 0.4315515 298738.9 256254.9 ## delivery_fees_doc delivery_fees_midw delivery_fees_trad control_pkh ## 1 82076.82 178308.9 39173.57 0 ## 2 102721.64 176275.7 40711.49 1 ## pkh_kec_ever pkh_ever bw_kec bw_low_kec death_ch_kec doctor_birth_base ## 1 0 0 3165.103 0.08319263 0.01333814 0.07897035 ## 2 0 0 3180.449 0.08287012 0.01137171 0.08718138 ## midwife_birth_base traditional_birth_base bid_pkh1 bid_pkh1_base ## 1 0.5848753 0.4195088 3.574443e-02 3.574443e-02 ## 2 0.5585186 0.4353818 -1.469066e-17 -2.999183e-18 ## delivery_fees_top delivery_fees_prof_top delivery_fees_doc_top ## 1 274278.3 233523.5 64174.41 ## 2 278295.7 235811.7 72279.24 ## delivery_fees_midw_top delivery_fees_trad_top delivery_fees_top_base ## 1 167942.7 36862.44 273320.4 ## 2 162837.3 37162.07 275470.0 ## delivery_fees_prof_top_base delivery_fees_doc_top_base ## 1 232340.5 63631.20 ## 2 232639.0 70302.13 ## delivery_fees_midw_top_base delivery_fees_trad_top_base hh_serial_no_w1 ## 1 166914.5 37767.86 2.898702 ## 2 162082.5 38660.42 2.871713 ## hh_survey_date_w1 hh_adult hh_5_15 hh_u5 hh_serial_no_w2 ## 1 17368.93 3.426614 1.858130 1.631684 NaN ## 2 17367.48 3.363539 1.887016 1.638240 NaN ## hh_serial_no_w3 hh_u2 hh_3_6 hh_7_15 hh_phone hh_rf_tile ## 1 NaN 1.231982 1.496021 2.336163 0.1167988 0.7119683 ## 2 NaN 1.232099 1.467532 2.340625 0.1122957 0.7157072 ## hh_rf_shingle hh_rf_fiber hh_rf_oth hh_wall_plaster hh_wall_brick ## 1 0.2025955 0.1304975 0.04578226 0.3183129 0.1503244 ## 2 0.1943852 0.1339730 0.04442075 0.3471926 0.1368159 ## hh_wall_wood hh_wall_fiber hh_wall_oth hh_fl_tile hh_fl_plaster ## 1 0.2599135 0.3604903 0.03208363 0.2058399 0.3713050 ## 2 0.2356077 0.3681592 0.03020611 0.2160625 0.3901919 ## hh_fl_wood hh_fl_dirt hh_fl_oth hh_drink_pam hh_drink_mechwell ## 1 0.1398702 0.3738284 0.000000000 0.1528479 0.2025955 ## 2 0.1346837 0.3471926 0.000355366 0.1464108 0.2174840 ## hh_drink_well hh_drink_spring hh_drink_oth hh_drinkhome hh_drinkdist ## 1 0.4913482 0.1870944 0.08723864 0.2826244 1052.188 ## 2 0.4626866 0.1915423 0.09985785 0.2782516 1064.803 ## hh_waterdrink hh_water_pam hh_water_mechwell hh_water_well ## 1 0.8280461 0.1171593 0.2004326 0.4783706 ## 2 0.8312011 0.1055437 0.2235252 0.4552239 ## hh_water_spring hh_water_river hh_water_oth hh_waterhome hh_waterdist ## 1 0.1856525 0.09841384 0.04109589 0.05839942 610.1139 ## 2 0.1847903 0.09630419 0.04157783 0.05934613 634.7447 ## hh_toilet_own hh_toilet_pub hh_toilet_none hh_toilet_1 hh_toilet_2 ## 1 0.4340303 0.1784427 0.4480894 0.2811824 0.1049027 ## 2 0.4648188 0.1844350 0.4097370 0.3280028 0.1080313 ## hh_toilet_3 hh_waste_tank hh_waste_hole hh_waste_river hh_waste_field ## 1 0.2505407 0.2606345 0.2487383 0.3695025 0.2119683 ## 2 0.2313433 0.3120114 0.2341862 0.3454158 0.1968728 ## hh_waste_oth hh_listrik hh_pln hh_kitchen hh_cook_wood ## 1 0 0.8601298 0.8417448 0.8936554 0.8136265 ## 2 0 0.8681592 0.8464819 0.9033404 0.7782516 ## hh_cook_kerosene hh_cook_gas hh_cook_oth hh_kec hh_land hh_home ## 1 0.2105263 0.03640952 0 9400.184 0.3734679 0.8846431 ## 2 0.2469794 0.03375977 0 9618.802 0.3848614 0.8749112 ## hh_land_miss hh_home_miss hh_xp hh_xp_all tv parabola ## 1 0 0.03028118 3.024874 2.601298 0.5313627 0.03893295 ## 2 0 0.02949538 3.096304 2.676617 0.5458422 0.04015636 ## fridge motorbike car pig goat cow horse ## 1 0.05803893 0.2011536 0.03496756 0.1196828 0.1456381 0.1121125 0.03640952 ## 2 0.05685856 0.1986496 0.03269367 0.1364606 0.1552950 0.1186923 0.03802416","title":"Baseline characteristics"},{"location":"mlExamplePKH/#main-results","text":"","title":"Main results"},{"location":"mlExamplePKH/#delivery-attendant-usage","text":"library(lfe) lhs <- \"pkh_kec_ever + doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | 0 | Location_ID\" itt <- list(felm(as.formula(paste(\"doctor_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"midwife_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"traditional_birth ~\",lhs)),data=pw)) lhs <- \"doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | (pkh_ever ~ pkh_kec_ever) | Location_ID\" iv <- list(felm(as.formula(paste(\"doctor_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"midwife_birth ~\",lhs)),data=pw), felm(as.formula(paste(\"traditional_birth ~\",lhs)),data=pw)) library(stargazer) stargazer(itt, type=\"html\", title=\"ITT estimates\", dep.var.labels = c(\"Doctor\",\"Midwife\",\"Traditional\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) ITT estimates Dependent variable: Doctor Midwife Traditional (1) (2) (3) pkh_kec_ever 0.043 *** 0.094 *** -0.090 *** (0.012) (0.017) (0.016) Observations 6,629 6,629 6,629","title":"Delivery attendant usage"},{"location":"mlExamplePKH/#delivery-attendant-usage-delivery-attendant-usage-1","text":"stargazer(iv, type=\"html\", title=\"IV effect of program participation\", dep.var.labels = c(\"Doctor\",\"Midwife\",\"Traditional\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) IV effect of program participation Dependent variable: Doctor Midwife Traditional (1) (2) (3) pkh_ever(fit) 0.091 *** 0.198 *** -0.189 *** (0.025) (0.036) (0.035) Observations 6,629 6,629 6,629","title":"Delivery attendant usage [delivery-attendant-usage-1]"},{"location":"mlExamplePKH/#health-outcomes","text":"library(lfe) lhs <- \"pkh_kec_ever + doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | 0 | Location_ID\" itt <- list(felm(as.formula(paste(\"death_ch ~\",lhs)),data=pw), felm(as.formula(paste(\"bw ~\",lhs)),data=pw), felm(as.formula(paste(\"bw_low ~\",lhs)),data=pw)) lhs <- \"doctor_birth_base + as.factor(edu) + as.factor(agecat) + log_xp_percap + hh_land + hh_home | dist | (pkh_ever ~ pkh_kec_ever) | Location_ID\" iv <- list(felm(as.formula(paste(\"death_ch ~\",lhs)),data=pw), felm(as.formula(paste(\"bw ~\",lhs)),data=pw), felm(as.formula(paste(\"bw_low ~\",lhs)),data=pw)) library(stargazer) stargazer(itt, type=\"html\", title=\"ITT estimates\", dep.var.labels = c(\"Infant mortality\",\"Birthweight\",\"Low birthweight\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) ITT estimates Dependent variable: Infant mortality Birthweight Low birthweight (1) (2) (3) pkh_kec_ever 0.005 -5.559 0.017 (0.004) (23.805) (0.012) Observations 8,303 4,988 4,988","title":"Health outcomes"},{"location":"mlExamplePKH/#health-outcomes-health-outcomes-1","text":"stargazer(iv, type=\"html\", title=\"IV effect of program participation\", dep.var.labels = c(\"Infant mortality\",\"Birthweight\",\"Lowbirthweigt\"), keep=\"pkh\", omit.table.layout=\"n\",omit.stat = c(\"rsq\",\"adj.rsq\",\"ser\")) IV effect of program participation Dependent variable: Infant mortality Birthweight Lowbirthweigt (1) (2) (3) pkh_ever(fit) 0.011 -12.674 0.039 (0.008) (54.269) (0.026) Observations 8,303 4,988 4,988","title":"Health outcomes [health-outcomes-1]"},{"location":"mlExamplePKH/#exploring-heterogeneity","text":"","title":"Exploring heterogeneity"},{"location":"mlExamplePKH/#machine-learning-as-proxy","text":"Generic machine learning approach of Chernozhukov et al. ( 2018 ) Estimate machine learning proxies for $B(x) = \\Er[y(0)|x]$ and $S(x) = \\Er[y(1) - y(0) |x]$ Use proxies to : Estimate best linear projection on true $\\Er[y(1) - y(0)|x]$ Estimate $\\Er[y(1) - y(0) | groups]$","title":"Machine learning as proxy"},{"location":"mlExamplePKH/#heterogeneity-in-cate-for-birthweight","text":"## Function for Generic machine learning of Chernozhukov, Demirer, Duflo, & Fernandez-Val (2018) genericML <- function(x,y,treat, fit.function, predict.function, n.split=10, n.group=5, clusterid=NULL) { if (!is.null(clusterid)) require(sandwich) blp <- matrix(NA, nrow=n.split, ncol=2) blp.se <- blp gate <- matrix(NA, nrow=n.split, ncol=n.group) gate.se <- gate baseline <- matrix(NA, nrow=nrow(x), ncol=n.split) cate <- matrix(NA, nrow=nrow(x), ncol=n.split) Lambda <- matrix(NA, nrow=n.split, ncol=2) for(i in 1:n.split) { main <- runif(nrow(x))>0.5 fit1 <- fit.function(x[!main & treat==1,], y[!main & treat==1]) fit0 <- fit.function(x[!main & treat==0,], y[!main & treat==0]) B <- as.vector(predict.function(fit0,x)) S <- as.vector(predict.function(fit1,x)) - B baseline[,i] <- B cate[,i] <- S ES <- mean(S) ## BLP # assume P(treat|x) = P(treat) = mean(treat) p <- mean(treat) df <- data.frame(y, B, treat, S, main) reg <- lm(y ~ B + I(treat-p) + I((treat-p)*(S-ES)), data=subset(df, main)) blp[i,] <- reg$coef[3:4] if (is.null(clusterid)) blp.se[i,] <- sqrt(diag(vcovHC(reg))[3:4]) else blp.se[i,] <- sqrt(diag(vcovCL(reg, clusterid[main]))[3:4]) Lambda[i,1] <- reg$coefficient[4]^2*var(S) ## GATES cut <- quantile(S, probs=seq(0,1,length.out=(n.group+1))) cut[n.group+1] <- cut[n.group+1] + 1 for(k in 1:n.group) { df[,sprintf(\"G.%d\",k)] <- (cut[k]<=S & S<cut[k+1]) } greg <- lm(as.formula(paste(c(\"y ~ B \", sprintf(\"I((treat-p)*G.%d)\",1:n.group)), collapse=\" + \")), data=subset(df,main)) gc <- grep(\"G\", names(greg$coefficients)) gate[i,] <- greg$coefficients[gc] if (is.null(clusterid)) gate.se[i,] <- sqrt(diag(vcovHC(greg))[gc]) else gate.se[i,] <- sqrt(diag(vcovCL(greg, clusterid[main]))[gc]) Lambda[i,2] <- sum(gate[i,]^2)/n.group } out <- list( gate=gate, gate.se=gate.se, blp=blp, blp.se=blp.se, Lambda=Lambda, baseline=baseline, cate=cate) } genericML.summary <- function(gml) { blp <- apply(gml$blp, 2, function(x) median(x, na.rm=TRUE)) blp.se <- apply(gml$blp.se, 2, function(x) median(x, na.rm=TRUE)) gate <- apply(gml$gate, 2, function(x) median(x, na.rm=TRUE)) gate.se <- apply(gml$gate.se, 2, function(x) median(x, na.rm=TRUE)) Lambda <- apply(gml$Lambda, 2, function(x) median(x, na.rm=TRUE)) return(list(blp=blp, blp.se=blp.se, gate=gate, gate.se=gate.se, Lambda=Lambda)) } library(glmnet) # create x matrix fmla.l <- bw ~ pkh_kec_ever + as.factor(edu)*as.factor(agecat) + log_xp_percap + hh_land + hh_home + as.factor(dist) + hh_phone + hh_rf_tile + hh_rf_shingle + hh_rf_fiber + hh_wall_plaster + hh_wall_brick + hh_wall_wood + hh_wall_fiber + hh_fl_tile + hh_fl_plaster + hh_fl_wood + hh_fl_dirt + hh_water_pam + hh_water_mechwell + hh_water_well + hh_water_spring + hh_water_river + hh_waterhome + hh_toilet_own + hh_toilet_pub + hh_toilet_none + hh_waste_tank + hh_waste_hole + hh_waste_river + hh_waste_field + hh_kitchen + hh_cook_wood + hh_cook_kerosene + hh_cook_gas + tv + fridge + motorbike + car + goat + cow + horse m <- lm(fmla.l, data=pw, x=TRUE, y=TRUE) treat <- m$x[,2] Xl <- m$x[,3:ncol(m$x)] scale <- sd(m$y) center <- mean(m$y) yl <- (m$y-center)/scale lid <- as.factor(pw[as.numeric(rownames(m$x)),]$Location_ID) gml.lasso <- genericML(Xl,m$y, treat, function(x,y) cv.glmnet(x,(y-center)/scale,alpha=1,parallel=FALSE, intercept=TRUE, nfolds=20), function(model, x) { predict(model, x, s=model$lambda.min, type=\"response\")*scale + center }, n.split=11,n.group=5, clusterid=lid) library(grf) gml.rf <- genericML(Xl,m$y, treat, function(x,y) regression_forest(x, (y-center)/scale, tune.parameters=TRUE), function(model, x) { predict(model,x)$predictions*scale + center}, n.split=11,n.group=5, clusterid=lid) library(RSNNS) gml.nn <- genericML(Xl,m$y, treat, function(x,y) mlp(x,(y-center)/scale,linOut=TRUE, size=c(10,10), learnFunc=\"SCG\"), function(model, x) { predict(model,x)*scale + center}, n.split=11,n.group=5, clusterid=lid) library(GGally) df <- data.frame(Lasso=apply(gml.lasso$cate,1,median), Forest=apply(gml.rf$cate,1,median), Neural=apply(gml.nn$cate,1,median)) ggpairs(df, lower=list(continuous=\"smooth\")) + theme_minimal()","title":"Heterogeneity in CATE for Birthweight"},{"location":"mlExamplePKH/#best-linear-projection-of-cate","text":"Randomly partition sample into auxillary and main samples Use any method on auxillary sample to estimate S(x) = \\widehat{\\Er[y(1) - y(0) | x]} and B(x) = \\widehat{\\Er[y(0)|x]} Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\beta_0 (d-P(x)) + \\beta_1 (d-P(x))(S(x) - \\Er[S(x)]) + \\epsilon $\\hat{\\beta} 0, \\hat{\\beta}_1 \\to_p \\argmin \\Er[(s_0(x) - b_0 - b_1 (S(x)-E[S(x)]))^2]$ $\\Lambda = \\beta_1^2 Var(S(x)) = corr(s_0(x),S(X))^2 Var(s_0(x))$ library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$blp[1], s$blp.se[1], s$blp[2], s$blp.se[2], s$Lambda[1]) } tbl <- cbind(colfn(gml.lasso), colfn(gml.rf), colfn(gml.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(\"ATE=b0\",\"se\",\"b1\",\"se\",\"Lambda\") kable_styling(kable(tbl, caption=\"Machine learning proxies as BLP of CATE on Birthweight\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Machine learning proxies as BLP of CATE on Birthweight Lasso Regression forest Neural network ATE=b0 -11.381 -1.751 6.185 se 31.414 32.285 32.309 b1 0.371 0.369 0.044 se 0.547 0.491 0.115 Lambda 601.155 683.726 372.192","title":"Best linear projection of CATE"},{"location":"mlExamplePKH/#group-average-treatment-effects-group-average-treatment-effects","text":"Define $G_k = 1{\\ell_{k-1} \\leq S(x) \\leq \\ell_k}$ with $\\ell_k = k/5$ quantile of $S(x)$ Use main sample to regress with weights $(P(x)(1-P(X)))^{-1}$ y = \\alpha_0 + \\alpha_1 B(x) + \\sum_k \\gamma_k (d-P(X)) 1(G_k) + \\epsilon $\\hat{\\gamma}_k \\to_p \\Er[y(1) - y(0) | G_k]$ $\\bar{\\Lambda} = \\frac{1}{K} \\sum_k \\gamma_k^2$ library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$gate[1], s$gate.se[1], s$gate[2], s$gate.se[2], s$gate[3], s$gate.se[3], s$gate[4], s$gate.se[4], s$gate[5], s$gate.se[5], s$Lambda[2]) } tbl <- cbind(colfn(gml.lasso), colfn(gml.rf), colfn(gml.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(as.vector(sapply(1:5, function(x) c(sprintf(\"GATE %d\",x),\"se\"))),\"Lambda\") kable_styling(kable(tbl, caption=\"GATE on Birthweight\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) GATE on Birthweight Lasso Regression forest Neural network GATE 1 -48.985 -3.950 19.586 se 69.276 69.646 62.126 GATE 2 21.303 -21.106 -30.948 se 74.528 60.761 67.161 GATE 3 -9.369 -44.090 -11.807 se 65.917 66.654 66.981 GATE 4 1.730 10.772 -13.668 se 66.391 67.695 66.693 GATE 5 -22.329 5.736 52.466 se 66.335 77.828 77.496 Lambda 5998.909 4268.074 3912.912","title":"Group average treatment effects [group-average-treatment-effects]"},{"location":"mlExamplePKH/#heterogeneity-in-cate-on-midwife-utilization","text":"# create x matrix mwb <- pw[as.numeric(rownames(m$x)),]$midwife_birth mw.lasso <- genericML(Xl,mwb, treat, function(x,y) cv.glmnet(x,y,family=\"binomial\", alpha=1,parallel=FALSE, intercept=TRUE, nfolds=20), function(model, x) { predict(model, x, s=model$lambda.min, type=\"response\") }, n.split=11,n.group=5, clusterid=lid) library(grf) mw.rf <- genericML(Xl,mwb, treat, function(x,y) regression_forest(x, y, tune.parameters=TRUE), function(model, x) { predict(model,x)$predictions }, n.split=11,n.group=5, clusterid=lid) library(RSNNS) mw.nn <- genericML(Xl,mwb, treat, function(x,y) mlp(x, y, linOut=FALSE, size=c(10,10), learnFunc=\"SCG\"), function(model, x) { predict(model,x) }, n.split=11,n.group=5, clusterid=lid) library(GGally) df <- data.frame(Lasso=apply(mw.lasso$cate,1,median), Forest=apply(mw.rf$cate,1,median), Neural=apply(mw.nn$cate,1,median)) ggpairs(df, lower=list(continuous=\"smooth\")) + theme_minimal() library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$blp[1], s$blp.se[1], s$blp[2], s$blp.se[2], s$Lambda[1]) } tbl <- cbind(colfn(mw.lasso), colfn(mw.rf), colfn(mw.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(\"ATE=b0\",\"se\",\"b1\",\"se\",\"Lambda\") kable_styling(kable(tbl, caption=\"Machine learning proxies as BLP of CATE on Midwife Use\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) Machine learning proxies as BLP of CATE on Midwife Use Lasso Regression forest Neural network ATE=b0 0.048 0.056 0.043 se 0.023 0.023 0.024 b1 0.466 0.625 0.207 se 0.189 0.240 0.089 Lambda 0.003 0.003 0.003 library(kableExtra) colfn <- function(gml) { s <- genericML.summary(gml) c(s$gate[1], s$gate.se[1], s$gate[2], s$gate.se[2], s$gate[3], s$gate.se[3], s$gate[4], s$gate.se[4], s$gate[5], s$gate.se[5], s$Lambda[2]) } tbl <- cbind(colfn(mw.lasso), colfn(mw.rf), colfn(mw.nn)) colnames(tbl) <- c(\"Lasso\",\"Regression forest\",\"Neural network\") rownames(tbl) <- c(as.vector(sapply(1:5, function(x) c(sprintf(\"GATE %d\",x),\"se\"))),\"Lambda\") kable_styling(kable(tbl, caption=\"GATE on Midwife Use\", format=\"html\", digits=3), bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width=TRUE) GATE on Midwife Use Lasso Regression forest Neural network GATE 1 -0.002 0.030 -0.027 se 0.045 0.054 0.060 GATE 2 0.017 -0.024 0.035 se 0.050 0.054 0.052 GATE 3 0.010 0.038 0.003 se 0.054 0.050 0.050 GATE 4 0.089 0.080 0.058 se 0.053 0.046 0.050 GATE 5 0.166 0.189 0.136 se 0.053 0.052 0.052 Lambda 0.007 0.011 0.006","title":"Heterogeneity in CATE on Midwife utilization"},{"location":"mlExamplePKH/#covariate-means-by-group-covariate-means-by-group","text":"df <- pw[as.numeric(rownames(m$x)),] df$edu99 <- df$edu==99 df$educ <- df$edu df$educ[df$educ==99] <- NA vars <- c(\"log_xp_percap\",\"agecat\",\"educ\",\"tv\",\"goat\", \"hh_toilet_own\",\"motorbike\",\"hh_cook_wood\",\"pkh_ever\") tmp <- data.frame() groupMeans <- function(var, gml, clusterid) { n.group <- ncol(gml$gate) gate <- matrix(NA, nrow=nrow(gml$gate), ncol=ncol(gml$gate)) gate.se <- gate dat <- data.frame(y=var) for (i in 1:ncol(gml$cate)) { S <- gml$cate[,i] cut <- quantile(S, probs=seq(0,1,length.out=(n.group+1))) cut[n.group+1] <- cut[n.group+1] + 1 for(k in 1:n.group) { dat[,sprintf(\"G.%d\",k)] <- 1*(cut[k]<=S & S<cut[k+1]) } greg <- lm(as.formula(paste(c(\"y ~ -1\", sprintf(\"G.%d\",1:n.group)), collapse=\" + \")), data=dat) gc <- grep(\"G\", names(greg$coefficients)) gate[i,] <- greg$coefficients[gc] if (is.null(clusterid)) gate.se[i,] <- sqrt(diag(vcovHC(greg))[gc]) else gate.se[i,] <- sqrt(diag(vcovCL(greg,clusterid))[gc]) } return(list(mean=apply(gate, 2, function(x) median(x,na.rm=TRUE)), se = apply(gate.se, 2, function(x) median(x,na.rm=TRUE)))) } methods <- c(\"Lasso\",\"Forest\",\"Neural\") gmls <- list(mw.lasso,mw.rf,mw.nn) for(v in vars) { for (m in 1:length(methods)) { gm <- groupMeans(df[,v], gmls[[m]], lid) tmp <- rbind(tmp, data.frame(group=1:length(gm$mean),variable=v, method=methods[m], mean=gm$mean, se=gm$se)) } } library(ggplot2) fig <- ggplot(data=tmp, aes(x=group, y=mean, colour=method)) + geom_line() + geom_line(aes(y=(mean+1.96*se), colour=method), linetype=2) + geom_line(aes(y=(mean-1.96*se), colour=method), linetype=2) + facet_wrap(~ variable,scales=\"free_y\") + theme_minimal() print(fig)","title":"Covariate means by group [covariate-means-by-group]"},{"location":"mlExamplePKH/#references","text":"Alatas, Vivi, Nur Cahyadi, Elisabeth Ekasari, Sarah Harmoun, Budi Hidayat, Edgar Janz, Jon Jellema, H Tuhiman, and M Wai-Poi. 2011. \u201cProgram Keluarga Harapan : Impact Evaluation of Indonesia\u2019s Pilot Household Conditional Cash Transfer Program.\u201d World Bank. http://documents.worldbank.org/curated/en/589171468266179965/Program-Keluarga-Harapan-impact-evaluation-of-Indonesias-Pilot-Household-Conditional-Cash-Transfer-Program . Chernozhukov, Victor, Mert Demirer, Esther Duflo, and Iv\u00e1n Fern\u00e1ndez-Val. 2018. \u201cGeneric Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experimentsxo.\u201d Working Paper 24678. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w24678 . Triyana, Margaret. 2016. \u201cDo Health Care Providers Respond to Demand-Side Incentives? Evidence from Indonesia.\u201d American Economic Journal: Economic Policy 8 (4): 255\u201388. https://doi.org/10.1257/pol.20140048 .","title":"References"},{"location":"slp/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 Neural networks, especially deep neural networks, have come to dominate some areas of machine learning. Neural networks are especially prominent in natural language processing, image classification, and reinforcement learning. This documents gives a brief introduction to neural networks. Examples in this document will use Flux.jl . An alternative Julia package for deep learning is Knet.jl . There is a good discussion comparing Flux and Knet on discourse. . We will not have Knet examples here, but the documentation for Knet is excellent and worth reading even if you plan to use Flux. Additional Reading \u00b6 Goodfellow, Bengio, and Courville ( 2016 ) Deep Learning Knet.jl documentation especially the textbook Klok and Nazarathy ( 2019 ) Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence Single Layer Neural Networks \u00b6 We will describe neural networks from a perspective of nonparametric estimation. Suppose we have a target function, $f: \\R^p \\to \\R$. In many applications the target function will be a conditional expectation, $f(x) = \\Er[y|x]$. A single layer neural network approximates $f$ as follows \\hat{f}(x) = \\sum_{j=1}^r \\beta_j \\psi(w_j'x + b_j) Here $r$ is the width of the layer. $\\beta_j$ are scalars. $\\psi:\\R \\to \\R$ is a nonlinear activation function. Common activation functions include: Sigmoid $\\psi(t) = 1/(1+e^{-t})$ Tanh $\\psi(t) = \\frac{e^t -e^{-t}}{e^t + e^{-t}}$ Rectified linear $\\psi(t) = t 1(t\\geq 0)$ The $w_j \\in \\R^p$ are called weights and $b_j \\in \\R$ are biases. You may have heard about the universal approximation theorem. This refers to the fact that as $r$ increases, a neural network can approximate any function. Mathematically, for some large class of functions $\\mathcal{F}$, \\sup_{f \\in \\mathcal{F}} \\lim_{r \\to \\infty} \\inf_{\\beta, w, b} \\Vert f(x) - \\sum_{j=1}^r \\beta_j \\psi(w_j'x+b_j) \\Vert = 0 Hornik, Stinchcombe, and White ( 1989 ) contains one of the earliest results along these lines. Some introductory texts mention the universal approximation theorem as though it is something special for neural networks. This is incorrect. In particular, the universal approximation theorem does not explain why neural networks seem to be unusually good at prediction. Most nonparametric estimation methods (kernel, series, forests, etc) satisfy a similar conditions. Training \u00b6 Models in Flux.jl all involve a differentiable loss function. The loss function is minimized by a variant of gradient descent. Gradients are usually calculated using reverse automatic differentiation (backpropagation is a variant of reverse automatic differentiation specialized for the structue of neural networks). A low level way to use Flux.jl is to write your loss function as a typical Julia function, as in the following code block. using Plots, Flux, Statistics # some function to estimate f(x) = sin(x^x)/2^((x^x-\u03c0/2)/\u03c0) function simulate(n,\u03c3=1) x = rand(n,1).*\u03c0 y = f.(x) .+ randn(n).*\u03c3 (x,y) end \"\"\" slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1 ) Construct a single layer perception with width `r`. \"\"\" function slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1) # Parameters to be minimized wrt have to be declared for tracking # for reverse mode autodiff. w = param(randn(dimx,r)) b = param(randn(1,r)) \u03b2 = param(randn(r)) \u03b8 = Tracker.Params([\u03b2, w, b]) pred(x) = activation(x*w.+b)*\u03b2 loss(x,y) = mean((y.-pred(x)).^2) return(\u03b8=\u03b8, predict=pred,loss=loss) end x, y = simulate(1000, 0.5) xg = 0:0.01:\u03c0 rs = [2, 3, 5, 7, 9] 5-element Array{Int64,1}: 2 3 5 7 9 figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) m = slp(rs[r]) figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\") figs[r]=scatter!(x,y, alpha=0.3, lab=\"\") maxiter = 5000 for i = 1:maxiter Flux.train!(m.loss, m.\u03b8, [(x, y)], Flux.AMSGrad()) if (i % (maxiter \u00f7 5))==0 l=Tracker.data(m.loss(x,y)) println(\"$i iteration, loss=$l\") figs[r]=plot!(xg,Tracker.data(m.predict(xg))) #, lab=\"$i iterations, loss=$l\") end end end 1000 iteration, loss=0.3290077210105611 2000 iteration, loss=0.2807986603619935 3000 iteration, loss=0.265157539249332 4000 iteration, loss=0.25863031117932045 5000 iteration, loss=0.25583798367648847 1000 iteration, loss=0.3172316306510282 2000 iteration, loss=0.2785425790553236 3000 iteration, loss=0.2629610884244914 4000 iteration, loss=0.2546851846024121 5000 iteration, loss=0.2531279475540013 1000 iteration, loss=0.3091030072869277 2000 iteration, loss=0.2927215199130735 3000 iteration, loss=0.28572878081186653 4000 iteration, loss=0.2806191460547027 5000 iteration, loss=0.2757036481913938 1000 iteration, loss=0.30581903986975667 2000 iteration, loss=0.2857782150582308 3000 iteration, loss=0.27267647080160623 4000 iteration, loss=0.2654190425703386 5000 iteration, loss=0.2614191053579672 1000 iteration, loss=0.3048528643501064 2000 iteration, loss=0.294790343516291 3000 iteration, loss=0.2828043945799718 4000 iteration, loss=0.2729555670333174 5000 iteration, loss=0.2668846204027194 plot(figs...) Notice how even though a wider network can approximate $f$ better, wider networks also take more training iterations to minimize the loss. This is typical of any minimization algorithm \u2014 the number of iterations increases with the problem size. Each invocation of Flux.train! completes one iteration of gradient descent. As you might guess from this API, it is common to train neural networks for a fixed number of iterations instead of until convergence to a local minimum. The number of training iterations can act as a regularization parameter. Flux.jl also contains some higher level functions for creating loss functions for neural networks. Here is the same network as in the previous code block, but using the higher level API. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) m = Chain(Dense(dimx, rs[r], Flux.\u03c3), Dense(rs[r], 1)) figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\") figs[r]=scatter!(x,y, alpha=0.3, lab=\"\") maxiter = 5000 for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x'),y'), Flux.params(m), [(x, y)], Flux.AMSGrad() ) #, #cb = Flux.throttle(()->@show(Flux.mse(m(x'),y')),100)) if (i % (maxiter \u00f7 5))==0 l=Tracker.data(Flux.mse(m(x'), y')) println(\"$i iteration, loss=$l\") figs[r]=plot!(xg,Tracker.data(m(xg'))')#, lab=\"$i iterations, loss=$l\") end end end 1000 iteration, loss=0.31834717414986796 2000 iteration, loss=0.27547015218115556 3000 iteration, loss=0.2650481908676505 4000 iteration, loss=0.2615274233520728 5000 iteration, loss=0.2595011818422149 1000 iteration, loss=0.33287597953115394 2000 iteration, loss=0.32136193007368763 3000 iteration, loss=0.27953449577288897 4000 iteration, loss=0.26542259488463016 5000 iteration, loss=0.2601493104968424 1000 iteration, loss=0.3099920519983593 2000 iteration, loss=0.2917687749042323 3000 iteration, loss=0.28246049693406555 4000 iteration, loss=0.27539924265585747 5000 iteration, loss=0.2688202719660268 1000 iteration, loss=0.30330455005352513 2000 iteration, loss=0.2878439282706598 3000 iteration, loss=0.2829433894637187 4000 iteration, loss=0.2799039605614631 5000 iteration, loss=0.27714391629018226 1000 iteration, loss=0.3338539081337899 2000 iteration, loss=0.3287796516437119 3000 iteration, loss=0.3057442513246996 4000 iteration, loss=0.28617590173325286 5000 iteration, loss=0.2708732236144369 plot(figs..., title=\"Sigmoid\") The figures may not appear identical to the first example since the initial values differ. Large applications of neural networks often use rectified linear activation for efficiency. Let\u2019s see how the same example behaves with (leaky) rectified linear activation. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\") figs[r]=scatter!(x,y, alpha=0.3, lab=\"\") maxiter = 5000 for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x'),y'), Flux.params(m), [(x, y)], Flux.AMSGrad()) #, #cb = Flux.throttle(()->@show(Flux.mse(m(x'),y')),100)) if (i % (maxiter \u00f7 5))==0 l=Tracker.data(Flux.mse(m(x'), y')) println(\"$i iteration, loss=$l\") figs[r]=plot!(xg,Tracker.data(m(xg'))')#, lab=\"$i iterations, loss=$l\") end end end 1000 iteration, loss=0.3121117793071286 2000 iteration, loss=0.31196137075517 3000 iteration, loss=0.3119586386582882 4000 iteration, loss=0.3119593901481877 5000 iteration, loss=0.31195971855933596 1000 iteration, loss=0.3119716151434345 2000 iteration, loss=0.31193981766714274 3000 iteration, loss=0.3119192446707223 4000 iteration, loss=0.31187488029142973 5000 iteration, loss=0.31176990427852125 1000 iteration, loss=0.30373780015242985 2000 iteration, loss=0.30177585469527424 3000 iteration, loss=0.30030991229554294 4000 iteration, loss=0.29915315848968566 5000 iteration, loss=0.29830289017342837 1000 iteration, loss=0.3159883943516261 2000 iteration, loss=0.31452976729072385 3000 iteration, loss=0.3137682264114405 4000 iteration, loss=0.3134272981975322 5000 iteration, loss=0.31322881236459327 1000 iteration, loss=0.31271998164415193 2000 iteration, loss=0.3124917243064098 3000 iteration, loss=0.31235600521122137 4000 iteration, loss=0.3123294828344058 5000 iteration, loss=0.312325008489763 plot(figs..., title=\"ReLu\") Rate of convergence \u00b6 Chen and White ( 1999 ) $f(x) = \\Er[y|x]$ with Fourier representation f(x) = \\int e^{i a'x} d\\sigma_f(a) where $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$ Network sieve : \\begin{align*} \\mathcal{G}_n = \\{ & g: g(x) = \\sum_{j=1}^{r_n} \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j), \\\\ & \\norm{\\beta}_1 \\leq B_n \\} \\end{align*} The setup in Chen and White ( 1999 ) is more general. They consider estimating both $f$ and its first $m$ derivatives. Here, we focus on the case of just estimating $f$. Chen and White ( 1999 ) also consider estimation of functions other than conditional expectations. The restriction on $f$ in the second bullet is used to control approximation error. The second bullet says that $f$ is the inverse Fourier transform of measure $\\sigma_f$. The bite of the restriction on $f$ comes from the requirement that $\\sigma_f$ be absolutely integral, $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$. It would be a good exercise to check whether this restriction is satisfied by some familiar types of functions. Barron ( 1993 ) first showed that neural networks approximate this class of functions well, and compares the approximation rate of neural networks to other function approximation results. References [references] \u00b6 Barron, A. R. 1993. \u201cUniversal Approximation Bounds for Superpositions of a Sigmoidal Function.\u201d IEEE Transactions on Information Theory 39 (3): 930\u201345. https://doi.org/10.1109/18.256500 . Chen, Xiaohong, and H. White. 1999. \u201cImproved Rates and Asymptotic Normality for Nonparametric Neural Network Estimators.\u201d IEEE Transactions on Information Theory 45 (2): 682\u201391. https://doi.org/10.1109/18.749011 . Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning . MIT Press. http://www.deeplearningbook.org . Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. \u201cMultilayer Feedforward Networks Are Universal Approximators.\u201d Neural Networks 2 (5): 359\u201366. https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8 . Klok, Hayden, and Yoni Nazarathy. 2019. Statistics with Julia:Fundamentals for Data Science, Machinelearning and Artificial Intelligence . DRAFT. https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf .","title":"Introduction"},{"location":"slp/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and associated jupyter notebook . \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min}","title":"About this document"},{"location":"slp/#introduction","text":"Neural networks, especially deep neural networks, have come to dominate some areas of machine learning. Neural networks are especially prominent in natural language processing, image classification, and reinforcement learning. This documents gives a brief introduction to neural networks. Examples in this document will use Flux.jl . An alternative Julia package for deep learning is Knet.jl . There is a good discussion comparing Flux and Knet on discourse. . We will not have Knet examples here, but the documentation for Knet is excellent and worth reading even if you plan to use Flux.","title":"Introduction"},{"location":"slp/#additional-reading","text":"Goodfellow, Bengio, and Courville ( 2016 ) Deep Learning Knet.jl documentation especially the textbook Klok and Nazarathy ( 2019 ) Statistics with Julia:Fundamentals for Data Science, MachineLearning and Artificial Intelligence","title":"Additional Reading"},{"location":"slp/#single-layer-neural-networks","text":"We will describe neural networks from a perspective of nonparametric estimation. Suppose we have a target function, $f: \\R^p \\to \\R$. In many applications the target function will be a conditional expectation, $f(x) = \\Er[y|x]$. A single layer neural network approximates $f$ as follows \\hat{f}(x) = \\sum_{j=1}^r \\beta_j \\psi(w_j'x + b_j) Here $r$ is the width of the layer. $\\beta_j$ are scalars. $\\psi:\\R \\to \\R$ is a nonlinear activation function. Common activation functions include: Sigmoid $\\psi(t) = 1/(1+e^{-t})$ Tanh $\\psi(t) = \\frac{e^t -e^{-t}}{e^t + e^{-t}}$ Rectified linear $\\psi(t) = t 1(t\\geq 0)$ The $w_j \\in \\R^p$ are called weights and $b_j \\in \\R$ are biases. You may have heard about the universal approximation theorem. This refers to the fact that as $r$ increases, a neural network can approximate any function. Mathematically, for some large class of functions $\\mathcal{F}$, \\sup_{f \\in \\mathcal{F}} \\lim_{r \\to \\infty} \\inf_{\\beta, w, b} \\Vert f(x) - \\sum_{j=1}^r \\beta_j \\psi(w_j'x+b_j) \\Vert = 0 Hornik, Stinchcombe, and White ( 1989 ) contains one of the earliest results along these lines. Some introductory texts mention the universal approximation theorem as though it is something special for neural networks. This is incorrect. In particular, the universal approximation theorem does not explain why neural networks seem to be unusually good at prediction. Most nonparametric estimation methods (kernel, series, forests, etc) satisfy a similar conditions.","title":"Single Layer Neural Networks"},{"location":"slp/#training","text":"Models in Flux.jl all involve a differentiable loss function. The loss function is minimized by a variant of gradient descent. Gradients are usually calculated using reverse automatic differentiation (backpropagation is a variant of reverse automatic differentiation specialized for the structue of neural networks). A low level way to use Flux.jl is to write your loss function as a typical Julia function, as in the following code block. using Plots, Flux, Statistics # some function to estimate f(x) = sin(x^x)/2^((x^x-\u03c0/2)/\u03c0) function simulate(n,\u03c3=1) x = rand(n,1).*\u03c0 y = f.(x) .+ randn(n).*\u03c3 (x,y) end \"\"\" slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1 ) Construct a single layer perception with width `r`. \"\"\" function slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1) # Parameters to be minimized wrt have to be declared for tracking # for reverse mode autodiff. w = param(randn(dimx,r)) b = param(randn(1,r)) \u03b2 = param(randn(r)) \u03b8 = Tracker.Params([\u03b2, w, b]) pred(x) = activation(x*w.+b)*\u03b2 loss(x,y) = mean((y.-pred(x)).^2) return(\u03b8=\u03b8, predict=pred,loss=loss) end x, y = simulate(1000, 0.5) xg = 0:0.01:\u03c0 rs = [2, 3, 5, 7, 9] 5-element Array{Int64,1}: 2 3 5 7 9 figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) m = slp(rs[r]) figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\") figs[r]=scatter!(x,y, alpha=0.3, lab=\"\") maxiter = 5000 for i = 1:maxiter Flux.train!(m.loss, m.\u03b8, [(x, y)], Flux.AMSGrad()) if (i % (maxiter \u00f7 5))==0 l=Tracker.data(m.loss(x,y)) println(\"$i iteration, loss=$l\") figs[r]=plot!(xg,Tracker.data(m.predict(xg))) #, lab=\"$i iterations, loss=$l\") end end end 1000 iteration, loss=0.3290077210105611 2000 iteration, loss=0.2807986603619935 3000 iteration, loss=0.265157539249332 4000 iteration, loss=0.25863031117932045 5000 iteration, loss=0.25583798367648847 1000 iteration, loss=0.3172316306510282 2000 iteration, loss=0.2785425790553236 3000 iteration, loss=0.2629610884244914 4000 iteration, loss=0.2546851846024121 5000 iteration, loss=0.2531279475540013 1000 iteration, loss=0.3091030072869277 2000 iteration, loss=0.2927215199130735 3000 iteration, loss=0.28572878081186653 4000 iteration, loss=0.2806191460547027 5000 iteration, loss=0.2757036481913938 1000 iteration, loss=0.30581903986975667 2000 iteration, loss=0.2857782150582308 3000 iteration, loss=0.27267647080160623 4000 iteration, loss=0.2654190425703386 5000 iteration, loss=0.2614191053579672 1000 iteration, loss=0.3048528643501064 2000 iteration, loss=0.294790343516291 3000 iteration, loss=0.2828043945799718 4000 iteration, loss=0.2729555670333174 5000 iteration, loss=0.2668846204027194 plot(figs...) Notice how even though a wider network can approximate $f$ better, wider networks also take more training iterations to minimize the loss. This is typical of any minimization algorithm \u2014 the number of iterations increases with the problem size. Each invocation of Flux.train! completes one iteration of gradient descent. As you might guess from this API, it is common to train neural networks for a fixed number of iterations instead of until convergence to a local minimum. The number of training iterations can act as a regularization parameter. Flux.jl also contains some higher level functions for creating loss functions for neural networks. Here is the same network as in the previous code block, but using the higher level API. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) m = Chain(Dense(dimx, rs[r], Flux.\u03c3), Dense(rs[r], 1)) figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\") figs[r]=scatter!(x,y, alpha=0.3, lab=\"\") maxiter = 5000 for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x'),y'), Flux.params(m), [(x, y)], Flux.AMSGrad() ) #, #cb = Flux.throttle(()->@show(Flux.mse(m(x'),y')),100)) if (i % (maxiter \u00f7 5))==0 l=Tracker.data(Flux.mse(m(x'), y')) println(\"$i iteration, loss=$l\") figs[r]=plot!(xg,Tracker.data(m(xg'))')#, lab=\"$i iterations, loss=$l\") end end end 1000 iteration, loss=0.31834717414986796 2000 iteration, loss=0.27547015218115556 3000 iteration, loss=0.2650481908676505 4000 iteration, loss=0.2615274233520728 5000 iteration, loss=0.2595011818422149 1000 iteration, loss=0.33287597953115394 2000 iteration, loss=0.32136193007368763 3000 iteration, loss=0.27953449577288897 4000 iteration, loss=0.26542259488463016 5000 iteration, loss=0.2601493104968424 1000 iteration, loss=0.3099920519983593 2000 iteration, loss=0.2917687749042323 3000 iteration, loss=0.28246049693406555 4000 iteration, loss=0.27539924265585747 5000 iteration, loss=0.2688202719660268 1000 iteration, loss=0.30330455005352513 2000 iteration, loss=0.2878439282706598 3000 iteration, loss=0.2829433894637187 4000 iteration, loss=0.2799039605614631 5000 iteration, loss=0.27714391629018226 1000 iteration, loss=0.3338539081337899 2000 iteration, loss=0.3287796516437119 3000 iteration, loss=0.3057442513246996 4000 iteration, loss=0.28617590173325286 5000 iteration, loss=0.2708732236144369 plot(figs..., title=\"Sigmoid\") The figures may not appear identical to the first example since the initial values differ. Large applications of neural networks often use rectified linear activation for efficiency. Let\u2019s see how the same example behaves with (leaky) rectified linear activation. dimx = 1 figs = Array{typeof(plot(0)),1}(undef,length(rs)) for r in eachindex(rs) m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\") figs[r]=scatter!(x,y, alpha=0.3, lab=\"\") maxiter = 5000 for i = 1:maxiter Flux.train!((x,y)->Flux.mse(m(x'),y'), Flux.params(m), [(x, y)], Flux.AMSGrad()) #, #cb = Flux.throttle(()->@show(Flux.mse(m(x'),y')),100)) if (i % (maxiter \u00f7 5))==0 l=Tracker.data(Flux.mse(m(x'), y')) println(\"$i iteration, loss=$l\") figs[r]=plot!(xg,Tracker.data(m(xg'))')#, lab=\"$i iterations, loss=$l\") end end end 1000 iteration, loss=0.3121117793071286 2000 iteration, loss=0.31196137075517 3000 iteration, loss=0.3119586386582882 4000 iteration, loss=0.3119593901481877 5000 iteration, loss=0.31195971855933596 1000 iteration, loss=0.3119716151434345 2000 iteration, loss=0.31193981766714274 3000 iteration, loss=0.3119192446707223 4000 iteration, loss=0.31187488029142973 5000 iteration, loss=0.31176990427852125 1000 iteration, loss=0.30373780015242985 2000 iteration, loss=0.30177585469527424 3000 iteration, loss=0.30030991229554294 4000 iteration, loss=0.29915315848968566 5000 iteration, loss=0.29830289017342837 1000 iteration, loss=0.3159883943516261 2000 iteration, loss=0.31452976729072385 3000 iteration, loss=0.3137682264114405 4000 iteration, loss=0.3134272981975322 5000 iteration, loss=0.31322881236459327 1000 iteration, loss=0.31271998164415193 2000 iteration, loss=0.3124917243064098 3000 iteration, loss=0.31235600521122137 4000 iteration, loss=0.3123294828344058 5000 iteration, loss=0.312325008489763 plot(figs..., title=\"ReLu\")","title":"Training"},{"location":"slp/#rate-of-convergence","text":"Chen and White ( 1999 ) $f(x) = \\Er[y|x]$ with Fourier representation f(x) = \\int e^{i a'x} d\\sigma_f(a) where $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$ Network sieve : \\begin{align*} \\mathcal{G}_n = \\{ & g: g(x) = \\sum_{j=1}^{r_n} \\beta_j (a_j'a_j \\vee 1)^{-1} \\psi(a_j'x + b_j), \\\\ & \\norm{\\beta}_1 \\leq B_n \\} \\end{align*} The setup in Chen and White ( 1999 ) is more general. They consider estimating both $f$ and its first $m$ derivatives. Here, we focus on the case of just estimating $f$. Chen and White ( 1999 ) also consider estimation of functions other than conditional expectations. The restriction on $f$ in the second bullet is used to control approximation error. The second bullet says that $f$ is the inverse Fourier transform of measure $\\sigma_f$. The bite of the restriction on $f$ comes from the requirement that $\\sigma_f$ be absolutely integral, $\\int (\\sqrt{a\u2019a} \\vee 1) d|\\sigma_f|(a) < \\infty$. It would be a good exercise to check whether this restriction is satisfied by some familiar types of functions. Barron ( 1993 ) first showed that neural networks approximate this class of functions well, and compares the approximation rate of neural networks to other function approximation results.","title":"Rate of convergence"},{"location":"slp/#references-references","text":"Barron, A. R. 1993. \u201cUniversal Approximation Bounds for Superpositions of a Sigmoidal Function.\u201d IEEE Transactions on Information Theory 39 (3): 930\u201345. https://doi.org/10.1109/18.256500 . Chen, Xiaohong, and H. White. 1999. \u201cImproved Rates and Asymptotic Normality for Nonparametric Neural Network Estimators.\u201d IEEE Transactions on Information Theory 45 (2): 682\u201391. https://doi.org/10.1109/18.749011 . Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning . MIT Press. http://www.deeplearningbook.org . Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. \u201cMultilayer Feedforward Networks Are Universal Approximators.\u201d Neural Networks 2 (5): 359\u201366. https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8 . Klok, Hayden, and Yoni Nazarathy. 2019. Statistics with Julia:Fundamentals for Data Science, Machinelearning and Artificial Intelligence . DRAFT. https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf .","title":"References [references]"}]}