{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title       : \"Introduction to Neural Networks\" \n",
    "subtitle    : \"Single Layer Perceptrons\"\n",
    "author      : Paul Schrimpf\n",
    "date        : `j using Dates; print(Dates.today())`\n",
    "bibliography: \"../ml.bib\"\n",
    "---\n",
    "\n",
    "[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution-ShareAlike\n",
    "4.0 International\n",
    "License](http://creativecommons.org/licenses/by-sa/4.0/) \n",
    "\n",
    "\n",
    "### About this document \n",
    "\n",
    "This document was created using Weave.jl. The code is available in\n",
    "[on github](https://github.com/schrimpf/NeuralNetworkEconomics.jl). The same\n",
    "document generates both static webpages and associated (jupyter\n",
    "notebook)[slp.ipynb]. \n",
    "\n",
    "$$\n",
    "\\def\\indep{\\perp\\!\\!\\!\\perp}\n",
    "\\def\\Er{\\mathrm{E}}\n",
    "\\def\\R{\\mathbb{R}}\n",
    "\\def\\En{{\\mathbb{E}_n}}\n",
    "\\def\\Pr{\\mathrm{P}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n",
    "\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mActivating\u001b[22m\u001b[39m environment at `~/.julia/dev/NeuralNetworkEconomics/docs/Project.toml`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h"
     ]
    }
   ],
   "source": [
    "markdown = try\n",
    "  \"md\" in keys(WEAVE_ARGS) && WEAVE_ARGS[\"md\"]\n",
    "catch\n",
    "  false\n",
    "end\n",
    "\n",
    "if !(\"DISPLAY\" ∈ keys(ENV))\n",
    "  # Make gr and pyplot backends for Plots work without a DISPLAY\n",
    "  ENV[\"GKSwstype\"]=\"nul\"\n",
    "  ENV[\"MPLBACKEND\"]=\"Agg\"\n",
    "end\n",
    "\n",
    "using NeuralNetworkEconomics\n",
    "docdir = joinpath(dirname(Base.pathof(NeuralNetworkEconomics)), \"..\",\"docs\")\n",
    "\n",
    "using Pkg\n",
    "Pkg.activate(docdir)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Neural networks, especially deep neural networks, have come to\n",
    "dominate some areas of machine learning. Neural networks are\n",
    "especially prominent in natural language processing, image\n",
    "classification, and reinforcement learning. This documents gives a\n",
    "brief introduction to neural networks. \n",
    "\n",
    "Examples in this document will use\n",
    "[`Flux.jl`](https://fluxml.ai/Flux.jl/stable/). \n",
    "An alternative Julia package for deep learning is\n",
    "[`Knet.jl`](https://denizyuret.github.io/Knet.jl/latest/). There is a\n",
    "good discussion comparing Flux and Knet [on\n",
    "discourse.](https://discourse.julialang.org/t/state-of-deep-learning-in-julia/28049). \n",
    "We will not have Knet examples here, but the documentation for Knet is\n",
    "excellent and worth reading even if you plan to use Flux.\n",
    "\n",
    "## Additional Reading\n",
    "\n",
    "- @goodfellow2016 [*Deep Learning*](http://www.deeplearningbook.org)\n",
    "- [`Knet.jl`\n",
    "  documentation](https://denizyuret.github.io/Knet.jl/latest/)\n",
    "  especially the textbook\n",
    "- @klok2019 *Statistics with Julia:Fundamentals for Data Science,\n",
    "  MachineLearning and Artificial Intelligence*\n",
    "    \n",
    "  \n",
    "# Single Layer Neural Networks\n",
    "\n",
    "We will describe neural networks from a perspective of nonparametric\n",
    "estimation. Suppose we have a target function, $f: \\R^p \\to \\R$. In\n",
    "many applications the target function will be a conditional\n",
    "expectation, $f(x) = \\Er[y|x]$. \n",
    "\n",
    "A single layer neural network approximates $f$ as follows\n",
    "$$\n",
    "\\hat{f}(x) = \\sum_{j=1}^r \\beta_j \n",
    "\\psi(w_j'x + b_j)\n",
    "$$\n",
    "Here $r$ is the width of the layer. $\\beta_j$ are scalars. \n",
    "$\\psi:\\R \\to \\R$ is a nonlinear activation function. Common activation\n",
    "functions include:\n",
    "\n",
    "- Sigmoid $\\psi(t) = 1/(1+e^{-t})$\n",
    "\n",
    "- Tanh $\\psi(t) = \\frac{e^t -e^{-t}}{e^t + e^{-t}}$\n",
    "\n",
    "- Rectified linear $\\psi(t) = t 1(t\\geq 0)$\n",
    "\n",
    "The $w_j \\in \\R^p$ are called weights and $b_j \\in \\R$ are biases. \n",
    "\n",
    "You may have heard about the universal approximation theorem. This\n",
    "refers to the fact that as $r$ increases, a neural network can\n",
    "approximate any function. Mathematically, for some large \n",
    "class of functions $\\mathcal{F}$, \n",
    "\n",
    "$$\n",
    "\\sup_{f \\in \\mathcal{F}} \\lim_{r \\to \\infty} \\inf_{\\beta, w, b} \\Vert\n",
    "f(x) - \\sum_{j=1}^r \\beta_j \\psi(w_j'x+b_j) \\Vert = 0\n",
    "$$\n",
    "\n",
    "@hornik1989 contains one of the earliest results along these\n",
    "lines. Some introductory texts mention the universal approximation\n",
    "theorem as though it is something special for neural networks. This is\n",
    "incorrect. In particular, the universal approximation theorem does not\n",
    "explain why neural networks seem to be unusually good at\n",
    "prediction. Most nonparametric estimation methods (kernel, series,\n",
    "forests, etc) satisfy a similar conditions. \n",
    "\n",
    "# Training \n",
    "\n",
    "Models in `Flux.jl` all involve a differentiable loss function. The\n",
    "loss function is minimized by a variant of gradient descent. Gradients\n",
    "are usually calculated using reverse automatic differentiation\n",
    "(backpropagation is a variant of reverse automatic differentiation\n",
    "specialized for the structue of neural networks). A low level way to\n",
    "use `Flux.jl` is to write your loss function as a typical Julia\n",
    "function, as in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 5\n",
       " 7\n",
       " 9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots, Flux, Statistics\n",
    "# some function to estimate\n",
    "f(x) = sin(x^x)/2^((x^x-π/2)/π)\n",
    "function simulate(n,σ=1)\n",
    "  x = rand(n,1).*π\n",
    "  y = f.(x) .+ randn(n).*σ\n",
    "  (x,y)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1 )\n",
    "\n",
    "Construct a single layer perception with width `r`. \n",
    "\"\"\"\n",
    "function slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1)\n",
    "  # Parameters to be minimized wrt have to be declared for tracking\n",
    "  # for reverse mode autodiff.\n",
    "  w = param(randn(dimx,r))\n",
    "  b = param(randn(1,r))\n",
    "  β = param(randn(r))\n",
    "  θ = Tracker.Params([β, w, b])\n",
    "  pred(x) = activation(x*w.+b)*β\n",
    "  loss(x,y) = mean((y.-pred(x)).^2)\n",
    "  return(θ=θ, predict=pred,loss=loss)\n",
    "end\n",
    "x, y = simulate(1000, 0.5)\n",
    "xg = 0:0.01:π\n",
    "rs = [2, 3, 5, 7, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: @display not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: @display not defined",
      ""
     ]
    }
   ],
   "source": [
    "figs = Array{typeof(plot(0)),1}(undef,length(rs))\n",
    "for r in eachindex(rs)\n",
    "  m = slp(rs[r])\n",
    "  figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\")\n",
    "  figs[r]=scatter!(x,y, alpha=0.3, lab=\"\")\n",
    "  maxiter = 5000\n",
    "  for i = 1:maxiter\n",
    "    Flux.train!(m.loss, m.θ, [(x, y)], Flux.AMSGrad())\n",
    "    if (i % (maxiter ÷ 5))==0\n",
    "      l=Tracker.data(m.loss(x,y))\n",
    "      @display \"$i iteration, loss=$l\"\n",
    "      figs[r]=plot!(xg,Tracker.data(m.predict(xg))) #, lab=\"$i iterations, loss=$l\")\n",
    "    end\n",
    "  end\n",
    "end\n",
    "plot(figs...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how even though a wider network can approximate $f$ better,\n",
    "wider networks also take more training iterations to minimize the\n",
    "loss. This is typical of any minimization algorithm --- the number of\n",
    "iterations increases with the problem size. \n",
    "\n",
    "Each invocation of `Flux.train!` completes one iteration of gradient\n",
    "descent. As you might guess from this API, it is common to train\n",
    "neural networks for a fixed number of iterations instead of until\n",
    "convergence to a local minimum. The number of training iterations can\n",
    "act as a regularization parameter. \n",
    "\n",
    "`Flux.jl` also contains some higher level functions for creating loss\n",
    "functions for neural networks. Here is the same network as in the\n",
    "previous code block, but using the higher level API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: @display not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: @display not defined",
      ""
     ]
    }
   ],
   "source": [
    "dimx = 1\n",
    "figs = Array{typeof(plot(0)),1}(undef,length(rs))\n",
    "for r in eachindex(rs)\n",
    "  m = Chain(Dense(dimx, rs[r], Flux.σ), Dense(rs[r], 1))\n",
    "  figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\")\n",
    "  figs[r]=scatter!(x,y, alpha=0.3, lab=\"\")  \n",
    "  maxiter = 5000\n",
    "  for i = 1:maxiter\n",
    "    Flux.train!((x,y)->Flux.mse(m(x'),y'), Flux.params(m), [(x, y)], Flux.AMSGrad() ) #,\n",
    "                #cb = Flux.throttle(()->@show(Flux.mse(m(x'),y')),100))\n",
    "    if (i % (maxiter ÷ 5))==0\n",
    "      l=Tracker.data(Flux.mse(m(x'), y'))\n",
    "      @display \"$i iteration, loss=$l\"\n",
    "      figs[r]=plot!(xg,Tracker.data(m(xg'))')#, lab=\"$i iterations, loss=$l\")\n",
    "    end\n",
    "  end\n",
    "end\n",
    "plot(figs..., title=\"Sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures may not appear identical to the first example since the\n",
    "initial values differ. \n",
    "\n",
    "Large applications of neural networks often use rectified linear\n",
    "activation for efficiency. Let's see how the same example behaves with\n",
    "(leaky) rectified linear activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: @display not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: @display not defined",
      ""
     ]
    }
   ],
   "source": [
    "dimx = 1\n",
    "figs = Array{typeof(plot(0)),1}(undef,length(rs))\n",
    "for r in eachindex(rs)\n",
    "  m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change \n",
    "  figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\")\n",
    "  figs[r]=scatter!(x,y, alpha=0.3, lab=\"\")  \n",
    "  maxiter = 5000\n",
    "  for i = 1:maxiter\n",
    "    Flux.train!((x,y)->Flux.mse(m(x'),y'), Flux.params(m), [(x, y)], Flux.AMSGrad()) #,\n",
    "                #cb = Flux.throttle(()->@show(Flux.mse(m(x'),y')),100))\n",
    "    if (i % (maxiter ÷ 5))==0\n",
    "      l=Tracker.data(Flux.mse(m(x'), y'))\n",
    "      @display \"$i iteration, loss=$l\"\n",
    "      figs[r]=plot!(xg,Tracker.data(m(xg'))')#, lab=\"$i iterations, loss=$l\")\n",
    "    end\n",
    "  end\n",
    "end\n",
    "plot(figs..., title=\"ReLu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate of convergence\n",
    "\n",
    "- @chen1999\n",
    "- $f(x) = \\Er[y|x]$ with Fourier representation\n",
    "$$ f(x) = \\int e^{i a'x} d\\sigma_f(a) $$\n",
    "  where $\\int (\\sqrt{a'a} \\vee 1) d|\\sigma_f|(a) < \\infty$\n",
    "- Network sieve :\n",
    "$$ \\begin{align*}\n",
    "\\mathcal{G}_n = \\{ &\n",
    "g: g(x) = \\sum_{j=1}^{r_n} \\beta_j (a_j'a_j \\vee 1)^{-1}\n",
    "\\psi(a_j'x + b_j), \\\\ & \\norm{\\beta}_1 \\leq B_n \\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The setup in @chen1999 is more general. They consider estimating both\n",
    "$f$ and its first $m$ derivatives. Here, we focus on the case of just\n",
    "estimating $f$. @chen1999 also consider estimation of functions other\n",
    "than conditional expectations.\n",
    "\n",
    "The restriction on $f$ in the second bullet is used to control\n",
    "approximation error. The second bullet says that $f$ is the inverse\n",
    "Fourier transform of measure $\\sigma_f$. The bite of the restriction\n",
    "on $f$ comes from the requirement that $\\sigma_f$ be absolutely\n",
    "integral, $\\int (\\sqrt{a'a} \\vee 1) d|\\sigma_f|(a) < \\infty$. It would\n",
    "be a good exercise to check whether this restriction is satisfied by\n",
    "some familiar types of functions. @barron1993 first showed that neural\n",
    "networks approximate this class of functions well, and compares the\n",
    "approximation rate of neural networks to other function approximation\n",
    "results.\n",
    "\n",
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
