{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\ntitle       : \"Introduction to Neural Networks\" \nsubtitle    : \"Single Layer Perceptrons\"\nauthor      : Paul Schrimpf\ndate        : `j using Dates; print(Dates.today())`\nbibliography: \"../ml.bib\"\n---\n\n[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)\n\nThis work is licensed under a [Creative Commons Attribution-ShareAlike\n4.0 International\nLicense](http://creativecommons.org/licenses/by-sa/4.0/) \n\n\n### About this document \n\nThis document was created using Weave.jl. The code is available in\n[on github](https://github.com/schrimpf/NeuralNetworkEconomics.jl). The same\ndocument generates both static webpages and associated (jupyter\nnotebook)[neural.ipynb]. \n\n$$\n\\def\\indep{\\perp\\!\\!\\!\\perp}\n\\def\\Er{\\mathrm{E}}\n\\def\\R{\\mathbb{R}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\Pr{\\mathrm{P}}\n\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "markdown = try\n  \"md\" in keys(WEAVE_ARGS) && WEAVE_ARGS[\"md\"]\ncatch\n  false\nend\n\nif !(\"DISPLAY\" ∈ keys(ENV))\n  # Make gr and pyplot backends for Plots work without a DISPLAY\n  ENV[\"GKSwstype\"]=\"nul\"\n  ENV[\"MPLBACKEND\"]=\"Agg\"\nend\n\nusing NeuralNetworkEconomics\ndocdir = joinpath(dirname(Base.pathof(NeuralNetworkEconomics)), \"..\",\"docs\")\n\nusing Pkg\nPkg.activate(docdir)\nPkg.instantiate()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n\nNeural networks, especially deep neural networks, have come to\ndominate some areas of machine learning. Neural networks are\nespecially prominent in natural language processing, image\nclassification, and reinforcement learning. This documents gives a\nbrief introduction to neural networks. \n\nExamples in this document will use\n[`Flux.jl`](https://fluxml.ai/Flux.jl/stable/). \nAn alternative Julia package for deep learning is\n[`Knet.jl`](https://denizyuret.github.io/Knet.jl/latest/). There is a\ngood discussion comparing Flux and Knet [on\ndiscourse.](https://discourse.julialang.org/t/state-of-deep-learning-in-julia/28049). \nWe will not have Knet examples here, but the documentation for Knet is\nexcellent and worth reading even if you plan to use Flux.\n\n## Additional Reading\n\n- @goodfellow2016 [*Deep Learning*](http://www.deeplearningbook.org)\n- [`Knet.jl`\n  documentation](https://denizyuret.github.io/Knet.jl/latest/)\n  especially the textbook\n- @klok2019 *Statistics with Julia:Fundamentals for Data Science,\n  MachineLearning and Artificial Intelligence*\n    \n  \n# Single Layer Neural Networks\n\nWe will describe neural networks from a perspective of nonparametric\nestimation. Suppose we have a target function, $f: \\R^p \\to \\R$. In\nmany applications the target function will be a conditional\nexpectation, $f(x) = \\Er[y|x]$. \n\nA single layer neural network approximates $f$ as follows\n$$\n\\hat{f}(x) = \\sum_{j=1}^r \\beta_j \n\\psi(w_j'x + b_j)\n$$\nHere $r$ is the width of the layer. $\\beta_j$ are scalars. \n$\\psi:\\R \\to \\R$ is a nonlinear activation function. Common activation\nfunctions include:\n\n- Sigmoid $\\psi(t) = 1/(1+e^{-t})$\n\n- Tanh $\\psi(t) = \\frac{e^t -e^{-t}}{e^t + e^{-t}}$\n\n- Rectified linear $\\psi(t) = t 1(t\\geq 0)$\n\nThe $w_j \\in \\R^p$ are called weights and $b_j \\in \\R$ are biases. \n\nYou may have heard about the universal approximation theorem. This\nrefers to the fact that as $r$ increases, a neural network can\napproximate any function. Mathematically, for some large \nclass of functions $\\mathcal{F}$, \n\n$$\n\\sup_{f \\in \\mathcal{F}} \\lim_{r \\to \\infty} \\inf_{\\beta, w, b} \\Vert\nf(x) - \\sum_{j=1}^r \\beta_j \\psi(w_j'x+b_j) \\Vert = 0\n$$\n\n@hornik1989 contains one of the earliest results along these\nlines. Some introductory texts mention the universal approximation\ntheorem as though it is something special for neural networks. This is\nincorrect. In particular, the universal approximation theorem does not\nexplain why neural networks seem to be unusually good at\nprediction. Most nonparametric estimation methods (kernel, series,\nforests, etc) satisfy a similar conditions. \n\n# Training \n\nModels in `Flux.jl` all involve a differentiable loss function. The\nloss function is minimized by a variant of gradient descent. Gradients\nare usually calculated using reverse automatic differentiation\n(backpropagation is a variant of reverse automatic differentiation\nspecialized for the structue of neural networks). A low level way to\nuse `Flux.jl` is to write your loss function as a typical Julia\nfunction, as in the following code block."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots, Flux, Statistics\n# some function to estimate\nf(x) = sin(x^x)/2^((x^x-π/2)/π)\nfunction simulate(n,σ=1)\n  x = rand(n,1).*π\n  y = f.(x) .+ randn(n).*σ\n  (x,y)\nend\n\n\"\"\"\n   slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1 )\n\nConstruct a single layer perception with width `r`. \n\"\"\"\nfunction slp(r, activation=(t)-> 1 ./ (1 .+ exp.(.-t)), dimx=1)\n  # Parameters to be minimized wrt have to be declared for tracking\n  # for reverse mode autodiff.\n  w = param(randn(dimx,r))\n  b = param(randn(1,r))\n  β = param(randn(r))\n  θ = Tracker.Params([β, w, b])\n  pred(x) = activation(x*w.+b)*β\n  loss(x,y) = mean((y.-pred(x)).^2)\n  return(θ=θ, predict=pred,loss=loss)\nend\nx, y = simulate(1000, 0.5)\nxg = 0:0.01:π\nrs = [2, 3, 5, 7, 9]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "figs = Array{typeof(plot(0)),1}(undef,length(rs))\nfor r in eachindex(rs)\n  m = slp(rs[r])\n  figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\")\n  figs[r]=scatter!(x,y, alpha=0.3)\n  maxiter = 5000\n  for i = 1:maxiter\n    Flux.train!(m.loss, m.θ, [(x, y)], Flux.AMSGrad())\n    if (i % (maxiter ÷ 5))==0\n      l=Tracker.data(m.loss(x,y))\n      figs[r]=plot!(xg,Tracker.data(m.predict(xg)), lab=\"$i iterations, loss=$l\")\n    end\n  end\nend\nplot(figs...)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how even though a wider network can approximate $f$ better,\nwider networks also take more training iterations to minimize the\nloss. This is typical of any minimization algorithm --- the number of\niterations increases with the problem size. \n\nEach invocation of `Flux.train!` completes one iteration of gradient\ndescent. As you might guess from this API, it is common to train\nneural networks for a fixed number of iterations instead of until\nconvergence to a local minimum. The number of training iterations can\nact as a regularization parameter. \n\n`Flux.jl` also contains some higher level functions for creating loss\nfunctions for neural networks. Here is the same network as in the\nprevious code block, but using the higher level API."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "dimx = 1\nfigs = Array{typeof(plot(0)),1}(undef,length(rs))\nfor r in eachindex(rs)\n  m = Chain(Dense(dimx, rs[r], Flux.σ), Dense(rs[r], 1))\n  figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\")\n  figs[r]=scatter!(x,y, alpha=0.3)  \n  maxiter = 5000\n  for i = 1:maxiter\n    Flux.train!((x,y)->Flux.mse(m(x'),y'), Flux.params(m), [(x, y)], Flux.AMSGrad() ) #,\n                #cb = Flux.throttle(()->@show(Flux.mse(m(x'),y')),100))\n    if (i % (maxiter ÷ 5))==0\n      l=Tracker.data(Flux.mse(m(x'), y'))\n      figs[r]=plot!(xg,Tracker.data(m(xg'))', lab=\"$i iterations, loss=$l\")\n    end\n  end\nend\nplot(figs..., title=\"Sigmoid\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The figures may not appear identical to the first example since the\ninitial values differ. \n\nLarge applications of neural networks often use rectified linear\nactivation for efficiency. Let's see how the same example behaves with\n(leaky) rectified linear activation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "dimx = 1\nfigs = Array{typeof(plot(0)),1}(undef,length(rs))\nfor r in eachindex(rs)\n  m = Chain(Dense(dimx, rs[r], Flux.leakyrelu), Dense(rs[r], 1)) # notice the change \n  figs[r]=plot(xg, f.(xg), lab=\"True f\", title=\"$(rs[r]) units\")\n  figs[r]=scatter!(x,y, alpha=0.3)  \n  maxiter = 5000\n  for i = 1:maxiter\n    Flux.train!((x,y)->Flux.mse(m(x'),y'), Flux.params(m), [(x, y)], Flux.AMSGrad()) #,\n                #cb = Flux.throttle(()->@show(Flux.mse(m(x'),y')),100))\n    if (i % (maxiter ÷ 5))==0\n      l=Tracker.data(Flux.mse(m(x'), y'))\n      figs[r]=plot!(xg,Tracker.data(m(xg'))', lab=\"$i iterations, loss=$l\")\n    end\n  end\nend\nplot(figs..., title=\"ReLu\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rate of convergence\n\n- @chen1999\n- $f(x) = \\Er[y|x]$ with Fourier representation\n$$ f(x) = \\int e^{i a'x} d\\sigma_f(a) $$\n  where $\\int (\\sqrt{a'a} \\vee 1) d|\\sigma_f|(a) < \\infty$\n- Network sieve :\n$$ \\begin{align*}\n\\mathcal{G}_n = \\{ &\ng: g(x) = \\sum_{j=1}^{r_n} \\beta_j (a_j'a_j \\vee 1)^{-1}\n\\psi(a_j'x + b_j), \\\\ & \\norm{\\beta}_1 \\leq B_n \\}\n\\end{align*}\n$$\n\nThe setup in @chen1999 is more general. They consider estimating both\n$f$ and its first $m$ derivatives. Here, we focus on the case of just\nestimating $f$. @chen1999 also consider estimation of functions other\nthan conditional expectations.\n\nThe restriction on $f$ in the second bullet is used to control\napproximation error. The second bullet says that $f$ is the inverse\nFourier transform of measure $\\sigma_f$. The bite of the restriction\non $f$ comes from the requirement that $\\sigma_f$ be absolutely\nintegral, $\\int (\\sqrt{a'a} \\vee 1) d|\\sigma_f|(a) < \\infty$. It would\nbe a good exercise to check whether this restriction is satisfied by\nsome familiar types of functions. @barron1993 first showed that neural\nnetworks approximate this class of functions well, and compares the\napproximation rate of neural networks to other function approximation\nresults."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.2.0"
    },
    "kernelspec": {
      "name": "julia-1.2",
      "display_name": "Julia 1.2.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
