<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Convolutional -  </title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <link href="../assets/extra.css" rel="stylesheet">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href=".."> </a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href=".." class="nav-link">Package Docs</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">ML in Economics <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../ml-intro/" class="dropdown-item">Introduction</a>
</li>
                                    
<li>
    <a href="../ml-methods/" class="dropdown-item">Methods</a>
</li>
                                    
<li>
    <a href="../ml-doubledebiased/" class="dropdown-item">Inference</a>
</li>
                                    
<li>
    <a href="../mlExamplePKH/" class="dropdown-item">Detecting heterogeneity</a>
</li>
                                    
<li>
    <a href="../ml-julia/" class="dropdown-item">With Julia</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Neural Networks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../slp/" class="dropdown-item">Introduction</a>
</li>
                                    
<li>
    <a href="../mlp/" class="dropdown-item">Multi-Layer</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active">Convolutional</a>
</li>
                                    
<li>
    <a href="../rnn/" class="dropdown-item">Recurrent</a>
</li>
                                    
<li>
    <a href="../transformers/" class="dropdown-item">Transformers</a>
</li>
                                    
<li>
    <a href="../nn-semiparametric/" class="dropdown-item">In semiparametric models</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../license/" class="dropdown-item">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../mlp/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../rnn/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/schrimpf/NeuralNetworkEconomics.jl/edit/master/docs/conv.md" class="nav-link"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            
            <li class="nav-item" data-level="1"><a href="#introduction" class="nav-link">Introduction</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#additional-reading" class="nav-link">Additional Reading</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#convolutions" class="nav-link">Convolutions</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#example-mnist" class="nav-link">Example : MNIST</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#small-batches" class="nav-link">Small Batches</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#data-augmentation" class="nav-link">Data Augmentation</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#small-batches-and-data-augmentation" class="nav-link">Small Batches and Data Augmentation</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#looking-inside-the-black-box" class="nav-link">Looking inside the Black Box</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#references" class="nav-link">References</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a></p>
<h3 id="about-this-document">About this document<a class="headerlink" href="#about-this-document" title="Permanent link">&para;</a></h3>
<p>This document was created using Weave.jl. The code is available in <a href="https://github.com/schrimpf/NeuralNetworkEconomics.jl">on
github</a>. The same
document generates both static webpages and associated <a href="../slp.ipynb">jupyter
notebook</a>.</p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
</script>
</p>
<h1 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<p><a href="../mlp/">The previous notes</a> discussed multiple layer feedforward
networks, and applied them to image classification. However,
state-of-art image classifiers typically do not use feedforward
networks. They use convolutional networks, which will be the topic of
this document. Convolutional neural networks also have applications to
PDEs, see @rackauckas2019conv. They also have potential applications to
time series and spatial data.</p>
<h2 id="additional-reading">Additional Reading<a class="headerlink" href="#additional-reading" title="Permanent link">&para;</a></h2>
<ul>
<li>@goodfellow2016 <a href="http://www.deeplearningbook.org"><em>Deep Learning</em></a>
    especially chapter 9</li>
<li><a href="https://denizyuret.github.io/Knet.jl/latest/"><code>Knet.jl</code>
    documentation</a>
    especially the textbook</li>
<li>@klok2019 <em>Statistics with Julia:Fundamentals for Data Science,
    MachineLearning and Artificial Intelligence</em></li>
</ul>
<h1 id="convolutions">Convolutions<a class="headerlink" href="#convolutions" title="Permanent link">&para;</a></h1>
<p>A convolution is an operation on a set of functions. If $f:\R \to \R$
and $g:\R \to \R$, then their convolution is <script type="math/tex; mode=display">
(f \ast g)(x) = \int_\R f(t) g(x-t) dt
</script> The convolution is commutative, $(f\ast g)(x) = (g\ast f)(x)$. For
functions with other domains, convolutions can be defined analogously.
For example, for $f, g: \mathbb{Z} \to \R$, <script type="math/tex; mode=display">
(f \ast g)(j) = \sum_{i \in \mathbb{Z}} f(i)g(j-i)
</script>
</p>
<p>Kernel density estimation and regression are convolutions. For example,
the kernel density estimator can be written as <script type="math/tex; mode=display">
\begin{align*}
\hat{f}(x) = & \frac{1}{nh} \sum_{i=1}^n k((x-x_i)/h) \\
= & \int \left(\frac{1}{n} \sum_{i=1}^n \delta_{x_i}(t) \right)k((x-t)/h)/h
dt
\end{align*}
</script> where $\delta_{x_i}(t)$ is the Dirac measure at $x_i$.</p>
<p>Convolutions appear in image processing as blurring and smoothing
filters. Taking a local average of pixels is a convolution.</p>
<p><img alt="convolution" src="https://hackernoon.com/hn-images/1*ZCjPUFrB6eHPRi4eyP6aaA.gif" /></p>
<p><a href="https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59">Image
source</a></p>
<p>The similarity to kernel regression should be obvious. The 3x3 matrix</p>
<pre><code>1 0 1
0 1 0
1 0 1
</code></pre>
<p>in the animation above is called a stencil or a kernel.</p>
<p>In addition to blurring and smoothing, convolutions can detect patterns
in images. For example 1 × 2 filters of <code>[-1 1]</code> and <code>[1, -1]</code> will pick
out vertical transitions from dark to light and vice versa.</p>
<p>Here’s an illustration of how this looks using an image from the MNIST
data.</p>
<pre><code class="language-julia">using MLDatasets
using ImageFiltering
using Plots
Plots.pyplot()

train_x, labels = MNIST(split=:train)[:]
imgs = [Gray.(train_x[:,:,i]') for i ∈ 1:size(train_x,3)]

i = 1
plot(plot(RGB.(1 .- imgs[i]), title=&quot;Original&quot;, aspect_ratio=:equal,
          axis=false, ticks=false),
     plot(RGB.(1 .- imgs[i] .+ imfilter(imgs[i], centered([-1 1])),
               1 .- imgs[i],
               1 .- imgs[i] .+ imfilter(imgs[i], centered([1 -1]))),
               title=&quot;Vertical edge filters&quot;, aspect_ratio=:equal,
          axis=false, ticks=false),
     layout=(1, 2)
     )
</code></pre>
<p><img alt="" src="../figures/conv_2_1.png" /></p>
<p>In this image, the output of one edge filter is yellow, the other is
blue, and the original image is black.</p>
<p>From this example, we see that small hand-crafted convolution kernels
can pick out patterns, like edges. One idea would be to use the output
of these kernels as features in a machine learning model. Another idea
is to treat the weigths in a convolution matrix as part of the model to
be estimated. This is exactly what convolutional neural networks do.</p>
<h1 id="example-mnist">Example : MNIST<a class="headerlink" href="#example-mnist" title="Permanent link">&para;</a></h1>
<p>Let’s see how convolutional neural networks can be applied to the MNIST
data.</p>
<div class="alert alert-danger">
<p>The code in this section was adapted from the <a href="https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl">Flux model
zoo.</a></p>
</div>
<pre><code class="language-julia">using Flux, MLDatasets, Statistics
using Flux: onehotbatch, onecold, throttle, @epochs
using Base.Iterators: repeated, partition
using JLD2
using ColorSchemes, ProgressMeter
cscheme = colorschemes[:BrBG_4];
</code></pre>
<p>When we were fitting feed-forward networks with this data, we simply
treated images as vetors of length $28^2$. That was appropriate there
because the model we were using did not explicitly utilize spacial
information. With convolutions, we need to preserve spatial information,
so we need to treat the images as $28 \times 28$ arrays.</p>
<pre><code class="language-julia"># set up training and testing data
createx(imgdata) = reduce((x,y)-&gt;cat(x,y, dims=4),
                          reshape.(float.(imgdata),28,28, 1) )
X = Float32.(reshape(train_x, 28, 28, 1, size(train_x,3))) |&gt; gpu
Y = onehotbatch(labels, 0:9) |&gt; gpu
test_x, test_y = MNIST(split=:test)[:]
tX = Float32.(reshape(test_x, 28, 28, 1, size(test_x,3))) |&gt; gpu
tY = onehotbatch(test_y, 0:9) |&gt; gpu;
</code></pre>
<p>When working with convolutions, <code>Flux.jl</code> wants the data to be stored as
a $K_1 \times K_2 \times C \times N$ array where $K_1 \times K_2$ is the
dimension of the images (or whatever else the data represents), $C$ is
the number of channels. Since we have grayscale images, we have one
channel. Color images would have three channels. Radar and satellite
imagery can have more channels (satellites often collect non-visible
frequencies of light).</p>
<p>Channels are also a useful abstraction throughout the neural network. We
usually want to apply multiple convolution filters to extract different
features of the images. The output of each convolution is stored in a
channel.</p>
<p>Each convolution is intended to pick up some local pattern in the image.
It might be useful to modify certain pattens by applying an activation
function to the output of the convolution.</p>
<p>Finally, convolutions detect patterns, but for many image classification
tasks, we do not necessarily care exactly where a pattern occurs. A cat
remains a a cat wherever it is located in a picture. Motivated by this,
a “pooling” operation is often applied to the output of convolutions.
These are similar to convolutions with fixed weights. Common pooling
operations include taking the average within a rectangle of fixed size
and taking the maximum. Pooling (and convolution) can reduce dimension
by only looking at non-overlapping (or partially non-overlapping)
regions. In <code>Flux.jl</code> the default for convolutions is to look at every
pixel, and the default for pooling is to look at non-overlapping
regions. This behavior can be changed by changing the <code>stride</code> option.</p>
<p>Mathematically, we can express a convolutional layer as follows. Let $x$
be an $N \times N$ input channel, $\psi$ be an activation function,
$b \in \R$ is a bias, and $w$ be $M \times M$ convolution weights,
indexed from -M/2 to M/2.</p>
<p>The result of applying the convolution is: <script type="math/tex; mode=display">
\psi((x \ast w)[k,\ell]) = \psi\left(\sum_{i=1-P}^{N+P} \sum_{j=1-P}^{N+P}
x[i,j]*w[k-i, j - \ell] + b\right)
</script> where if an index is out-of-bounds simply set that term in the sum to
$0$. $P$ is the “padding”. If $P=M/2$, then the size of $x\ast w$ is the
same as $x$. If $P&lt;M/2$, the convolution decreases the size of the
image. Then if we apply a maximum pooling function with dimension
$D \times D$, we get a $N/D \times N/D$ array with elements <script type="math/tex; mode=display">
maxpool(\psi((x \ast w) )[n,m] = \max_{1 \leq i \leq d, 1 \leq j \leq
d} \{\psi((x \ast w)[dk+i,d\ell+j])\}
</script>
</p>
<p>The following code defines a convolutional network with three
convolutional layers followed by a dense feed-forward layer for the
output.</p>
<pre><code class="language-julia">model = Chain(
  # First convolution layer, operating upon a 28x28 image
  Conv((3, 3), 1=&gt;16,  pad=(1,1), relu),
  MaxPool((2,2)),

  # Second convolution, operating upon a 14x14 image
  Conv((3, 3), 16=&gt;32, pad=(1,1), relu),
  MaxPool((2,2)),

  # Third convolution, operating upon a 7x7 image
  Conv((3, 3), 32=&gt;32, pad=(1,1), relu),
  MaxPool((2,2)),

  # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N)
  # which is where we get the 288 in the `Dense` layer below:
  x -&gt; reshape(x, :, size(x, 4)),
  Dense(288, 10) #,

  # Finally, softmax to get nice probabilities
  #softmax
) |&gt; gpu


# count parameters
nparam(m::Chain)=sum(nparam.(m))
nparam(m::Conv)=length(m.weight) + length(m.bias)
nparam(m::Dense)=length(m.weight)+length(m.bias)
nparam(m) = 0

println(&quot;There $(nparam(model)) parameters&quot;)
</code></pre>
<pre><code>There 16938 parameters
</code></pre>
<p>Now, let’s train the model. We are going to train the model a few times
with slightly different details, so it makes sense to define a function
for the training loop.</p>
<pre><code class="language-julia">function accuracy(m, x, y)
  # onecold(m(x)) results in very slow code for large x, so we avoid it
  coldx = vec(map(x-&gt;x[1], argmax(m(x), dims=1)))
  coldy = onecold(y)
  return(mean(coldx.==coldy))
end

function train_mnist!(model, X, Y, tX, tY,
                      modelname;
                      loss = (x,y)-&gt;Flux.logitcrossentropy(model(x), y),
                      accuracy = (x,y)-&gt;accuracy(model, x, y),
                      batchsize=length(tY),
                      reps_per_epoch=1,
                      maxepochs=200,
                      rerun=false
                      )
  Xsmall = X[:,:,:,1:1000]
  Ysmall = Y[:,1:1000]
  evalcb = () -&gt; @show(loss(Xsmall, Ysmall), loss(tX,tY), accuracy(Xsmall,Ysmall), accuracy(tX,tY))
  parts=Base.Iterators.partition(1:size(X,ndims(X)), batchsize);
  batches = [(X[:,:,:,p], Y[:,p]) for p in parts];
  data = repeat(batches, reps_per_epoch);
  # The model and entire training data do not fit in my GPU
  # memory. For monitoring progress we will occassionally print the
  # loss and accuracy summed over the entire data.
  function sumloss(batches)
    L = zero(Float32)
    for i in 1:length(batches)
      L += loss(batches[i]...)
    end
    L /= length(batches)
  end
  function acc(batches)
    L = zero(Float32)
    for i in 1:length(batches)
      L += accuracy(batches[i]...)
    end
    L /= length(batches)
  end

  opt = ADAM(0.001)
  acctest  = zeros(maxepochs)
  acctrain = zeros(maxepochs)
  losstest  = zeros(maxepochs)
  losstrain = zeros(maxepochs)
  @info(&quot;Beginning training loop...&quot;)
  best_acc = 0.0
  last_improvement = 0
  e = 0
  docdir = joinpath(dirname(Base.pathof(NeuralNetworkEconomics)),
                    &quot;..&quot;,&quot;docs&quot;)
  progress = Progress(maxepochs, 1, &quot;Training model&quot;, 50)
  while e&lt;maxepochs
    e += 1
    modelfile = joinpath(docdir,&quot;jmd&quot;,&quot;models&quot;,&quot;$modelname-$e-epochs.jld2&quot;)
    if rerun || !isfile(modelfile)
      @time Flux.train!(loss, Flux.params(model), data, opt, cb = throttle(evalcb, 20))
      # save model
      cpum = cpu(model)
      losstrain[e]= sumloss(batches)
      acctrain[e] =  acc(batches)
      losstest[e]=loss(tX,tY)
      acctest[e] =accuracy(tX,tY)
      @save modelfile cpum losstrain acctrain losstest acctest
    else
      @load modelfile cpum losstrain acctrain losstest acctest
      model = gpu(cpum)
    end
    next!(progress)
    if (acctest[e]&gt;best_acc)
      best_acc = acctest[e]
      last_improvement=e
    end
    # If we haven't seen improvement in 3 epochs, we stop (could also
    # try droping learning rat but it would take time) we are cheating
    # here by using thest test accuracy as a stopping criteria ...
    if ((e - last_improvement) &gt;= 3) &amp;&amp; (opt.eta &lt;= 1e-6)
      @warn(&quot; -&gt; At epoch $e, haven't improved in 3 epochs. Stopping training.&quot;)
      break
    end
  end
  return(model=model,
         losstrain=losstrain[1:e], acctrain=acctrain[1:e],
         losstest=losstest[1:e],   acctest=acctest[1:e])
end
</code></pre>
<pre><code>train_mnist! (generic function with 1 method)
</code></pre>
<p>Now we train the model. We will begin by following a similar training
strategy as in the <a href="../mlp/">previous notes.</a> That is, we will use large
batches and a low number of passes through the data per-epoch.</p>
<pre><code class="language-julia">out = train_mnist!(model, X, Y, tX, tY, &quot;conv&quot;;
                   batchsize=2500,
                   reps_per_epoch=2,
                   maxepochs=200,
                   rerun=false
                   )
@show maximum(out.acctest)
</code></pre>
<pre><code>maximum(out.acctest) = 0.99
0.99
</code></pre>
<p>Since I save the model to disk to avoid waiting for it to rerun
everytime I change this document, the above output does not include the
training time. It takes roughly 10 seconds per epoch.</p>
<p>In terms of testing accuracy, this model does quite well. The deep
feedforward network with nearly 12 million parameters from the <a href="../mlp/">previous
notes</a>, had an accuracy greater than 98%. The convolutional
network used here with 16 thousand parameters has a accuracy of 0.99%.
The training time of these two models was roughly the same.</p>
<pre><code class="language-julia">function training_plot(out)
  ll = Int(round(length(out.losstrain)*0.75))
  lt = Int(round(length(out.losstrain)*0.5))
  plot(
    plot([out.losstrain, out.losstest], xscale=:log10, xlab=&quot;Epochs&quot;,
         title=&quot;Cross-Entropy Loss&quot;,
         ylims=(0.0, 0.125),
         annotations=[(ll, out.losstrain[ll],
                       Plots.text(&quot;training&quot;, pointsize=12, valign=:bottom,
                                  color=get(cscheme,1))),
                      (lt, out.losstest[lt],
                       Plots.text(&quot;test&quot;, pointsize=12, valign=:bottom,
                                  color=get(cscheme,0)))], leg=false,
         color_palette=get(cscheme,[1,0])
         ),
    plot([out.acctrain, out.acctest], xscale=:log10, xlab=&quot;Epochs&quot;,
         title=&quot;Accuracy&quot;,
         ylims=(0.95, 1.0),
         color_palette=get(cscheme,[1,0]), leg=false
         ),
    layout=(2,1)
  )
end
training_plot(out)
</code></pre>
<p><img alt="" src="../figures/conv_8_1.png" /></p>
<h2 id="small-batches">Small Batches<a class="headerlink" href="#small-batches" title="Permanent link">&para;</a></h2>
<p>If you look at the code in the <a href="https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl">Flux model
zoo</a>
for this model, it claims to achieve over 99% testing accuracy. The code
above is mostly identical to the model zoo, but it differs in two ways.
One is that the model zoo uses much smaller batches of 128 observations.
The second is that the model zoo adds some gaussian noise to the images
during training. We will look at how each of these changes affect the
results.</p>
<p>First, let’s just reduce the batch size to 128.</p>
<pre><code class="language-julia">model = Chain(  # same as before, but resetting initial values
  Conv((3, 3), 1=&gt;16,  pad=(1,1), relu),
  MaxPool((2,2)),
  Conv((3, 3), 16=&gt;32, pad=(1,1), relu),
  MaxPool((2,2)),
  Conv((3, 3), 32=&gt;32, pad=(1,1), relu),
  MaxPool((2,2)),
  x -&gt; reshape(x, :, size(x, 4)),
  Dense(288, 10)
  ) |&gt; gpu

smallbatch = train_mnist!(model, X, Y, tX, tY, &quot;conv-smallbatch&quot;;
                    loss=(x,y)-&gt;Flux.logitcrossentropy(model(x),y),
                    batchsize=128,
                    reps_per_epoch=1,
                    maxepochs=200,
                    rerun=false
                    )
@show maximum(smallbatch.acctest)
training_plot(smallbatch)
</code></pre>
<pre><code>maximum(smallbatch.acctest) = 0.9926
</code></pre>
<p><img alt="" src="../figures/conv_9_1.png" /></p>
<p>Smaller batches have improved the accuracy from 98.83% to just over 99%.
Note that although the number of epochs are roughly the same as above,
the number of gradient descent iterations is much higher. The total
run-time is roughly the same.</p>
<h2 id="data-augmentation">Data Augmentation<a class="headerlink" href="#data-augmentation" title="Permanent link">&para;</a></h2>
<p>The example in the model zoo also augments the training data by adding a
small amount of Gaussian noise to the training images. Data augmentation
is used by the best models among the <a href="http://yann.lecun.com/exdb/mnist/">MNIST benchmarks on LeCun’s
website</a>. For example @ciresan2010
randomly distorts images with small rotations and stretching. Adding
Gaussian noise is not as well geometrically motivated, but it has the
advantage of being very easy to implement.</p>
<p>We will add a $\epsilon \sim N(0, 0.1)$ to each pixel in the images.
Here is what this looks like for a few images</p>
<pre><code class="language-julia">plot([plot(RGB.(1 .- 0.1*randn(size(imgs[i]))) .- round.(imgs[i])) for i in
      1:9]..., xlab=&quot;&quot;, ylab=&quot;&quot;, aspect_ratio=:equal, axis=false, ticks=false)
</code></pre>
<p><img alt="" src="../figures/conv_10_1.png" /></p>
<p>With this added noise, the digits are still easily recognizable, so we
would hope that our model can classify them.</p>
<pre><code class="language-julia">addnoise(x, σ) = x .+ σ.*gpu(randn(eltype(x), size(x)))
@show accuracy(out.model, addnoise(tX,0.1f0), tY)
@show accuracy(smallbatch.model, addnoise(tX,0.1f0), tY)
</code></pre>
<pre><code>Error: DimensionMismatch: A has dimensions (10,288) but B has dimensions (3
,960000)
</code></pre>
<p>The models trained above do a pretty good job of classifying noisy
images, but not quite as well as the original. What if we train the
models with noise?</p>
<p>Here, we will train with the original large batches and added noise.</p>
<pre><code class="language-julia">model = Chain(  # same as before, but resetting initial values
  Conv((3, 3), 1=&gt;16,  pad=(1,1), relu),
  MaxPool((2,2)),
  Conv((3, 3), 16=&gt;32, pad=(1,1), relu),
  MaxPool((2,2)),
  Conv((3, 3), 32=&gt;32, pad=(1,1), relu),
  MaxPool((2,2)),
  x -&gt; reshape(x, :, size(x, 4)),
  Dense(288, 10)
) |&gt; gpu

outn = train_mnist!(model, X, Y, tX, tY, &quot;conv-augrandn&quot;;
                    loss=(x,y)-&gt;Flux.logitcrossentropy(model(x .+ 0.1f0*gpu(randn(eltype(x), size(x)))),y),
                    batchsize=2500,
                    reps_per_epoch=2,
                    maxepochs=200,
                    rerun=false
                    )
@show maximum(outn.acctest)
</code></pre>
<pre><code>maximum(outn.acctest) = 0.9922
0.9922
</code></pre>
<pre><code class="language-julia">training_plot(outn)
</code></pre>
<p><img alt="" src="../figures/conv_13_1.png" /></p>
<p>With large batches, adding noise has improved the accuracy very little</p>
<h2 id="small-batches-and-data-augmentation">Small Batches and Data Augmentation<a class="headerlink" href="#small-batches-and-data-augmentation" title="Permanent link">&para;</a></h2>
<p>Let’s try combining small batches and data augmentation.</p>
<pre><code class="language-julia">model = Chain(  # same as before, but resetting initial values
  Conv((3, 3), 1=&gt;16,  pad=(1,1), relu),
  MaxPool((2,2)),
  Conv((3, 3), 16=&gt;32, pad=(1,1), relu),
  MaxPool((2,2)),
  Conv((3, 3), 32=&gt;32, pad=(1,1), relu),
  MaxPool((2,2)),
  x -&gt; reshape(x, :, size(x, 4)),
  Dense(288, 10)
) |&gt; gpu

smallnoisy = train_mnist!(model, X, Y, tX, tY, &quot;conv-augrandn-smallbatch&quot;;
                    loss=(x,y)-&gt;Flux.logitcrossentropy(model(x .+ 0.1f0*gpu(randn(eltype(x), size(x)))),y),
                    batchsize=128,
                    reps_per_epoch=1,
                    maxepochs=200,
                    rerun=false
                    )
@show maximum(smallnoisy.acctest)
training_plot(smallnoisy)
</code></pre>
<pre><code>maximum(smallnoisy.acctest) = 0.9934
</code></pre>
<p><img alt="" src="../figures/conv_14_1.png" /></p>
<p>This gives the highest test accuracy we have achieved so far. Let’s look
at some of the missclassified digits.</p>
<pre><code class="language-julia">tlabels = test_y
timgs = [Gray.(test_x[:,:,i]') for i ∈ 1:size(test_x,3)]

# predicted labels
mlabels = onecold(cpu(smallnoisy.model(tX))).-1
@show mean(mlabels.==tlabels) # = accuracy
@show sum(mlabels .!= tlabels)
miss=findall(mlabels .!= tlabels)
plot( [plot(RGB.(1 .- timgs[i]),
            title=&quot;$(tlabels[i]) as $(mlabels[i])&quot;,
            axis=false, ticks=false,
            aspect_ratio=:equal) for i in miss[1:16]]...)
</code></pre>
<pre><code>mean(mlabels .== tlabels) = 0.9916
sum(mlabels .!= tlabels) = 84
</code></pre>
<p><img alt="" src="../figures/conv_15_1.png" /></p>
<h2 id="looking-inside-the-black-box">Looking inside the Black Box<a class="headerlink" href="#looking-inside-the-black-box" title="Permanent link">&para;</a></h2>
<p>Our fitted model is somewhat of a black box, but when we are working
with images and convolution, we can somewhat look inside it. We can
display the images that are generated by each convolutional and/or max
pool layer. Let’s do this for one image.</p>
<pre><code class="language-julia">i = 1
m = smallnoisy.model
figs = Array{typeof(plot()), 1}(undef, 3)
j = 1
for l in 1:length(m)
  global j, figs
  if (typeof(m[l]) &lt;: MaxPool)
    layer = m[1:l](reshape(X[:,:,:,i], 28,28,1,1))
    figs[j] = plot( [plot(RGB.(cpu(1 .- layer[:,:,c,1])), aspect_ratio=:equal,
                          axis=false, ticks=false) for c in
                     1:size(layer,3)]...)
    j += 1
  end
end
plot(RGB.(cpu(1 .- X[:,:,1,i])), aspect_ratio=:equal, axis=false,
     ticks=false)
</code></pre>
<p><img alt="" src="../figures/conv_16_1.png" /></p>
<p>The original is above. Below is are the output of the first
convolution + max pool layer.</p>
<pre><code class="language-julia">figs[1]
</code></pre>
<p><img alt="" src="../figures/conv_17_1.png" /></p>
<p>Now the second</p>
<pre><code class="language-julia">figs[2]
</code></pre>
<p><img alt="" src="../figures/conv_18_1.png" /></p>
<p>And the last</p>
<pre><code class="language-julia">figs[3]
</code></pre>
<p><img alt="" src="../figures/conv_19_1.png" /></p>
<p>These 32 three by three images then get stacked into a vector and passed
into the final dense layer.</p>
<p>I’m not sure that there’s much to learn from looking at these images.
Maybe it’s best to keep the box closed.</p>
<h1 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h1></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
